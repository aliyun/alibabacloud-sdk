/**
 *
 */
import BaseClientBuilder;
import TeaAsyncHandler;
import TeaRequest;
import AsyncRequestBody;
import RequestBody;
import AsyncResponseHandler;
import ClientConfiguration;
import ClientExecutionParams;
extends BaseClientBuilder;
type @product = string
type @version = string
type @endpointRule = string
type @endpointMap = map[string]string
type @REQUEST = TeaRequest
type @handler = TeaAsyncHandler

init(configuration: ClientConfiguration){
  @handler = new TeaAsyncHandler(configuration);
  @product = 'Mts';
  @version = '2014-06-18';
  @endpointRule = 'regional';
  @endpointMap = {
    'ap-northeast-2-pop' = 'mts.aliyuncs.com',
    'ap-southeast-2' = 'mts.aliyuncs.com',
    'ap-southeast-3' = 'mts.aliyuncs.com',
    'cn-beijing-finance-1' = 'mts.aliyuncs.com',
    'cn-beijing-finance-pop' = 'mts.aliyuncs.com',
    'cn-beijing-gov-1' = 'mts.aliyuncs.com',
    'cn-beijing-nu16-b01' = 'mts.aliyuncs.com',
    'cn-chengdu' = 'mts.aliyuncs.com',
    'cn-edge-1' = 'mts.aliyuncs.com',
    'cn-fujian' = 'mts.aliyuncs.com',
    'cn-haidian-cm12-c01' = 'mts.aliyuncs.com',
    'cn-hangzhou-bj-b01' = 'mts.aliyuncs.com',
    'cn-hangzhou-finance' = 'mts.aliyuncs.com',
    'cn-hangzhou-internal-prod-1' = 'mts.aliyuncs.com',
    'cn-hangzhou-internal-test-1' = 'mts.aliyuncs.com',
    'cn-hangzhou-internal-test-2' = 'mts.aliyuncs.com',
    'cn-hangzhou-internal-test-3' = 'mts.aliyuncs.com',
    'cn-hangzhou-test-306' = 'mts.aliyuncs.com',
    'cn-hongkong-finance-pop' = 'mts.aliyuncs.com',
    'cn-huhehaote-nebula-1' = 'mts.aliyuncs.com',
    'cn-north-2-gov-1' = 'mts.aliyuncs.com',
    'cn-qingdao-nebula' = 'mts.aliyuncs.com',
    'cn-shanghai-et15-b01' = 'mts.aliyuncs.com',
    'cn-shanghai-et2-b01' = 'mts.aliyuncs.com',
    'cn-shanghai-finance-1' = 'mts.aliyuncs.com',
    'cn-shanghai-inner' = 'mts.aliyuncs.com',
    'cn-shanghai-internal-test-1' = 'mts.aliyuncs.com',
    'cn-shenzhen-finance-1' = 'mts.aliyuncs.com',
    'cn-shenzhen-inner' = 'mts.aliyuncs.com',
    'cn-shenzhen-st4-d01' = 'mts.aliyuncs.com',
    'cn-shenzhen-su18-b01' = 'mts.aliyuncs.com',
    'cn-wuhan' = 'mts.aliyuncs.com',
    'cn-wulanchabu' = 'mts.aliyuncs.com',
    'cn-yushanfang' = 'mts.aliyuncs.com',
    'cn-zhangbei' = 'mts.aliyuncs.com',
    'cn-zhangbei-na61-b01' = 'mts.aliyuncs.com',
    'cn-zhangjiakou-na62-a01' = 'mts.aliyuncs.com',
    'cn-zhengzhou-nebula-1' = 'mts.aliyuncs.com',
    'eu-west-1-oxs' = 'mts.aliyuncs.com',
    'me-east-1' = 'mts.aliyuncs.com',
    'rus-west-1-pop' = 'mts.aliyuncs.com',
    'us-east-1' = 'mts.aliyuncs.com',
  };
}

function close(): void {
  @handler.close();
}

model ActivateMediaWorkflowRequest {
  mediaWorkflowId: string(name='MediaWorkflowId', description='The ID of the media workflow. You can obtain the ID from the response of the [AddMediaWorkflow](~~44437~~) operation.', example='93ab850b4f6f44eab54b6e9181d4****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model ActivateMediaWorkflowResponseBody = {
  mediaWorkflow?: {
    creationTime?: string(name='CreationTime', description='The time when the media workflow was created.', example='2016-04-01T05:29:37Z'),
    mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the media workflow.', example='93ab850b4f6f44eab54b6e9181d4****'),
    name?: string(name='Name', description='The name of the media workflow.', example='mediaworkflow-example'),
    state?: string(name='State', description='The status of the media workflow. The value is **Active**.', example='Active'),
    topology?: string(name='Topology', description='The topology of the media workflow.', example='{\\"Activities\\":{\\"Act-Start\\":{\\"Parameters\\":{\\"PipelineId\\":\\"130266f58161436a80bf07cb12c8****\\",\\"InputFile\\":\\"{\\\\\\"Bucket\\\\\\": \\\\\\"example\\\\\\",\\\\\\"Location\\\\\\": \\\\\\"oss-cn-hangzhou\\\\\\"}\\"},\\"Type\\":\\"Start\\"},\\"Act-Report\\":{\\"Parameters\\":{},\\"Type\\":\\"Report\\"},\\"Act-Transcode-M3U8\\":{\\"Parameters\\":{\\"Outputs\\":\\"[{\\\\\\"OutputObject\\\\\\":\\\\\\"transcode%2F%7BObjectPrefix%7D%7BFileName%7D\\\\\\",\\\\\\"TemplateId\\\\\\": \\\\\\"957d1719ee85ed6527b90cf62726****\\\\\\"}]\\",\\"OutputBucket\\":\\"panda-vod-hls\\",\\"OutputLocation\\":\\"oss-cn-hangzhou\\"},\\"Type\\":\\"Transcode\\"}},\\"Dependencies\\":{\\"Act-Start\\":[\\"Act-Transcode-M3U8\\"],\\"Act-Report\\":[],\\"Act-Transcode-M3U8\\":[\\"Act-Report\\"]}}'),
  }(name='MediaWorkflow', description='The details of the media workflow.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='A1326BD4-30B1-4CB6-Q123-3330B877B0D4'),
}

model ActivateMediaWorkflowResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: ActivateMediaWorkflowResponseBody(name='body'),
}

/**
  * You can call this operation to activate a media workflow that has been deactivated. After you activate a media workflow, you cannot modify the workflow information, such as the name, topology, or trigger mode. A media workflow is activated by default after it is created.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function activateMediaWorkflow(request: ActivateMediaWorkflowRequest): ActivateMediaWorkflowResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'ActivateMediaWorkflow', 'POST', '/', 'json', false, 'json', request);
}

model AddMediaRequest {
  cateId?: long(name='CateId', description='The ID of the category to which the media file belongs. The value cannot be negative.', example='123', position='Query'),
  coverURL?: string(name='CoverURL', description='The URL of the thumbnail. To obtain the URL, you can log on to the **MPS console** and choose **Workflows** > **Media Buckets**. Alternatively, you can log on to the **OSS console** and click **My OSS Paths**.

*   The value can be up to 3,200 bytes in length.
*   The URL complies with RFC 2396 and is encoded in UTF-8, with reserved characters being percent-encoded. For more information, see [URL encoding](~~423796~~).', example='http://bucket.oss-cn-hangzhou.aliyuncs.com/example/1.png', position='Query'),
  description?: string(name='Description', description='The description of the media file.

*   The description can be up to 1,024 bytes in length.
*   The value must be encoded in UTF-8.', example='A test video', position='Query'),
  fileURL: string(name='FileURL', description='The path of the input file. You can query the path of the input file in the MPS or OSS console. For more information, see the **Triggering and matching rules for a workflow** section of this topic.

*   The value can be up to 3,200 bytes in length.
*   The URL complies with RFC 2396 and is encoded in UTF-8, with reserved characters being percent-encoded. For more information, see [URL encoding](~~423796~~).', example='http://bucket.oss-cn-hangzhou.aliyuncs.com/A/B/C/test.mp4', position='Query'),
  inputUnbind?: boolean(name='InputUnbind', description='Specifies whether to check if the media workflow supports the specified input path. We recommend that you set this parameter to true to prevent errors that may result from invalid paths. Valid values:

*   **true**: checks whether the workflow supports the specified input path.
*   **false**: does not check whether the workflow supports the specified input path.', example='false', position='Query'),
  mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the media workflow that you want to run for the media file. To query the ID of a media workflow, you can log on to the MPS console or call the [AddMediaWorkflow](~~44437~~) operation.', example='07da6c65da7f458997336e0de192****', position='Query'),
  mediaWorkflowUserData?: string(name='MediaWorkflowUserData', description='The custom data of the media workflow.

*   The value can be up to 1,024 bytes in length.
*   The value must be encoded in UTF-8.', example='test', position='Query'),
  overrideParams?: string(name='OverrideParams', description='The subtitle settings that are used to overwrite the original settings.

*   Example 1: Use `{"WebVTTSubtitleOverrides",[{"RefActivityName":"subtitleNode","WebVTTSubtitleURL":"http://test.oss-cn-hangzhou.aliyuncs.com/example1.vtt"}]}` to overwrite the original subtitle settings during HTTP Live Streaming (HLS) packaging.
*   Example 2: Use `{"subtitleTransNodeName":{"InputConfig":{"Format":"stl","InputFile":{"URL":"http://subtitleBucket.oss-cn-hangzhou.aliyuncs.com/package/example/CENG.stl"}}}}` to overwrite the original subtitle settings during Dynamic Adaptive Streaming over HTTP (DASH) packaging.', example='{“subtitleTransNodeName”:{“InputConfig”:{“Format”:”stl”,”InputFile”:{“URL”:”http://exampleBucket.oss-cn-hangzhou.aliyuncs.com/package/example/CENG.stl"}}}}', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  tags?: string(name='Tags', description='The tags that you want to add to the media file.

> In MPS, each tag that is specified for a media file is independent. You can search for all the media files that have the same tags in the Media Library.

*   You can specify up to 16 tags for a media file. Separate multiple tags with commas (,).
*   Each tag can be up to 32 bytes in size
*   The value must be encoded in UTF-8.', example='tag1,tag2', position='Query'),
  title?: string(name='Title', description='The title of the media file.

*   The title can be up to 128 bytes in length.
*   The value must be encoded in UTF-8.', example='mytest', position='Query'),
}

model AddMediaResponseBody = {
  media?: {
    bitrate?: string(name='Bitrate', description='The bitrate.', example='1148.77'),
    cateId?: long(name='CateId', description='The ID of the category to which the media file belongs.', example='1'),
    censorState?: string(name='CensorState', description='The review status of the media file. Valid values:

*   **Initiated**: The media file is uploaded but not reviewed.
*   **Pass**: The media file is uploaded and passes the review.', example='Initiated'),
    coverURL?: string(name='CoverURL', description='The URL of the thumbnail.', example='http://bucket.oss-cn-hangzhou.aliyuncs.com/example/1.png'),
    creationTime?: string(name='CreationTime', description='The time when the media file was created.', example='2016-09-20T03:02:40Z'),
    description?: string(name='Description', description='The description of the media file. The description can be up to 1,024 bytes in length.', example='A test video'),
    duration?: string(name='Duration', description='The duration of the media file.', example='2.645333'),
    file?: {
      state?: string(name='State', description='The status of the file. The default value is **Normal**.', example='Normal'),
      URL?: string(name='URL', description='The URL of the media file.', example='http://bucket.oss-cn-hangzhou.aliyuncs.com/A/B/C/test.mp4'),
    }(name='File', description='The information about the input file.'),
    format?: string(name='Format', description='The format of the media file. Valid values: mov, mp4, m4a, 3gp, 3g2, and mj2.', example='mp4'),
    fps?: string(name='Fps', description='The frame rate of the media file.', example='25.0'),
    height?: string(name='Height', description='The height of the media file.', example='1280'),
    mediaId?: string(name='MediaId', description='The ID of the media file.', example='3e6149d5a8c944c09b1a8d2dc3e4****'),
    publishState?: string(name='PublishState', description='The publishing status of the media file. Valid values:

*   **Initiated**: The media file is in the initial state.
*   **UnPublish**: The media file has not been published, and the playback permission on the OSS object is Private.
*   **Published**: The media file has been published, and the playback permission on the OSS object is Default.', example='Published'),
    runIdList?: {
      runId?: [ string ](name='RunId')
    }(name='RunIdList', description='The IDs of the media workflow execution instances.'),
    size?: string(name='Size', description='The size of the media file.', example='379860'),
    tags?: {
      tag?: [ string ](name='Tag')
    }(name='Tags', description='The tags of the media file.'),
    title?: string(name='Title', description='The title of the media file. The title can be up to 128 bytes in length.', example='mytest.mp4'),
    width?: string(name='Width', description='The width of the media file.', example='1280'),
  }(name='Media', description='The detailed information about the media file.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='05F8B913-E9F3-4A6F-9922-48CADA0FFAAD'),
}

model AddMediaResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: AddMediaResponseBody(name='body'),
}

/**
  * *   You can call this operation to process videos that are uploaded to Object Storage Service (OSS) but not processed. This way, you do not need to upload the videos to OSS again. If you have configured media workflows, OSS automatically notifies ApsaraVideo Media Processing (MPS) when a media file is uploaded to OSS. MPS automatically finds the corresponding workflow in the Active state based on the specified OSS bucket and object. Therefore, in most cases, you do not need to manually call the AddMedia operation to process the media file.
  * *   Media information is automatically obtained only when the specified media workflow is in the Active state. If no media workflow is specified or the specified media workflow is not in the Active state, media information is not obtained.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function addMedia(request: AddMediaRequest): AddMediaResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'AddMedia', 'POST', '/', 'json', false, 'json', request);
}

model AddMediaTagRequest {
  mediaId: string(name='MediaId', description='The ID of the media file to which you want to add tags.

> To obtain the ID of a media file, you can call the [AddMedia](~~44458~~) operation. Alternatively, perform the following operations in the ApsaraVideo Media Processing (MPS) console: In the left-side navigation pane, choose **Media Management** > **Media List**. Find the file that you want to manage and click **Manage** in the Actions column. The ID of the file is displayed on the Basics tab.', example='3e6149d5a8c944c09b1a8d2dc3e4****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  tag?: string(name='Tag', description='The tag that you want to add to the medial file. The value is encoded in UTF-8 and can be up to 32 bytes in length.', example='tag1', position='Query'),
}

model AddMediaTagResponseBody = {
  requestId?: string(name='RequestId', description='The ID of the request.', example='91B6CAB9-034C-4E4E-A40B-E7F5C81E1A2K'),
}

model AddMediaTagResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: AddMediaTagResponseBody(name='body'),
}

/**
  * You can call this operation to add only one tag. To add multiple tags at a time, you can call the [UpdateMedia](~~44464~~) operation.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function addMediaTag(request: AddMediaTagRequest): AddMediaTagResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'AddMediaTag', 'POST', '/', 'json', false, 'json', request);
}

model AddMediaWorkflowRequest {
  name: string(name='Name', description='The name of the media workflow.

*   The value cannot be empty.
*   The name cannot be the same as that of an existing media workflow within the current Alibaba Cloud account.
*   The name can be up to 64 characters in length.
*   The value must be encoded in the UTF-8 format.', example='mediaworkflow-example', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  topology: string(name='Topology', description='The topology of the media workflow. The value must be a JSON object that contains the activities and activity dependencies. For more information, see the **Sample topology** section of this topic.

>  The Object Storage Service (OSS) bucket must reside in the same region as your MPS service.', position='Query'),
  triggerMode?: string(name='TriggerMode', description='The triggering mode of the media workflow. Valid values:

*   **OssAutoTrigger**: The media workflow is automatically triggered.
*   **NotInAuto**: The media workflow is not automatically triggered.', example='OssAutoTrigger', position='Query'),
}

model AddMediaWorkflowResponseBody = {
  mediaWorkflow?: {
    creationTime?: string(name='CreationTime', description='The time when the media workflow was created.', example='016-04-01T05:29:37Z'),
    mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the media workflow. We recommend that you keep this ID for later operations on this workflow.', example='e00732b977da427d9177a4deb1aa****'),
    name?: string(name='Name', description='The name of the media workflow.', example='mediaworkflow-example'),
    state?: string(name='State', description='The state of the media workflow. By default, the created workflow is in the **Active** state.', example='Active'),
    topology?: string(name='Topology', description='The topology of the media workflow. The value is a JSON object that contains the activities and activity dependencies.', example='{mediaworkflow","State":"Active","Topology":"{\\"Activities\\":{\\"Act-Start\\":{\\"Parameters\\":{\\"PipelineId\\":\\"130266f58161436a80bf07cb12c8****\\",\\"InputFile\\":\\"{\\\\\\"Bucket\\\\\\": \\\\\\"example-bucket-****\\\\\\",\\\\\\"Location\\\\\\": \\\\\\"cn-shanghai\\\\\\"}\\"},\\"Type\\":\\"Start\\"},\\"Act-Report\\":{\\"Parameters\\":{},\\"Type\\":\\"Report\\"},\\"Act-Transcode-M3U8\\":{\\"Parameters\\":{\\"Outputs\\":\\"[{\\\\\\"Object\\\\\\":\\\\\\"transcode/{ObjectPrefix}{FileName}\\\\\\",\\\\\\"TemplateId\\\\\\": \\\\\\"957d1719ee85ed6527b90cf62726****\\\\\\"}]\\",\\"OutputBucket\\":\\"example-bucket-****\\",\\"OutputLocation\\":\\"cn-shanghai\\"},\\"Type\\":\\"Transcode\\"}},\\"Dependencies\\":{\\"Act-Start\\":[\\"Act-Transcode-M3U8\\"],\\"Act-Report\\":[],\\"Act-Transcode-M3U8\\":[\\"Act-Report\\"]}}","MediaWorkflowId":"93ab850b4f6f44eab54b6e91d24d****"}]},"RequestId":"16CD0CDD-457E-420D-9755-8385075A1234"}'),
    triggerMode?: string(name='TriggerMode', description='The triggering mode of the media workflow. Valid values:

*   **OssAutoTrigger**: The media workflow is automatically triggered.
*   **NotInAuto**: The media workflow is not automatically triggered.', example='OssAutoTrigger'),
  }(name='MediaWorkflow', description='The information about the media workflow.'),
  requestId?: string(name='RequestId', description='The request ID.', example='F1D21261-ADB9-406A-1234-491382139D59'),
}

model AddMediaWorkflowResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: AddMediaWorkflowResponseBody(name='body'),
}

/**
  * *   You can call this operation to define the topology, activities, and dependencies of a media workflow. The topology is represented by a directed acyclic graph (DAG) in the console. For more information, see [Workflow activities](~~68494~~). You can view and run the workflows that are created by calling this operation in the ApsaraVideo Media Processing (MPS) console.
  * *   MPS media workflows can be automatically triggered only by using the prefix of the file path. Automatic triggering by using the suffix is not supported. For more information about the trigger rules, see [Workflow triggering rules for files](~~68574~~).
  * ### [](#qps)QPS limits
  * You can call this API operation up to 100 times per second per account. Requests that exceed this limit are dropped, and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limits](~~342832~~).
  *
 */
async function addMediaWorkflow(request: AddMediaWorkflowRequest): AddMediaWorkflowResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'AddMediaWorkflow', 'POST', '/', 'json', false, 'json', request);
}

model AddPipelineRequest {
  name: string(name='Name', description='The name of the MPS queue. The name can be up to 128 bytes in size.', example='test-pipeline', position='Query'),
  notifyConfig?: string(name='NotifyConfig', description='The Message Service (MNS) configuration.', example='{"Topic":"mts-topic-1"}', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  role?: string(name='Role', description='The role.', example='AliyunMTSDefaultRole', position='Query'),
  speed?: string(name='Speed', description='The type of the MPS queue. Valid values:

*   **Boost**: MPS queue with transcoding speed boosted.
*   **Standard**: standard MPS queue.
*   **NarrowBandHDV2**: MPS queue that supports Narrowband HD 2.0.
*   **AIVideoCover**: MPS queue for intelligent snapshot capture.
*   **AIVideoTag**: MPS queue for video tagging. The supported regions are China (Shanghai), China (Beijing), and China (Hangzhou).

Default value: **Standard**.', example='Standard', position='Query'),
  speedLevel?: long(name='SpeedLevel', description='The level of the MPS queue. Valid values: **1 to 3**.', example='1', position='Query'),
}

model AddPipelineResponseBody = {
  pipeline?: {
    id?: string(name='Id', description='The ID of the MPS queue.', example='ed450ea0bfbd41e29f80a401fb4d****'),
    name?: string(name='Name', description='The name of the MPS queue.', example='Media Fingerprint'),
    notifyConfig?: {
      mqTag?: string(name='MqTag', description='The tag string.', example='mts-test'),
      mqTopic?: string(name='MqTopic', description='The queue of messages that are received.', example='example1'),
      queueName?: string(name='QueueName', description='The name of the queue.', example='mts-queue-1'),
      topic?: string(name='Topic', description='The name of the topic.', example='mts-topic-1'),
    }(name='NotifyConfig', description='The MNS configuration.'),
    quotaAllocate?: long(name='QuotaAllocate', description='The quota that is allocated to the MPS queue.', example='10'),
    role?: string(name='Role', description='The role.', example='AliyunMTSDefaultRole'),
    speed?: string(name='Speed', description='The type of the MPS queue.', example='Standard'),
    speedLevel?: long(name='SpeedLevel', description='The level of the MPS queue.', example='1'),
    state?: string(name='State', description='The state of the MPS queue.

*   Active: The MPS queue is active. The jobs in the MPS queue are scheduled and transcoded by MPS.
*   Paused: The MPS queue is paused. Jobs in the MPS queue are no longer scheduled for transcoding by MPS. All of the jobs in the MPS queue remain in the Submitted state. Jobs that are being transcoded are not affected.', example='Active'),
  }(name='Pipeline', description='The MPS queue.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='CFEA608A-5A1C-4C83-A54B-6197BC250D23'),
}

model AddPipelineResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: AddPipelineResponseBody(name='body'),
}

async function addPipeline(request: AddPipelineRequest): AddPipelineResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'AddPipeline', 'POST', '/', 'json', false, 'json', request);
}

model AddSmarttagTemplateRequest {
  analyseTypes: string(name='AnalyseTypes', example='ocr', position='Query'),
  faceCategoryIds?: string(name='FaceCategoryIds', example='celebrity', position='Query'),
  faceCustomParamsConfig?: string(name='FaceCustomParamsConfig', example='{ "faceDetThreshold":0.999, "faceRegThreshold":0.9 }', position='Query'),
  industry: string(name='Industry', example='common', position='Query'),
  isDefault?: boolean(name='IsDefault', example='true', position='Query'),
  keywordConfig?: string(name='KeywordConfig', example='"type": "name,location,organization,other" }', position='Query'),
  knowledgeConfig?: string(name='KnowledgeConfig', example='{ "movie":"name,alias,chnl,genre", "music":"songName,artistName", "person":"name,gender" }', position='Query'),
  labelType?: string(name='LabelType', example='hmi', position='Query'),
  labelVersion?: string(name='LabelVersion', example='1.0', position='Query'),
  landmarkGroupIds?: string(name='LandmarkGroupIds', example='common', position='Query'),
  objectGroupIds?: string(name='ObjectGroupIds', example='general,item,weapon,animal', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  scene: string(name='Scene', example='search', position='Query'),
  templateName: string(name='TemplateName', example='template-example-****', position='Query'),
}

model AddSmarttagTemplateResponseBody = {
  requestId?: string(name='RequestId', example='7B117AF5-2A16-412C-B127-FA6175EDAS3Q'),
  templateId?: string(name='TemplateId', example='05de22f255284c7a8d2aab535dde****'),
}

model AddSmarttagTemplateResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: AddSmarttagTemplateResponseBody(name='body'),
}

async function addSmarttagTemplate(request: AddSmarttagTemplateRequest): AddSmarttagTemplateResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'AddSmarttagTemplate', 'POST', '/', 'json', false, 'json', request);
}

model AddTemplateRequest {
  audio?: string(name='Audio', description='The audio stream settings. The value must be a JSON object. For more information, see [Audio](~~29253~~).

> If you do not specify this parameter, output files do not contain audio streams. This parameter is required if you want to retain the audio streams.', example='{"Codec":"H.264","Samplerate":"44100","Bitrate":"500","Channels":"2"}', position='Query'),
  container?: string(name='Container', description='The container format. The value must be a JSON object that contains the Format parameter. If you do not specify this parameter, the transcoded media file is in MP4 format by default. This parameter is required if you want to use the transcoding template to generate media files in other formats. For more information, see [Container](~~29253~~).

*   Default value: MP4.
*   Video transcoding supports the following formats: FLV, MP4, HLS (M3U8 + TS), and MPEG-DASH (MPD + fMP4).

> If the container format is FLV, the video codec cannot be set to H.265.

*   Audio transcoding supports the following formats: MP3, MP4, OGG, FLAC, and M4A.
*   Image transcoding supports the GIF and WebP formats.

> 

*   If the container format is GIF, the video codec must be set to GIF.

*   If the container format is WebP, the video codec must be set to WebP.', example='{"Format":"mp4"}', position='Query'),
  muxConfig?: string(name='MuxConfig', description='The segment settings. The value must be a JSON object. For more information, see [MuxConfig](~~29253~~). If you do not specify this parameter, media segment files are not generated. This parameter is required if you want to generate media segment files.', example='{"Segment":{"Duration":"10"}}', position='Query'),
  name: string(name='Name', description='The name of the transcoding template. The name can be up to 128 bytes in length.', example='mps-example', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  transConfig?: string(name='TransConfig', description='The general transcoding settings. The value must be a JSON object. For more information, see [TransConfig](~~29253~~). If you do not specify this parameter, the default settings are used. This parameter is required if the default settings cannot meet your business requirements.', example='{"TransMode":"onepass"}', position='Query'),
  video?: string(name='Video', description='The video stream settings. The value must be a JSON object. For more information, see [Video](~~29253~~).

> If you do not specify this parameter, output files do not contain video streams. This parameter is required if you want to retain the video streams.', example='{"Codec":"H.264","Profile":"high","Bitrate":"500","Crf":"15","Width":"256","Height":"800","Fps":"25","Gop":"10s"}', position='Query'),
}

model AddTemplateResponseBody = {
  requestId?: string(name='RequestId', description='The ID of the request.', example='FA258E67-09B8-4EAA-8F33-BA567834A2C3'),
  template?: {
    audio?: {
      bitrate?: string(name='Bitrate', description='The audio bitrate of the output file.

*   Unit: Kbit/s.
*   Default value: **128**.', example='500'),
      channels?: string(name='Channels', description='The number of sound channels. Default value: **2**.', example='2'),
      codec?: string(name='Codec', description='The audio codec format. Default value: **aac**. Valid values:

*   **aac**
*   **mp3**
*   **vorbis**
*   **flac**', example='aac'),
      profile?: string(name='Profile', description='The codec profile of the audio. Valid values if the **Codec** parameter is set to **AAC**:

*   **aac_low**
*   **aac_he**
*   **aac_he_v2**
*   **aac_ld**
*   **aac_eld**', example='aac_low'),
      qscale?: string(name='Qscale', description='The level of the independent denoising algorithm.', example='5'),
      remove?: string(name='Remove', description='Indicates whether the audio stream is deleted.

*   **true**: The audio stream is deleted.
*   **false**: The audio stream is retained.
*   Default value: **false**.', example='true'),
      samplerate?: string(name='Samplerate', description='The sampling rate.

*   Unit: Hz.
*   Default value: **44100**.', example='44100'),
      volume?: {
        integratedLoudnessTarget?: string(name='IntegratedLoudnessTarget', description='The output volume.

This parameter takes effect only when the value of Method is dynamic.

Unit: dB.

Valid values: \\[-70,-5].

Default value: -6.', example='-6'),
        level?: string(name='Level', description='The volume adjustment range.

*   Default value: **-20**.
*   Unit: dB.', example='-20'),
        loudnessRangeTarget?: string(name='LoudnessRangeTarget', description='The range of the volume relative to the output volume.

This parameter takes effect only when the value of Method is dynamic.

Unit: dB.

Valid values: \\[1,20].

Default value: 8.', example='8'),
        method?: string(name='Method', description='The volume adjustment method. Valid values:

*   **auto**: The volume is automatically adjusted.
*   **dynamic**: The volume is dynamically adjusted.
*   **linear**: The volume is linearly adjusted.', example='auto'),
        peakLevel?: string(name='PeakLevel', description='The volume adjustment coefficient.

This parameter takes effect only when the value of Method is adaptive.

Valid values: \\[0,1].

Default value: 0.9.', example='0.9'),
        truePeak?: string(name='TruePeak', description='The peak volume.

This parameter takes effect only when the value of Method is dynamic.

Unit: dB.

Valid values: \\[-9,0].

Default value: -1.', example='0'),
      }(name='Volume', description='The volume control configurations'),
    }(name='Audio', description='The audio codec configurations.'),
    container?: {
      format?: string(name='Format', description='The container format.', example='mp4'),
    }(name='Container', description='The container format settings.'),
    id?: string(name='Id', description='The ID of the transcoding template. We recommend that you keep this ID for subsequent operation calls.', example='16f01ad6175e4230ac42bb5182cd****'),
    muxConfig?: {
      gif?: {
        ditherMode?: string(name='DitherMode', description='The color dithering algorithm of the palette. Valid values: sierra and bayer.', example='sierra'),
        finalDelay?: string(name='FinalDelay', description='The duration for which the final frame is paused. Unit: centiseconds.', example='0'),
        isCustomPalette?: string(name='IsCustomPalette', description='Indicates whether the custom palette is used.', example='false'),
        loop?: string(name='Loop', description='The loop count.', example='0'),
      }(name='Gif', description='The transmuxing settings for GIF.'),
      segment?: {
        duration?: string(name='Duration', description='The length of the segment. Unit: seconds.', example='10'),
      }(name='Segment', description='The segment settings.'),
      webp?: {
        loop?: string(name='Loop', description='The loop count.', example='0'),
      }(name='Webp', description='The transmuxing settings for WebP.'),
    }(name='MuxConfig', description='The transmuxing settings.'),
    name?: string(name='Name', description='The name of the transcoding template.', example='mps-example'),
    state?: string(name='State', description='The status of the template. Valid values:

*   **Normal**: The template is normal.
*   **Deleted**: The template is deleted.', example='Normal'),
    transConfig?: {
      adjDarMethod?: string(name='AdjDarMethod', description='The method of resolution adjustment. Default value: **none**. Valid values:

*   **rescale**: The input video is rescaled.
*   **crop**: The input video is cropped.
*   **none**: No change is made.', example='rescale'),
      isCheckAudioBitrate?: string(name='IsCheckAudioBitrate', description='Indicates whether the audio bitrate is checked.

If this feature is enabled and the system detects that the audio bitrate of the output file is greater than that of the input file, the audio bitrate of the input file is retained after transcoding.

*   **true**: The audio bitrate is checked.
*   **false**: The audio bitrate is not checked.
*   Default value: **false**.', example='true'),
      isCheckAudioBitrateFail?: string(name='IsCheckAudioBitrateFail', description='Indicates whether the audio bitrate is checked. If this feature is enabled and the system detects that the audio bitrate of the output file is higher than that of the input file, the input file is not transcoded. This parameter has a higher priority than the **IsCheckAudioBitrate** parameter. Valid values:

*   **true**: The audio bitrate is checked. In this case, if the audio bitrate of the output file is higher than that of the input file, the input file is not transcoded.
*   **false**: The audio bitrate is not checked.
*   Default value: **false**.', example='true'),
      isCheckReso?: string(name='IsCheckReso', description='Indicates whether the resolution is checked.

*   **true**: The resolution is checked.
*   **false**: The resolution is not checked.
*   Default value: **false**.

> If this feature is enabled and the system detects that the resolution of the output file is higher than that of the input file based on the width or height, the resolution of the input file is retained after transcoding.', example='true'),
      isCheckResoFail?: string(name='IsCheckResoFail', description='Indicates whether the resolution is checked.

*   **true**: The resolution is checked.
*   **false**: The resolution is not checked.
*   Default value: **false**.

> If this feature is enabled and the system detects that the resolution of the output file is higher than that of the input file based on the width or height, an error that indicates a transcoding failure is returned.', example='true'),
      isCheckVideoBitrate?: string(name='IsCheckVideoBitrate', description='Indicates whether the video bitrate is checked.

*   **true**: The video bitrate is checked.
*   **false**: The video bitrate is not checked.
*   Default value: **false**.

> If this feature is enabled and the system detects that the video bitrate of the output file is greater than that of the input file, the video bitrate of the input file is retained after transcoding.', example='true'),
      isCheckVideoBitrateFail?: string(name='IsCheckVideoBitrateFail', description='Indicates whether the video bitrate is checked. If this feature is enabled and the system detects that the video bitrate of the output file is higher than that of the input file, the input file is not transcoded. This parameter has a higher priority than the IsCheckVideoBitrate parameter.

*   **true**: The video bitrate is checked. In this case, if the video bitrate of the output file is higher than that of the input file, the input file is not transcoded.
*   **false**: The video bitrate is not checked.
*   Default value: **false**.', example='true'),
      transMode?: string(name='TransMode', description='The transcoding mode. Valid values:

*   **onepass**
*   **twopass**
*   **CBR**
*   Default value: **onepass**.', example='onepass'),
    }(name='TransConfig', description='The general transcoding settings.'),
    video?: {
      bitrate?: string(name='Bitrate', description='The bitrate of the output video. Unit: Kbit/s.', example='500'),
      bitrateBnd?: {
        max?: string(name='Max', description='The maximum bitrate.', example='1500'),
        min?: string(name='Min', description='The minimum bitrate.', example='800'),
      }(name='BitrateBnd', description='The bitrate range of the video.'),
      bufsize?: string(name='Bufsize', description='The size of the buffer.

*   Default value: **6000**.
*   Unit: KB.', example='6000'),
      codec?: string(name='Codec', description='The video codec. Valid values: H.264, H.265, GIF, and WebP. Default value: **H.264**.', example='H.264'),
      crf?: string(name='Crf', description='The constant rate factor. Default value if the video codec is set to H.264: **23**. Default value if the video codec is set to H.265: **26**.

> If this parameter is specified, the setting of the Bitrate parameter becomes invalid.', example='15'),
      crop?: string(name='Crop', description='The method of video cropping. Valid values:

*   **border**: automatically detects and removes borders.
*   **Value in the format of width:height:left:top**: crops the video image based on the custom settings. Example: 1280:800:0:140.', example='border'),
      degrain?: string(name='Degrain', description='The level of quality control on the video.', example='10'),
      fps?: string(name='Fps', description='The frame rate. Default value: the frame rate of the input file. The value is 60 if the frame rate of the input file exceeds 60. Unit: frames per second.', example='25'),
      gop?: string(name='Gop', description='The GOP size. The GOP size can be the maximum interval of keyframes or the maximum number of frames in a frame group. If the maximum interval is specified, the value contains the unit (s). If the maximum number of frames is specified, the value does not contain a unit. Default value: **10s**.', example='10s'),
      hdr2sdr?: string(name='Hdr2sdr', description='Indicates whether the HDR2SDR conversion feature is enabled. If this feature is enabled, high dynamic range (HDR) videos are transcoded to standard dynamic range (SDR) videos.', example='true'),
      height?: string(name='Height', description='The height of the video.

*   Unit: pixel.
*   Default value: the height of the input video.', example='800'),
      longShortMode?: string(name='LongShortMode', description='Indicates whether the auto-rotate screen feature is enabled. Default value: **false**. Valid values:

*   **true**: The auto-rotate screen feature is enabled.
*   **false**: The auto-rotate screen feature is disabled.

> If this feature is enabled, the width of the output video corresponds to the long side of the input video, which is the height of the input video in portrait mode. The height of the output video corresponds to the short side of the input video, which is the width of the input video in portrait mode.', example='false'),
      maxFps?: string(name='MaxFps', description='The maximum frame rate.', example='60'),
      maxrate?: string(name='Maxrate', description='The maximum bitrate of the video. Unit: Kbit/s.', example='500'),
      narrowBand?: {
        abrmax?: float(name='Abrmax', description='The upper limit of the dynamic bitrate. If this parameter is set, the average bitrate is in the range of (0, 1000000].', example='3000'),
        maxAbrRatio?: float(name='MaxAbrRatio', description='The maximum ratio of the upper limit of dynamic bitrate. If this parameter is set, the value of Abrmax does not exceed x times of the source video bitrate. Valid values: (0,1.0].', example='1.0'),
        version?: string(name='Version', description='The Narrowband HD version. Only 1.0 may be returned.', example='1.0'),
      }(name='NarrowBand', description='The Narrowband HD settings.'),
      pad?: string(name='Pad', description='The black borders to be added to the video. The value is in the width:height:left:top format.', example='1280:800:0:140'),
      pixFmt?: string(name='PixFmt', description='The pixel format. Standard pixel formats such as yuv420p and yuvj420p are supported. The default pixel format can be **yuv420p** or the pixel format of the input video.', example='yuv420p'),
      preset?: string(name='Preset', description='The preset video algorithm. Default value: **medium**. Valid values:

*   **veryfast**
*   **fast**
*   **medium**
*   **slow**
*   **slower**

> This parameter is valid only if the Codec parameter is set to H.264.', example='fast'),
      profile?: string(name='Profile', description='The codec profile.

*   **baseline**: suitable for mobile devices
*   **main**: suitable for standard-definition devices
*   **high**: suitable for high-definition devices
*   Default value: **high**.

If multiple definitions are available, we recommend that you set this parameter to baseline for the lowest definition to ensure normal playback on low-end devices. Set this parameter to main or high for other definitions.

> This parameter is valid only if the Codec parameter is set to H.264.', example='high'),
      qscale?: string(name='Qscale', description='The level of the independent denoising algorithm.', example='1'),
      remove?: string(name='Remove', description='Indicates whether the video stream is deleted.

*   **true**: The video stream is deleted.
*   **false**: The video stream is retained.
*   Default value: **false**.', example='false'),
      resoPriority?: string(name='ResoPriority', description='The policy of resolution adjustment.', example='0'),
      scanMode?: string(name='ScanMode', description='The scan mode. Valid values:

*   **interlaced**
*   **progressive**', example='interlaced'),
      width?: string(name='Width', description='The width of the video.

*   Default value: the width of the input video.****
*   Unit: pixel.', example='256'),
    }(name='Video', description='The video codec configurations.'),
  }(name='Template', description='The details of the transcoding template.'),
}

model AddTemplateResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: AddTemplateResponseBody(name='body'),
}

/**
  * When you call this operation, you need to set transcoding parameters such as those related to the container format, video stream, and audio stream. If you do not specify some parameters, streams that are generated by using the template do not contain the information specified by those parameters.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function addTemplate(request: AddTemplateRequest): AddTemplateResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'AddTemplate', 'POST', '/', 'json', false, 'json', request);
}

model AddWaterMarkTemplateRequest {
  config: string(name='Config', description='The configuration of the watermark template. The value is a JSON object. For more information, see the "WaterMarks" section of the [Parameter details](~~29253~~) topic.

> If you do not require a positive correlation between the size of text in the watermark and the resolution, you can enable adaptation for the watermark. To do so, add `[\\"adaptive\\"]=true` to the TextWaterMark parameter.', example='{"Width":"10","Height":"30","Dx":"10","Dy":"5","ReferPos":"TopRight","Type":"Image","Timeline":{"Start":"0","Duration":"10"}}', position='Query'),
  name: string(name='Name', description='The name of the watermark template. The value can contain letters and digits and can be up to 128 bytes in size.', example='example-watermark-****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model AddWaterMarkTemplateResponseBody = {
  requestId?: string(name='RequestId', description='The ID of the request.', example='54BB917F-DD35-4F32-BABA-E60E31B21W63'),
  waterMarkTemplate?: {
    dx?: string(name='Dx', description='The horizontal offset. Unit: pixel.', example='10'),
    dy?: string(name='Dy', description='The vertical offset. Unit: pixel.', example='5'),
    height?: string(name='Height', description='The height of the watermark image. Unit: pixel.', example='30'),
    id?: string(name='Id', description='The ID of the watermark template. We recommend that you keep this ID for subsequent operation calls.', example='3780bd69b2b74540bc7b1096f564****'),
    name?: string(name='Name', description='The name of the watermark template.', example='example-watermark-****'),
    ratioRefer?: {
      dx?: string(name='Dx', description='The horizontal offset of the watermark relative to the output video image. Default value: **0**. The default value indicates no offset. The value can be an integer or a decimal.

*   **Integer**: the vertical offset. This indicates the absolute position. Unit: pixel.
*   **Decimal**: the ratio of the horizontal offset to the width of the output video. The ratio varies based on the size of the video. Four decimal places are supported, such as 0.9999. More decimal places are discarded.', example='0.51'),
      dy?: string(name='Dy', description='The vertical offset of the watermark relative to the output video image. Default value: **0**. The default value indicates no offset. The value can be an integer or a decimal.

*   **Integer**: the vertical offset. This indicates the absolute position. Unit: pixel.
*   **Decimal**: the ratio of the vertical offset to the height of the output video. The ratio varies based on the size of the video. Four decimal places are supported, such as 0.9999. More decimal places are discarded.', example='0.28'),
      height?: string(name='Height', description='The height of the watermark image in the output video. The value can be an integer or a decimal.

*   **Integer**: the height of the watermark image. This indicates the absolute position. Unit: pixel.
*   **Decimal**: the ratio of the height of the watermark image to the height of the output video. The ratio varies based on the size of the video. Four decimal places are supported, such as 0.9999. More decimal places are discarded.', example='0.33'),
      width?: string(name='Width', description='The width of the watermark image in the output video. The value can be an integer or a decimal.

*   **Integer**: the width of the watermark image. This indicates the absolute position. Unit: pixel.
*   **Decimal**: the ratio of the width of the watermark image to the width of the output video. The ratio varies based on the size of the video. Four decimal places are supported, such as 0.9999. More decimal places are discarded.', example='0.36'),
    }(name='RatioRefer', description='The values of the Height, Width, Dx, and Dy parameters relative to the reference edges. If the values of the Height, Width, Dx, and Dy parameters are decimals between 0 and 1, the values are calculated by referring to the following edges in sequence:

*   **Width**: the width edge.
*   **Height**: the height edge.
*   **Long**: the long edge.
*   **Short**: the short edge.'),
    referPos?: string(name='ReferPos', description='The position of the watermark. Valid values:

*   **TopRight**: the upper-right corner.
*   **TopLeft**: the upper-left corner.
*   **BottomRight**: the lower-right corner.
*   **BottomLeft**: the lower-left corner.', example='TopRight'),
    state?: string(name='State', description='The status of the watermark template.

*   **Normal**: The watermark template is normal.
*   **Deleted**: The watermark template is deleted.', example='Normal'),
    timeline?: {
      duration?: string(name='Duration', description='The display duration of the watermark. Default value: **ToEND**. The default value indicates that the watermark is displayed until the video ends.', example='ToEND'),
      start?: string(name='Start', description='The beginning of the time range during which the watermark is displayed.

*   Unit: seconds.
*   Default value: **0**.', example='0'),
    }(name='Timeline', description='The timeline of the watermark.'),
    type?: string(name='Type', description='The type of the watermark. Valid values:

*   Image: an image watermark.
*   Text: a text watermark.', example='Image'),
    width?: string(name='Width', description='The width of the watermark image. Unit: pixel.', example='10'),
  }(name='WaterMarkTemplate', description='The details of the watermark template.'),
}

model AddWaterMarkTemplateResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: AddWaterMarkTemplateResponseBody(name='body'),
}

/**
  * After you create a watermark template by calling this operation, you can specify the watermark template and watermark asset when you [submit a transcoding job](~~29226~~). This allows you to add watermark information to the output video.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function addWaterMarkTemplate(request: AddWaterMarkTemplateRequest): AddWaterMarkTemplateResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'AddWaterMarkTemplate', 'POST', '/', 'json', false, 'json', request);
}

model BindInputBucketRequest {
  bucket: string(name='Bucket', description='The name of the input media bucket to be bound. The name can be up to 64 bytes in size. To obtain the media bucket name, you can log on to the **ApsaraVideo Media Processing (MPS) console** and choose **Workflows** > **Media Buckets** in the left-side navigation pane.

> The bucket name can contain lowercase letters, digits, and hyphens (-), and cannot start or end with a hyphen (-).', example='example-bucket-****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  referer?: string(name='Referer', description='The settings of Object Storage Service (OSS) hotlink protection. For more information, see [Hotlink protection](~~31869~~).', example='http://www.example.com', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model BindInputBucketResponseBody = {
  requestId?: string(name='RequestId', description='The ID of the request.', example='4AEA0480-32F4-1656-92B3-F4D4CDE6BBB3'),
}

model BindInputBucketResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: BindInputBucketResponseBody(name='body'),
}

/**
  * Before you call this operation to bind an input media bucket, you must create a media bucket. For more information, see [Add media buckets](~~42430~~).
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function bindInputBucket(request: BindInputBucketRequest): BindInputBucketResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'BindInputBucket', 'POST', '/', 'json', false, 'json', request);
}

model BindOutputBucketRequest {
  bucket: string(name='Bucket', description='The name of the Object Storage Service (OSS) bucket that you want to bind. The name can be up to 64 bytes in size and can contain letters, digits, and hyphens (-). The name cannot start with a special character.', example='example-bucket-****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model BindOutputBucketResponseBody = {
  requestId?: string(name='RequestId', description='The ID of the request.', example='D0F80646-90D4-402F-9D56-CEFEAA6BCC9B'),
}

model BindOutputBucketResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: BindOutputBucketResponseBody(name='body'),
}

/**
  * Before you call this operation to bind an output media bucket to the media library, you must create a media bucket. For more information, see [Add media buckets](~~42430~~).
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function bindOutputBucket(request: BindOutputBucketRequest): BindOutputBucketResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'BindOutputBucket', 'POST', '/', 'json', false, 'json', request);
}

model CancelJobRequest {
  jobId: string(name='JobId', description='The ID of the transcoding job to be canceled. You can log on to the **MPS console** and click **Tasks** in the left-side navigation pane to obtain job IDs. Alternatively, you can obtain job IDs from the response of the [SubmitJobs](~~29226~~) operation.', example='d1ce4d3efcb549419193f50f1fcd****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model CancelJobResponseBody = {
  jobId?: string(name='JobId', description='The ID of the job.', example='d1ce4d3efcb549419193f50f1fcd****'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='338CA33A-AE83-5DF4-B6F2-C6D3ED8143F5'),
}

model CancelJobResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: CancelJobResponseBody(name='body'),
}

/**
  * *   You can cancel a transcoding job only if the job is in the Submitted state.
  * *   We recommend that you call the **UpdatePipeline** operation to set the status of the ApsaraVideo Media Processing (MPS) queue to Paused before you cancel a job. This suspends job scheduling in the MPS queue. After the job is canceled, you must set the status of the MPS queue back to Active so that the other jobs in the MPS queue can be scheduled.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function cancelJob(request: CancelJobRequest): CancelJobResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'CancelJob', 'POST', '/', 'json', false, 'json', request);
}

model CreateCustomEntityRequest {
  algorithm: string(name='Algorithm', example='landmark', position='Query'),
  customEntityInfo?: string(name='CustomEntityInfo', example='{ "finegrainName":"examplName" }', position='Query'),
  customEntityName: string(name='CustomEntityName', position='Query'),
  customGroupId: string(name='CustomGroupId', example='1', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model CreateCustomEntityResponseBody = {
  customEntityId?: string(name='CustomEntityId', example='1'),
  requestId?: string(name='RequestId', example='580e8ce3-3b80-44c5-9f3f-36ac3cc5bdd5'),
}

model CreateCustomEntityResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: CreateCustomEntityResponseBody(name='body'),
}

async function createCustomEntity(request: CreateCustomEntityRequest): CreateCustomEntityResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'CreateCustomEntity', 'POST', '/', 'json', false, 'json', request);
}

model CreateCustomGroupRequest {
  algorithm: string(name='Algorithm', example='landmark', position='Query'),
  customGroupDescription?: string(name='CustomGroupDescription', position='Query'),
  customGroupName: string(name='CustomGroupName', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model CreateCustomGroupResponseBody = {
  customGroupId?: string(name='CustomGroupId', example='129****'),
  requestId?: string(name='RequestId', example='580e8ce3-3b80-44c5-9f3f-36ac3cc5bdd5'),
}

model CreateCustomGroupResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: CreateCustomGroupResponseBody(name='body'),
}

async function createCustomGroup(request: CreateCustomGroupRequest): CreateCustomGroupResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'CreateCustomGroup', 'POST', '/', 'json', false, 'json', request);
}

model CreateFpShotDBRequest {
  config?: string(name='Config', description='The configurations of the media fingerprint library. By default, this parameter is left empty. You can customize the configurations based on your business requirements. The value must be a string in the JSON format.', example='null', position='Query'),
  description?: string(name='Description', description='The description of the media fingerprint library.', example='The library is a text fingerprint library.', position='Query'),
  modelId?: int32(name='ModelId', description='The model ID of the media fingerprint library. To create a text fingerprint library, set the parameter to **11**. To create a video fingerprint library, set the parameter to **12**. To create an audio fingerprint library, set the parameter to **13**. To create an image fingerprint library, set the parameter to **14**.', example='11', position='Query'),
  name: string(name='Name', description='The name of the media fingerprint library.', example='example name', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model CreateFpShotDBResponseBody = {
  fpShotDB?: {
    config?: string(name='Config', description='The configurations of the media fingerprint library.', example='null'),
    description?: string(name='Description', description='The description of the media fingerprint library.', example='The library is a text fingerprint library.'),
    fpDBId?: string(name='FpDBId', description='The ID of the media fingerprint library. We recommend that you keep this ID for subsequent operation calls.', example='88c6ca184c0e47098a5b665e2a12****'),
    modelId?: int32(name='ModelId', description='The model ID of the media fingerprint library.', example='11'),
    name?: string(name='Name', description='The name of the media fingerprint library.', example='example-name-****'),
    state?: string(name='State', description='The status of the media fingerprint library. After the media fingerprint library is created, it enters the **offline** state. After the media fingerprint library is processed at the backend, it enters the **active** state.', example='offline'),
  }(name='FpShotDB', description='The details of the media fingerprint library.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='25818875-5F78-4A13-BEF6-D7393642CA58'),
}

model CreateFpShotDBResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: CreateFpShotDBResponseBody(name='body'),
}

/**
  * *   You can call this operation to submit a job to create a video or text fingerprint library. You can use a text fingerprint library to store fingerprints for text.
  * *   You can submit a job of creating a text fingerprint library only in the China (Shanghai) region.
  * *   By default, you can submit up to 10 jobs of creating a video fingerprint library to an ApsaraVideo Media Processing (MPS) queue at a time. If you submit more than 10 jobs at a time, the call may fail.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function createFpShotDB(request: CreateFpShotDBRequest): CreateFpShotDBResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'CreateFpShotDB', 'POST', '/', 'json', false, 'json', request);
}

model DeactivateMediaWorkflowRequest {
  mediaWorkflowId: string(name='MediaWorkflowId', description='The ID of the media workflow that is deactivated.', example='93ab850b4f6f44eab54b6e9181d4****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model DeactivateMediaWorkflowResponseBody = {
  mediaWorkflow?: {
    creationTime?: string(name='CreationTime', description='*   After you deactivate a media workflow, you can modify the workflow information.
*   After you delete or deactivate a media workflow, the workflow cannot be used. In this case, the workflow is not automatically triggered when you upload a file to the bucket specified by the workflow.

## Limits on QPS

You can call this operation up to 100 times per second. If the number of the calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).', example='2016-04-01T05:29:37Z'),
    mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the media workflow that you want to deactivate. To obtain the ID of the media workflow, you can log on to the **ApsaraVideo Media Processing (MPS) console** and choose **Workflows** > **Workflow Settings** in the left-side navigation pane.', example='93ab850b4f6f44eab54b6e9181d4****'),
    name?: string(name='Name', description='The details of the media workflow.', example='example-mediaworkflow-****'),
    state?: string(name='State', description='The topology of the media workflow.The status of the media workflow. The value is **Inactive**.', example='Inactive'),
    topology?: string(name='Topology', description='The status of the media workflow. The value is **Inactive**.', example='{mediaworkflow","State":"Active","Topology":"{\\"Activities\\":{\\"Act-Start\\":{\\"Parameters\\":{\\"PipelineId\\":\\"130266f58161436a80bf07cb12c8****\\",\\"InputFile\\":\\"{\\\\\\"Bucket\\\\\\": \\\\\\"example-bucket-****\\\\\\",\\\\\\"Location\\\\\\": \\\\\\"cn-shanghai\\\\\\"}\\"},\\"Type\\":\\"Start\\"},\\"Act-Report\\":{\\"Parameters\\":{},\\"Type\\":\\"Report\\"},\\"Act-Transcode-M3U8\\":{\\"Parameters\\":{\\"Outputs\\":\\"[{\\\\\\"Object\\\\\\":\\\\\\"transcode/{ObjectPrefix}{FileName}\\\\\\",\\\\\\"TemplateId\\\\\\": \\\\\\"957d1719ee85ed6527b90cf62726****\\\\\\"}]\\",\\"OutputBucket\\":\\"example-bucket-****\\",\\"OutputLocation\\":\\"cn-shanghai\\"},\\"Type\\":\\"Transcode\\"}},\\"Dependencies\\":{\\"Act-Start\\":[\\"Act-Transcode-M3U8\\"],\\"Act-Report\\":[],\\"Act-Transcode-M3U8\\":[\\"Act-Report\\"]}}","MediaWorkflowId":"93ab850b4f6f44eab54b6e91d24d****"}]},"RequestId":"16CD0CDD-457E-420D-9755-8385075A1234"}'),
  }(name='MediaWorkflow', description='The topology of the media workflow.'),
  requestId?: string(name='RequestId', description='The name of the media workflow that is deactivated.', example='16CD0CDD-457E-420D-9755-8385075A1234'),
}

model DeactivateMediaWorkflowResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: DeactivateMediaWorkflowResponseBody(name='body'),
}

/**
  * The time when the media workflow was created.
  *
 */
async function deactivateMediaWorkflow(request: DeactivateMediaWorkflowRequest): DeactivateMediaWorkflowResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'DeactivateMediaWorkflow', 'POST', '/', 'json', false, 'json', request);
}

model DeleteCustomEntityRequest {
  algorithm: string(name='Algorithm', example='landmark', position='Query'),
  customEntityId: string(name='CustomEntityId', example='1', position='Query'),
  customGroupId: string(name='CustomGroupId', example='1', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model DeleteCustomEntityResponseBody = {
  requestId?: string(name='RequestId', example='580e8ce3-3b80-44c5-9f3f-36ac3cc5bdd5'),
}

model DeleteCustomEntityResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: DeleteCustomEntityResponseBody(name='body'),
}

async function deleteCustomEntity(request: DeleteCustomEntityRequest): DeleteCustomEntityResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'DeleteCustomEntity', 'POST', '/', 'json', false, 'json', request);
}

model DeleteCustomGroupRequest {
  algorithm: string(name='Algorithm', description='The custom type of the image library. Valid values:

*   landmark: custom landmarks.
*   object: custom objects.', example='landmark', position='Query'),
  customGroupId: string(name='CustomGroupId', description='The ID of the custom image library.', example='1', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model DeleteCustomGroupResponseBody = {
  requestId?: string(name='RequestId', description='The ID of the request. This parameter is unique.', example='580e8ce3-3b80-44c5-9f3f-36ac3cc5bdd5'),
}

model DeleteCustomGroupResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: DeleteCustomGroupResponseBody(name='body'),
}

/**
  * You can call this operation only in the China (Beijing), China (Shanghai), and China (Hangzhou) regions.
  * ### QPS limit
  * You can call this operation up to 50 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function deleteCustomGroup(request: DeleteCustomGroupRequest): DeleteCustomGroupResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'DeleteCustomGroup', 'POST', '/', 'json', false, 'json', request);
}

model DeleteCustomViewRequest {
  algorithm: string(name='Algorithm', example='landmark', position='Query'),
  customEntityId: string(name='CustomEntityId', example='1', position='Query'),
  customGroupId: string(name='CustomGroupId', example='1', position='Query'),
  customViewId: string(name='CustomViewId', example='1', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model DeleteCustomViewResponseBody = {
  requestId?: string(name='RequestId', example='580e8ce3-3b80-44c5-9f3f-36ac3cc5bdd5'),
}

model DeleteCustomViewResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: DeleteCustomViewResponseBody(name='body'),
}

async function deleteCustomView(request: DeleteCustomViewRequest): DeleteCustomViewResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'DeleteCustomView', 'POST', '/', 'json', false, 'json', request);
}

model DeleteMediaRequest {
  mediaIds: string(name='MediaIds', description='The IDs of the media files that you want to remove. Separate multiple IDs with commas (,). You can remove up to 10 media files at a time.

> You can obtain the ID of the media file from the response parameters of the [AddMedia](~~44458~~) operation. Alternatively, you can log on to the MPS console. In the left-side navigation pane, choose **Media Management** > **Media List**. Find the required video and click **Manage** in the Actions column. The ID of the video is displayed on the Basics tab.', example='3e1cd21131a94525be55acf65888****,3e6149d5a8c944c09b1a8d2dc3e4****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model DeleteMediaResponseBody = {
  requestId?: string(name='RequestId', description='The ID of the request.', example='05F8B913-E9F3-4A6F-9922-48CADA0FFAAD'),
}

model DeleteMediaResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: DeleteMediaResponseBody(name='body'),
}

/**
  * This operation allows you to logically delete a media file. The media file can no longer be processed, but the corresponding objects in the input and output Object Storage Service (OSS) buckets are retained.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function deleteMedia(request: DeleteMediaRequest): DeleteMediaResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'DeleteMedia', 'POST', '/', 'json', false, 'json', request);
}

model DeleteMediaTagRequest {
  mediaId: string(name='MediaId', description='The ID of the media file for which you want to remove a tag. To obtain the ID of a media file, you can call the [AddMedia](~~44458~~) operation. Alternatively, perform the following operations in the ApsaraVideo Media Processing (MPS) console: In the left-side navigation pane, choose **Media Management** > **Media List**. Find the required video and click **Manage** in the Actions column. The ID of the video is displayed on the Basics tab.', example='3e6149d5a8c944c09b1a8d2dc3e4****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  tag?: string(name='Tag', description='The media tag that you want to remove. The value is encoded in UTF-8 and can be up to 32 bytes in length.

> You can remove only one tag at a time.', example='tag1', position='Query'),
}

model DeleteMediaTagResponseBody = {
  requestId?: string(name='RequestId', description='The ID of the request.', example='283DC68C-146F-4489-A2A1-2F88F1472A56'),
}

model DeleteMediaTagResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: DeleteMediaTagResponseBody(name='body'),
}

/**
  * You can call this operation to remove only one tag at a time.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function deleteMediaTag(request: DeleteMediaTagRequest): DeleteMediaTagResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'DeleteMediaTag', 'POST', '/', 'json', false, 'json', request);
}

model DeleteMediaWorkflowRequest {
  mediaWorkflowId: string(name='MediaWorkflowId', description='The ID of the media workflow that you want to delete. To obtain the ID of the media workflow, you can log on to the **ApsaraVideo Media Processing (MPS) console** and choose **Workflows** > **Workflow Settings** in the left-side navigation pane.', example='93ab850b4f6f44eab54b6e9181d4****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model DeleteMediaWorkflowResponseBody = {
  mediaWorkflow?: {
    creationTime?: string(name='CreationTime', description='The time when the media workflow was created.', example='2016-04-01T05:29:37Z'),
    mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the media workflow that is deleted.', example='93ab850b4f6f44eab54b6e9181d4****'),
    name?: string(name='Name', description='The name of the media workflow that is deleted.', example='example-mediaworkflow-****'),
    state?: string(name='State', description='The status of the media workflow. The value is **Deleted**.', example='Deleted'),
    topology?: string(name='Topology', description='The topology of the media workflow.', example='{mediaworkflow","State":"Active","Topology":"{\\"Activities\\":{\\"Act-Start\\":{\\"Parameters\\":{\\"PipelineId\\":\\"130266f58161436a80bf07cb12c8****\\",\\"InputFile\\":\\"{\\\\\\"Bucket\\\\\\": \\\\\\"example-bucket-****\\\\\\",\\\\\\"Location\\\\\\": \\\\\\"cn-shanghai\\\\\\"}\\"},\\"Type\\":\\"Start\\"},\\"Act-Report\\":{\\"Parameters\\":{},\\"Type\\":\\"Report\\"},\\"Act-Transcode-M3U8\\":{\\"Parameters\\":{\\"Outputs\\":\\"[{\\\\\\"Object\\\\\\":\\\\\\"transcode/{ObjectPrefix}{FileName}\\\\\\",\\\\\\"TemplateId\\\\\\": \\\\\\"957d1719ee85ed6527b90cf62726****\\\\\\"}]\\",\\"OutputBucket\\":\\"example-bucket-****\\",\\"OutputLocation\\":\\"cn-shanghai\\"},\\"Type\\":\\"Transcode\\"}},\\"Dependencies\\":{\\"Act-Start\\":[\\"Act-Transcode-M3U8\\"],\\"Act-Report\\":[],\\"Act-Transcode-M3U8\\":[\\"Act-Report\\"]}}","MediaWorkflowId":"93ab850b4f6f44eab54b6e91d24d****"}]},"RequestId":"16CD0CDD-457E-420D-9755-8385075A1234"}'),
  }(name='MediaWorkflow', description='The information about the media workflow.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='7D752035-97DA-54E5-88E2-E8405EEA4394'),
}

model DeleteMediaWorkflowResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: DeleteMediaWorkflowResponseBody(name='body'),
}

/**
  * After you delete or disable a workflow, the workflow cannot be used. In this case, the workflow is not automatically triggered when you upload a file to the bucket specified by the workflow.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function deleteMediaWorkflow(request: DeleteMediaWorkflowRequest): DeleteMediaWorkflowResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'DeleteMediaWorkflow', 'POST', '/', 'json', false, 'json', request);
}

model DeletePipelineRequest {
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  pipelineId: string(name='PipelineId', description='The ID of the MPS queue that you want to delete. To obtain the ID of the MPS queue, you can log on to the **MPS console** and choose **Global Settings** > **Pipelines** in the left-side navigation pane.', example='d1ce4d3efcb549419193f50f1fcd****', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model DeletePipelineResponseBody = {
  pipelineId?: string(name='PipelineId', description='The ID of the MPS queue that is deleted.', example='d1ce4d3efcb549419193f50f1fcd****'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='338CA33A-AE83-5DF4-B6F2-C6D3ED8143F5'),
}

model DeletePipelineResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: DeletePipelineResponseBody(name='body'),
}

/**
  * You can call this operation to delete only one MPS queue at a time.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function deletePipeline(request: DeletePipelineRequest): DeletePipelineResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'DeletePipeline', 'POST', '/', 'json', false, 'json', request);
}

model DeleteSmarttagTemplateRequest {
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  templateId: string(name='TemplateId', description='The ID of the template that you want to delete. You can obtain the template ID from the response of the [AddSmarttagTemplate](~~187759~~) operation.', example='05de22f255284c7a8d2aab535dde****', position='Query'),
}

model DeleteSmarttagTemplateResponseBody = {
  requestId?: string(name='RequestId', description='The ID of the request.', example='5F37036F-5267-43F1-AE47-10A18E840739'),
}

model DeleteSmarttagTemplateResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: DeleteSmarttagTemplateResponseBody(name='body'),
}

/**
  * You can call this operation to delete only one template at a time.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped, and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function deleteSmarttagTemplate(request: DeleteSmarttagTemplateRequest): DeleteSmarttagTemplateResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'DeleteSmarttagTemplate', 'POST', '/', 'json', false, 'json', request);
}

model DeleteTemplateRequest {
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  templateId: string(name='TemplateId', description='The ID of the custom transcoding template that you want to delete. To obtain the ID of the custom transcoding template, you can log on to the **ApsaraVideo Media Processing (MPS) console** and choose **Global Settings** > **Encoding Templates** in the left-side navigation pane.', example='16f01ad6175e4230ac42bb5182cd****', position='Query'),
}

model DeleteTemplateResponseBody = {
  requestId?: string(name='RequestId', description='The ID of the request.', example='2247541A-9F27-47EE-B6EC-484B5475****'),
  templateId?: string(name='TemplateId', description='The ID of the custom transcoding template that is deleted.', example='16f01ad6175e4230ac42bb5182cd****'),
}

model DeleteTemplateResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: DeleteTemplateResponseBody(name='body'),
}

/**
  * A custom transcoding template cannot be deleted if it is being used by a job that has been submitted.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function deleteTemplate(request: DeleteTemplateRequest): DeleteTemplateResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'DeleteTemplate', 'POST', '/', 'json', false, 'json', request);
}

model DeleteWaterMarkTemplateRequest {
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  waterMarkTemplateId: string(name='WaterMarkTemplateId', description='The ID of the watermark template that you want to delete. To obtain the template ID, you can log on to the **ApsaraVideo Media Processing (MPS) console** and choose **Global Settings** > **Watermark Templates** in the left-side navigation pane.', example='3780bd69b2b74540bc7b1096f564****', position='Query'),
}

model DeleteWaterMarkTemplateResponseBody = {
  requestId?: string(name='RequestId', description='The ID of the request.', example='030E2671-806A-52AF-A93C-DA8E308603A6'),
  waterMarkTemplateId?: string(name='WaterMarkTemplateId', description='The ID of the deleted watermark template.', example='3780bd69b2b74540bc7b1096f564****'),
}

model DeleteWaterMarkTemplateResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: DeleteWaterMarkTemplateResponseBody(name='body'),
}

/**
  * A watermark template cannot be deleted if it is being used by a submitted job.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function deleteWaterMarkTemplate(request: DeleteWaterMarkTemplateRequest): DeleteWaterMarkTemplateResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'DeleteWaterMarkTemplate', 'POST', '/', 'json', false, 'json', request);
}

model ImAuditRequest {
  bizType?: string(name='BizType', description='The business type. By default, the public business type is used.', example='139440480445****', position='Query'),
  contents?: string(name='Contents', description='The custom text entries. You can specify up to 5 text entries. The value must be a JSON array. You must specify at least one of the Images and Contents parameters.', example='\\["Hello","Who are you","Where am I"]', position='Query'),
  images?: string(name='Images', description='The image URLs. You can specify up to 5 image URLs. The value must be a JSON array. To view the URLs of the images, you can log on to the **ApsaraVideo Media Processing (MPS) console** and choose **Media Management** > **Media List** in the left-side navigation pane. You must set at least one of the Images and Contents parameters. The image to be moderated must meet the following limits. Otherwise, the moderation task may fail.

*   The image size cannot exceed 20 MB, the height or width of the image cannot exceed 30,000 pixels, and the image cannot exceed 0.25 billion pixels.
*   We recommend that you upload images of at least 256 × 256 pixels to ensure required moderation result.', example='["http://127.66.**.**/image.jpeg","http://127.66.**.**/photo.jpeg"]', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  scenes: string(name='Scenes', description='The moderation scenarios. Separate multiple scenarios with commas (,). For example, if you specify {"porn","terrorism"} for this parameter, both pornographic content detection and terrorist content detection are performed on the images and text. Valid values:

*   porn: pornography
*   terrorism: terrorist content
*   ad: ad violation
*   qrcode: QR code
*   live: undesirable scene
*   logo: special logo
*   antispam: text anti-spam (valid only for text)', example='["porn","terrorism","ad"]', position='Query'),
}

model ImAuditResponseBody = {
  imageQuotaExceed?: boolean(name='ImageQuotaExceed', description='Indicates whether the image moderation QPS exceeds the limit. Valid values: true and false. A value of true indicates that the QPS does not exceed the limit. A value of false indicates that the QPS exceeds the limit.', example='false'),
  imageResults?: {
    result?: [ 
      {
        code?: long(name='code', description='The error code. The error code is the same as the HTTP status code. This parameter is not returned if the request is successful.', example='200'),
        dataId?: string(name='dataId', description='The ID of the moderated object.

>  If you set the dataId parameter in the moderation request, the dataId parameter is returned in the response.', example='uuid-1234-1234-1234'),
        extras?: map[string]any(name='extras', description='The additional information about the image. If ad is specified for the Scenes parameter, the following content may be returned for this parameter: hitLibInfo: the information about the custom text library that is hit by the text in the image. The value of this parameter is an array. For more information about the structure, see [hitLibInfo](~~268644~~).'),
        msg?: string(name='msg', description='The message that is returned for the request.', example='ok'),
        results?: [ 
          {
            label?: string(name='Label', description='The category of the moderation results. Valid values vary based on the specified moderation scenario.

*   If the Scenes parameter is set to porn, the valid values are:

    *   normal: no pornographic content
    *   sexy: sexy content
    *   porn: pornographic content

*   If the Scenes parameter is set to terrorism, the valid values are:

    *   normal: no pornographic content
    *   bloody: bloody content
    *   explosion: explosions and smoke
    *   outfit: special costume
    *   logo: special logo
    *   weapon: weapon
    *   politics: political content
    *   violence: violence
    *   crowd: crowd
    *   parade: parade
    *   carcrash: car accident
    *   flag: flag
    *   location: landmark
    *   others: other content

*   If the Scenes parameter is set to ad, the valid values are:

    *   normal: no pornographic content
    *   ad: ad violation
    *   politics: politically sensitive content in text
    *   porn: pornographic content in text
    *   abuse: abuse in text
    *   terrorism: terrorist content in text
    *   contraband: prohibited content in text
    *   spam: junk content in text
    *   npx: illegal ad
    *   qrcode: QR code
    *   programCode: mini program code

*   If the Scenes parameter is set to qrcode, the valid values are:

    *   normal: no pornographic content
    *   qrcode: QR code
    *   programCode: mini program code

*   If the Scenes parameter is set to live, the valid values are:

    *   normal: no pornographic content
    *   meaningless: no content in the image, such as black or white screen
    *   PIP: picture-in-picture
    *   smoking: smoking
    *   drivelive: live broadcasting in a running vehicle

*   If the Scenes parameter is set to logo, the valid values are:

    *   normal: no pornographic content
    *   TV: controlled logo
    *   trademark: trademark', example='sexy'),
            rate?: double(name='Rate', description='The score of the confidence level. Valid values: 0 to 100. A greater value indicates a higher confidence level. If a value of pass is returned for the suggestion parameter, a higher confidence level indicates a higher probability that the content is normal. If a value of review or block is returned for the suggestion parameter, a higher confidence level indicates a higher probability that the content contains violations.

>  This score is for reference only. We strongly recommend that you do not use this score in your business. We recommend that you use the values that are returned for the suggestion, label, and sublabel parameters to determine whether the content contains violations. The sublabel parameter is returned by some operations.', example='91.54'),
            scene?: string(name='Scene', description='The image moderation scenario. Valid values:

*   porn: pornography
*   terrorism: terrorist content
*   ad: ad violation
*   qrcode: QR code
*   live: undesirable scene
*   logo: special logo', example='porn'),
            suggestion?: string(name='Suggestion', description='The recommended subsequent operation. Valid values:

*   pass: The content passes the moderation. No further actions are required.
*   review: The moderation object contains suspected violations and requires human review.
*   block: The moderation object contains violations. We recommend that you delete or block the object.', example='block'),
            frames?: [ 
              {
                rate?: float(name='rate', description='The score of the confidence level. Valid values: 0 to 100. A higher confidence level indicates higher reliability of the moderation result. We recommend that you do not use this score in your business.', example='89.85'),
                url?: string(name='url', description='The temporary access URL of the truncated frame. The URL is valid for 5 minutes.', example='http://example.com/test-01.jpg'),
              }
            ](name='frames', description='If the temporary access URL of the image is too long, a truncated temporary access URL is returned for each frame.'),
            hintWordsInfo?: [ 
              {
                context?: string(name='context', description='The term hit by the detected text.', example='Sensitive words'),
              }
            ](name='hintWordsInfo', description='The information about the term hit by the ad or violation text detected in the moderated image.'),
            logoData?: [ 
              {
                h?: float(name='h', description='The height of the logo area. Unit: pixel.', example='106'),
                name?: string(name='name', description='The name of the detected logo.', example='Hunan TV'),
                type?: string(name='type', description='The type of the detected logo. For example, a value of TV indicates a controlled media logo.', example='TV'),
                w?: float(name='w', description='The width of the logo area. Unit: pixel.', example='106'),
                x?: float(name='x', description='The distance between the upper-left corner of the logo area and the y-axis, with the upper-left corner of the image being the coordinate origin. Unit: pixel.', example='140'),
                y?: float(name='y', description='The distance between the upper-left corner of the logo area and the x-axis, with the upper-left corner of the image being the coordinate origin. Unit: pixel.', example='68'),
              }
            ](name='logoData', description='The information about the logo detected in the moderated image.'),
            ocrData?: [ string ](name='ocrData', description='ocrData'),
            programCodeData?: [ 
              {
                h?: float(name='h', description='The height of the mini program code area. Unit: pixel.', example='413.0'),
                w?: float(name='w', description='The width of the mini program code area. Unit: pixel.', example='402.0'),
                x?: float(name='x', description='The distance between the upper-left corner of the mini program code area and the y-axis, with the upper-left corner of the image being the coordinate origin. Unit: pixel.', example='11.0'),
                y?: float(name='y', description='The distance between the upper-left corner of the mini program code area and the x-axis, with the upper-left corner of the image being the coordinate origin. Unit: pixel.', example='0.0'),
              }
            ](name='programCodeData', description='The location information of the mini program code detected in the moderated image.'),
            qrcodeData?: [ string ](name='qrcodeData', description='The information about the text that is included in the QR code detected in the moderated image.'),
            qrcodeLocations?: [ 
              {
                h?: float(name='h', description='The height of the QR code area. Unit: pixel.', example='413.0'),
                qrcode?: string(name='qrcode', description='The URL that the detected QR code points to.', example='http://xxx'),
                w?: float(name='w', description='The width of the QR code area. Unit: pixel.', example='402.0'),
                x?: float(name='x', description='The distance between the upper-left corner of the QR code area and the y-axis, with the upper-left corner of the image being the coordinate origin. Unit: pixel.', example='11'),
                y?: float(name='y', description='The distance between the upper-left corner of the QR code area and the x-axis, with the upper-left corner of the image being the coordinate origin. Unit: pixel.', example='0'),
              }
            ](name='qrcodeLocations', description='The coordinates of the QR code detected in the image.'),
            sfaceData?: [ 
              {
                faces?: [ 
                  {
                    idid?: string(name='idid', description='The ID of the detected face. The value is a string.', example='AliFace_0001234'),
                    name?: string(name='name', description='This value is a string, which indicates the name of a similar person.', example='Name'),
                    re?: float(name='re', description='The score of the confidence level. The value is a float point number. Valid values: 0 to 100. A greater value indicates a higher confidence level for facial recognition.', example='91.54'),
                  }
                ](name='faces', description='The information about the face detected in the moderated image.'),
                h?: float(name='h', description='The height of the face area. Unit: pixel.', example='121'),
                w?: float(name='w', description='The width of the face area. Unit: pixel.', example='47'),
                x?: float(name='x', description='The distance between the upper-left corner of the face area and the y-axis, with the upper-left corner of the image being the coordinate origin. Unit: pixel.', example='49'),
                y?: float(name='y', description='The distance between the upper-left corner of the face area and the y-axis, with the upper-left corner of the image being the coordinate origin. Unit: pixel.', example='39'),
              }
            ](name='sfaceData', description='The information about the terrorist content detected in the moderated image.'),
          }
        ](name='results', description='The returned data. If the call is successful, the array in the returned results contains one or more elements. Each element is a struct.'),
        taskId?: string(name='taskId', description='The ID of the moderation task.', example='img4wlJcb7p4wH4lAP3111111-12****'),
        url?: string(name='url', description='The URL of the moderated object.', example='http://example.com/example-****.jpg'),
      }
    ](name='result', description='The image moderation results.'),
  }(name='ImageResults', description='The image moderation results. If the HTTP status code 200 is returned, the array in the returned results contains one or more elements. For more information about the parameters, see [Data returned by the ImAudit operation](~~268644~~).'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='5210DBB0-E327-4D45-ADBC-0B83C8793421'),
  textQuotaExceed?: boolean(name='TextQuotaExceed', description='Indicates whether the text moderation QPS exceeds the limit. Valid values: true and false.', example='false'),
  textResults?: {
    result?: [ 
      {
        code?: long(name='code', description='The error code. The error code is the same as the HTTP status code. For more information, see [Error codes](~~29254~~).', example='200'),
        content?: string(name='content', description='The text that you specify in the moderation request.', example='This is test text.'),
        dataId?: string(name='dataId', description='The sequence number of the text.', example='cfd33235-71a4-468b-8137-a5ffe323****'),
        msg?: string(name='msg', description='The message that is returned for the request.', example='OK'),
        results?: [ 
          {
            details?: [ 
              {
                label?: string(name='Label', description='The category of the risky content that the moderated text hits. Valid values:

*   spam: spam
*   ad: ad
*   politics: political content
*   terrorism: terrorist content
*   abuse: abuse
*   porn: pornographic content
*   flood: excessive junk content
*   contraband: prohibited content
*   meaningless: meaningless content
*   customized: custom content, such as a custom keyword', example='porn'),
                contexts?: [ 
                  {
                    context?: string(name='context', description='The term that the moderated text hits. If the text hits a term, the term is returned. If the text hits the algorithmic model, this parameter is not returned.', example='Door-to-door service'),
                    libCode?: string(name='libCode', description='The code of the custom text library. This parameter is returned if the moderated text hits a term in the custom text library.', example='123456'),
                    libName?: string(name='libName', description='The name of the custom text library. This parameter is returned if the moderated text hits a term in the custom text library.', example='Name of your custom text library'),
                    positions?: [ string ](name='positions', description='The position of the term that the moderated text hits in the original text.'),
                    ruleType?: string(name='ruleType', description='The behavior rule. This parameter is returned if the moderated text hits the behavior rule. Valid values:

*   user_id
*   ip
*   umid
*   content
*   similar_content
*   imei
*   imsi', example='ip'),
                  }
                ](name='contexts', description='The context information of the risky content that the moderated text hits.'),
              }
            ](name='details', description='The risky content that the moderated text hits. A text entry can hit multiple pieces of risky content.'),
            label?: string(name='label', description='The category of the moderation result for the moderated text. Valid values:

*   normal: normal content
*   spam: spam
*   ad: ad
*   politics: political content
*   terrorism: terrorist content
*   abuse: abuse
*   porn: pornographic content
*   flood: excessive junk content
*   contraband: prohibited content
*   meaningless: meaningless content
*   customized: custom content, such as a custom keyword', example='porn'),
            rate?: double(name='rate', description='The score of the confidence level. Valid values: 0 to 100. A greater value indicates a higher confidence level. If a value of pass is returned for the suggestion parameter, a higher confidence level indicates a higher probability that the content is normal. If a value of review or block is returned for the suggestion parameter, a higher confidence level indicates a higher probability that the content contains violations.

>  This score is for reference only. We strongly recommend that you do not use this score in your business. We recommend that you use the values that are returned for the suggestion, label, and sublabel parameters to determine whether the content contains violations. The sublabel parameter is returned by some operations.', example='99.90'),
            scene?: string(name='scene', description='The moderation scenario.', example='antispam'),
            suggestion?: string(name='suggestion', description='The recommended subsequent operation. Valid values:

*   pass: The content passes the moderation.
*   review: The content needs to be manually reviewed again.
*   block: The content contains violations. We recommend that you delete or block the content.', example='block'),
          }
        ](name='results', description='The returned data. If the HTTP status code 200 is returned, the array in the returned results contains one or more elements. Each element is a struct.'),
        taskId?: string(name='taskId', description='The ID of the moderation task.', example='txt6HB8NQoEbU@5fosnj2xVEM-1t****'),
      }
    ](name='result', description='The text moderation results.'),
  }(name='TextResults', description='The text moderation results. If the HTTP status code 200 is returned, the array in the returned results contains one or more elements. For more information about the parameters, see [Data returned by the ImAudit operation](~~268644~~).'),
}

model ImAuditResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: ImAuditResponseBody(name='body'),
}

/**
  * *   The moderation results are synchronously returned after the moderation is complete.
  * *   You can use the image and text moderation feature only in the China (Beijing), China (Shanghai), and Singapore regions.
  * ### QPS limits
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function imAudit(request: ImAuditRequest): ImAuditResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'ImAudit', 'POST', '/', 'json', false, 'json', request);
}

model ImportFpShotJobRequest {
  fpDBId: string(name='FpDBId', description='The ID of the text fingerprint library to which the text file is imported. You can specify only one job of importing text files to a text fingerprint library at a time. You can obtain the library ID from the response parameters of the [CreateFpShotDB](~~170149~~) operation.', example='88c6ca184c0e47098a5b665e2a12****', position='Query'),
  fpImportConfig: string(name='FpImportConfig', description='The job configurations. The value must be a JSON object. Example: `{"SaveType":"onlysave"}`. The `SaveType` field indicates the storage type. Valid values of the SaveType field:

*   **save**: The fingerprints of the text file are saved to the text fingerprint library only if the text file is not duplicated with content in the text fingerprint library.
*   **onlysave**: The fingerprints of the text file are saved to the text fingerprint library.', example='{"SaveType":"onlysave"}', position='Query'),
  input: string(name='Input', description='The Object Storage Service (OSS) URL of the text file to be imported to the text fingerprint library. The value must be a JSON object. Example: {"Bucket":"example-bucket","Location":"oss-cn-shanghai","Object":"example.flv"}.

> The OSS bucket must reside in the same region as your MPS service.', example='{“Bucket”:”example-bucket”,“Location”:”oss-cn-shanghai”,“Object”:”example.txt”}', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  pipelineId?: string(name='PipelineId', description='The ID of the ApsaraVideo Media Processing (MPS) queue. To view the ID of the MPS queue, perform the following steps: Log on to the **MPS console**. In the left-side navigation pane, choose **Global Settings** > **Pipelines**. The MPS queue is associated with a specified Message Service (MNS) topic. You can submit jobs for different services to different MPS queues. If you do not specify this parameter, the job is submitted to the default MPS queue and no MNS topic is associated with the MPS queue.', example='ae687c02fe944327ba9631e50da2****', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  userData?: string(name='UserData', description='The user-defined data. The value can contain letters, digits, and special characters. The value can be up to 128 bytes in length.', example='\\&UserData={"Chapter":"Ordinary\\*\\*\\*\\*/Ordinary\\*\\*\\*\\*Volume 2/Volume 2 Chapter 47.txt"}', position='Query'),
}

model ImportFpShotJobResponseBody = {
  jobId?: string(name='JobId', description='The ID of the import job. We recommend that you save this ID for subsequent operations.', example='c074b118ace44395a02063a5ab94****'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='25818875-5F78-4A13-BEF6-D7393642CA58'),
}

model ImportFpShotJobResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: ImportFpShotJobResponseBody(name='body'),
}

/**
  * *   You can call this operation to import multiple text files to a text fingerprint library at a time. The system extracts fingerprints from the text files and saves the fingerprints to the text fingerprint library.
  * *   You can call this operation only in the China (Shanghai) region.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function importFpShotJob(request: ImportFpShotJobRequest): ImportFpShotJobResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'ImportFpShotJob', 'POST', '/', 'json', false, 'json', request);
}

model ListAllMediaBucketRequest {
  maximumPageSize?: int32(name='MaximumPageSize', description='The maximum number of media buckets to return. Valid values: 1 to 100. Default value: 50.', example='10', position='Query'),
  nextPageToken?: string(name='NextPageToken', description='The pagination token that is used in the next request to retrieve a new page of results. You do not need to specify this parameter for the first request. The response to the first request contains this parameter, which is added to the next request.', example='P2Zqo1PLGhZdygo-ajSsjUX5zrBHCgXy6j4hEvv****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model ListAllMediaBucketResponseBody = {
  mediaBucketList?: {
    mediaBucket?: [ 
    {
      bucket?: string(name='Bucket', description='The name of the media bucket.', example='example-bucket-****'),
      referer?: string(name='Referer', description='The settings of Object Storage Service (OSS) hotlink protection. For more information, see [Hotlink protection](~~31869~~).', example='http://www.example.com'),
      type?: string(name='Type', description='The type of the media bucket. Valid values:

*   Input: input media bucket
*   Output: output media bucket', example='Input'),
    }
  ](name='MediaBucket')
  }(name='MediaBucketList', description='The media buckets returned.'),
  nextPageToken?: string(name='NextPageToken', description='The returned value of NextPageToken is a pagination token, which can be used in the next request to retrieve a new page of results.', example='P2Zqo1PLGhZdygo-ajSsjUX5zrBHCgXy6j4hEvv****'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='79760D91-D3CF-4165-****-B7E2836EF62A'),
}

model ListAllMediaBucketResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: ListAllMediaBucketResponseBody(name='body'),
}

/**
  * A maximum of 100 media buckets can be returned.
  * ### QPS limit
  * You can call this operation up to 10 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function listAllMediaBucket(request: ListAllMediaBucketRequest): ListAllMediaBucketResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'ListAllMediaBucket', 'POST', '/', 'json', false, 'json', request);
}

model ListCustomEntitiesRequest {
  algorithm: string(name='Algorithm', example='landmark', position='Query'),
  customGroupId: string(name='CustomGroupId', example='1', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  pageNumber: int32(name='PageNumber', example='1', position='Query'),
  pageSize: int32(name='PageSize', example='10', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model ListCustomEntitiesResponseBody = {
  customEntities?: {
    customEntity?: [ 
    {
      customEntityId?: string(name='CustomEntityId', example='1'),
      customEntityInfo?: string(name='CustomEntityInfo', example='{ "finegrainName":"example" }'),
      customEntityName?: string(name='CustomEntityName', example='exampleName'),
    }
  ](name='CustomEntity')
  }(name='CustomEntities'),
  pageNumber?: int32(name='PageNumber', example='1'),
  pageSize?: int32(name='PageSize', example='10'),
  requestId?: string(name='RequestId', example='580e8ce3-3b80-44c5-9f3f-36ac3cc5bdd5'),
  totalCount?: long(name='TotalCount', example='1'),
}

model ListCustomEntitiesResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: ListCustomEntitiesResponseBody(name='body'),
}

async function listCustomEntities(request: ListCustomEntitiesRequest): ListCustomEntitiesResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'ListCustomEntities', 'POST', '/', 'json', false, 'json', request);
}

model ListCustomGroupsRequest {
  algorithm: string(name='Algorithm', example='landmark', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  pageNumber: int32(name='PageNumber', example='1', position='Query'),
  pageSize: int32(name='PageSize', example='10', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model ListCustomGroupsResponseBody = {
  customGroups?: {
    customGroup?: [ 
    {
      customGroupDescription?: string(name='CustomGroupDescription'),
      customGroupId?: string(name='CustomGroupId', example='1'),
      customGroupName?: string(name='CustomGroupName'),
    }
  ](name='CustomGroup')
  }(name='CustomGroups'),
  pageNumber?: int32(name='PageNumber', example='1'),
  pageSize?: int32(name='PageSize', example='10'),
  requestId?: string(name='RequestId', example='580e8ce3-3b80-44c5-9f3f-36ac3cc5bdd5'),
  totalCount?: long(name='TotalCount', example='1'),
}

model ListCustomGroupsResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: ListCustomGroupsResponseBody(name='body'),
}

async function listCustomGroups(request: ListCustomGroupsRequest): ListCustomGroupsResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'ListCustomGroups', 'POST', '/', 'json', false, 'json', request);
}

model ListCustomPersonsRequest {
  categoryId?: string(name='CategoryId', description='The ID of the figure library about which you want to query information. The ID is used to uniquely identify a custom figure library. Make sure that the ID is unique. If you do not specify this parameter, the operation returns all the custom figure libraries. The ID can be up to 120 characters in length and is not case-sensitive.

> You cannot specify the ID of the system figure library for this parameter.', example='CategoryId-****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  personId?: string(name='PersonId', description='The ID of the figure about which you want to query information. The ID is used to uniquely identify a figure. Make sure that the ID is unique. If you do not specify this parameter, the operation returns the information about all the figures in the specified figure library.', example='PersonId-****', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model ListCustomPersonsResponseBody = {
  categories?: {
    category?: [ 
    {
      categoryDescription?: string(name='CategoryDescription', description='The description of the figure library.', example='CategoryDescription-****'),
      categoryId?: string(name='CategoryId', description='The ID of the figure library.', example='CategoryId-****'),
      categoryName?: string(name='CategoryName', description='The name of the figure library.', example='CategoryName-****'),
      persons?: {
        person?: [ 
        {
          faces?: {
            face?: [ 
            {
              faceId?: string(name='FaceId', description='The ID of the face.', example='15****'),
              imageUrl?: string(name='ImageUrl', description='The URL of the facial image that was registered for the figure.', example='http://example-****.jpeg'),
            }
          ](name='Face')
          }(name='Faces', description='The array of the faces.'),
          personDescription?: string(name='PersonDescription', description='The description of the figure.', example='PersonDescription-****'),
          personId?: string(name='PersonId', description='The ID of the figure.', example='PersonId-****'),
          personName?: string(name='PersonName', description='The name of the figure.', example='PersonName-****'),
        }
      ](name='Person')
      }(name='Persons', description='The array of the figures.'),
    }
  ](name='Category')
  }(name='Categories', description='The array of the figure libraries.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='FD4DED6B-0C26-5A8B-A6BE-4FA542AE4D57'),
}

model ListCustomPersonsResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: ListCustomPersonsResponseBody(name='body'),
}

/**
  * You can specify the ID of a figure or a figure library to query the corresponding information. If neither the figure ID nor figure library ID is specified, the operation returns the information about all figures and faces in all figure libraries within the current RAM user.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function listCustomPersons(request: ListCustomPersonsRequest): ListCustomPersonsResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'ListCustomPersons', 'POST', '/', 'json', false, 'json', request);
}

model ListCustomViewsRequest {
  algorithm: string(name='Algorithm', example='landmark', position='Query'),
  customEntityId: string(name='CustomEntityId', example='1', position='Query'),
  customGroupId: string(name='CustomGroupId', example='1', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  pageNumber: int32(name='PageNumber', example='1', position='Query'),
  pageSize: int32(name='PageSize', example='10', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model ListCustomViewsResponseBody = {
  customViews?: {
    customView?: [ 
    {
      customViewId?: string(name='CustomViewId', example='1'),
      imageUrl?: string(name='ImageUrl', example='http://127.66.**.**/photo.jpeg'),
    }
  ](name='CustomView')
  }(name='CustomViews'),
  pageNumber?: int32(name='PageNumber', example='1'),
  pageSize?: int32(name='PageSize', example='10'),
  requestId?: string(name='RequestId', example='580e8ce3-3b80-44c5-9f3f-36ac3cc5bdd5'),
  totalCount?: long(name='TotalCount', example='1'),
}

model ListCustomViewsResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: ListCustomViewsResponseBody(name='body'),
}

async function listCustomViews(request: ListCustomViewsRequest): ListCustomViewsResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'ListCustomViews', 'POST', '/', 'json', false, 'json', request);
}

model ListFpShotDBRequest {
  fpDBIds?: string(name='FpDBIds', description='The ID of the media fingerprint library. You can obtain the library ID from the response parameters of the [CreateFpShotDB](~~170149~~) operation. You can query up to 10 libraries at a time. Separate multiple library IDs with commas (,).', example='2288c6ca184c0e47098a5b665e2a12****,ae687c02fe944327ba9631e50da2****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model ListFpShotDBResponseBody = {
  fpShotDBList?: {
    fpShotDB?: [ 
    {
      description?: string(name='Description', description='The description of the media fingerprint library.', example='The library is a text fingerprint library.'),
      fpDBId?: string(name='FpDBId', description='The ID of the media fingerprint library.', example='88c6ca184c0e47098a5b665e2a12****'),
      modelId?: int32(name='ModelId', description='The model ID of the media fingerprint library. A value of **11** indicates that the library is a text fingerprint library. A value of **12** indicates that the library is a video fingerprint library. A value of **13** indicates that the library is an audio fingerprint library. A value of **14** indicates that the library is an image fingerprint library.', example='11'),
      name?: string(name='Name', description='The name of the media fingerprint library.', example='test-****'),
      status?: string(name='Status', description='The status of the media fingerprint library. Default value: **offline**. ****Valid values:

*   **offline**: The media fingerprint library is offline.
*   **active**: The media fingerprint library is online.
*   **paused**: The media fingerprint library is paused.
*   **deleted**: The media fingerprint library is deleted.', example='active'),
    }
  ](name='FpShotDB')
  }(name='FpShotDBList', description='The media fingerprint libraries.'),
  nonExistIds?: {
    string?: [ string ](name='String')
  }(name='NonExistIds', description='The IDs of the media fingerprint libraries that do not exist.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='25818875-5F78-4A13-BEF6-D7393642CA58'),
}

model ListFpShotDBResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: ListFpShotDBResponseBody(name='body'),
}

/**
  * *   You can call this operation to query the status and information about the media fingerprint libraries based on the specified IDs.
  * *   You can query text fingerprint libraries only in the China (Shanghai) region.
  * *   You can call this operation to query up to 10 media fingerprint libraries.
  * ### QPS limit
  * You can call this operation up to 500 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function listFpShotDB(request: ListFpShotDBRequest): ListFpShotDBResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'ListFpShotDB', 'POST', '/', 'json', false, 'json', request);
}

model ListFpShotFilesRequest {
  endTime?: string(name='EndTime', description='The end of the time range to query. The media files to be returned must be stored before the specified end time. Specify the time in the ISO 8601 standard in the `YYYY-MM-DDThh:mm:ssZ` format. The time must be in UTC.

> This parameter is available only in the China (Beijing), China (Hangzhou), and China (Shanghai) regions.', example='2022-09-08T23:32:56Z', position='Query'),
  fpDBId: string(name='FpDBId', description='The ID of the media fingerprint library whose files you want to query. You can obtain the library ID from the response parameters of the [CreateFpShotDB](~~170149~~) operation.', example='2288c6ca184c0e47098a5b665e2a12****', position='Query'),
  nextPageToken?: string(name='NextPageToken', description='The pagination token that is used in the next request to retrieve a new page of results. You do not need to specify this parameter for the first request.', example='ae0fd49c0840e14daf0d66a75b83****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  pageSize?: int32(name='PageSize', description='The number of entries to return on each page. Default value: 20.', example='20', minimum=1, maximum=100, position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  startTime?: string(name='StartTime', description='The beginning of the time range to query. The media files to be returned must be stored after the specified start time. Specify the time in the ISO 8601 standard in the `YYYY-MM-DDThh:mm:ssZ` format. The time must be in UTC.

> This parameter is available only in the China (Beijing), China (Hangzhou), and China (Shanghai) regions.', example='2022-09-01T00:00:28Z', position='Query'),
}

model ListFpShotFilesResponseBody = {
  fpShotFileList?: {
    fpShotFile?: [ 
    {
      fileId?: string(name='FileId', description='The ID of the video file.', example='41e6536e4f2250e2e9bf26cdea19****'),
      inputFile?: {
        bucket?: string(name='Bucket', description='The name of the OSS bucket in which the input file is stored.', example='example-bucket-****'),
        location?: string(name='Location', description='The ID of the OSS region in which the input file resides.', example='oss-cn-beijing'),
        object?: string(name='Object', description='The name of the OSS object that is used as the input file.', example='example-****.mp4'),
      }(name='InputFile', description='The information about the input file.'),
      primaryKey?: string(name='PrimaryKey', description='The unique primary key of the input video.', example='fb712a6890464059b1b2ea7c8647****'),
      storeTime?: string(name='StoreTime', description='The time when the media fingerprint file was stored. The time follows the ISO 8601 standard in the `YYYY-MM-DDThh:mm:ssZ` format. The time is displayed in UTC.

> This parameter is available only in the China (Beijing), China (Hangzhou), and China (Shanghai) regions.', example='2022-09-08T23:32:56Z'),
    }
  ](name='FpShotFile')
  }(name='FpShotFileList', description='The media fingerprint files. For more information, see the "FpShotFile" section of the [Data types](~~29251~~) topic.'),
  nextPageToken?: string(name='NextPageToken', description='The returned value of NextPageToken is a pagination token, which can be used in the next request to retrieve a new page of results.', example='ae0fd49c0840e14daf0d66a75b83****'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='25818875-5F78-4A13-BEF6-D7393642CA58'),
}

model ListFpShotFilesResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: ListFpShotFilesResponseBody(name='body'),
}

/**
  * *   You can call this operation to query media files in a specific media fingerprint library based on the library ID. This operation supports paged queries.
  * *   You can call this operation only in the China (Beijing), China (Hangzhou), China (Shanghai), and Singapore regions.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function listFpShotFiles(request: ListFpShotFilesRequest): ListFpShotFilesResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'ListFpShotFiles', 'POST', '/', 'json', false, 'json', request);
}

model ListFpShotImportJobRequest {
  jobIds: string(name='JobIds', description='The job IDs. You can obtain the job IDs from the response to the [ImportFpShotJob](~~312262~~) operation. You can specify a maximum of 10 job IDs in a request. Separate multiple job IDs with commas (,).', example='88c6ca184c0e47098a5b665e2a12****,c074b118ace44395a02063a5ab94****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model ListFpShotImportJobResponseBody = {
  fpShotImportJobList?: [ 
    {
      code?: string(name='Code', description='The error code returned when the job fails.', example='0'),
      createTime?: string(name='CreateTime', description='The time when the job was created.', example='2020-06-30T00:33:18Z'),
      finishTime?: string(name='FinishTime', description='The time when the job was completed.', example='2020-06-30T00:34:02Z'),
      fpDBId?: string(name='FpDBId', description='The ID of the text fingerprint library.', example='2288c6ca184c0e47098a5b665e2a12****'),
      fpImportConfig?: string(name='FpImportConfig', description='The import configuration.', example='""'),
      id?: string(name='Id', description='The job ID.', example='25bacf2824614bcf9273dc0744db****'),
      input?: string(name='Input', description='The input file.', example='{\\"Bucket\\":\\"mts-example****\\",\\"Location\\":\\"oss-cn-shanghai\\",\\"Object\\":\\"test-0828/video/test.mp4\\"}'),
      message?: string(name='Message', description='The error message returned when the job fails.', example='Success'),
      pipelineId?: string(name='PipelineId', description='The ID of the ApsaraVideo Media Processing (MPS) queue to which the job is submitted.', example='ebb51ee30f0b49aba959823fa991****'),
      processMessage?: string(name='ProcessMessage', description='The processing information of the job.', example='http://testbucket.oss-cn-shanghai.aliyuncs.com/932ajjw***32ssoj_importResult.txt'),
      status?: string(name='Status', description='The status of the job. Valid values:

*   Processing: The job is in progress.
*   Fail: The job fails.
*   Success: The job is successful.', example='Success'),
      userData?: string(name='UserData', description='The user-defined data.', example='001'),
    }
  ](name='FpShotImportJobList', description='The jobs of importing text files to a text fingerprint library.'),
  nonExistIds?: [ string ](name='NonExistIds', description='The job IDs that do not exist. This parameter is not returned if all specified job IDs exist.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='25818875-5F78-4A13-BEF6-D7393642CA58'),
}

model ListFpShotImportJobResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: ListFpShotImportJobResponseBody(name='body'),
}

/**
  * You can call this operation only in the China (Shanghai) region.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function listFpShotImportJob(request: ListFpShotImportJobRequest): ListFpShotImportJobResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'ListFpShotImportJob', 'POST', '/', 'json', false, 'json', request);
}

model ListJobRequest {
  endOfJobCreatedTimeRange?: string(name='EndOfJobCreatedTimeRange', description='The end of the time range to query. Specify the time in the ISO 8601 standard in the `YYYY-MM-DDThh:mm:ssZ` format. The time must be in UTC.', example='2014-01-11T12:00:00Z', position='Query'),
  maximumPageSize?: long(name='MaximumPageSize', description='The number of entries per page.

*   Default value: **10**.
*   Valid values: **1 to 100**.', example='10', minimum=1, maximum=100, position='Query'),
  nextPageToken?: string(name='NextPageToken', description='The token that is used to retrieve the next page of the query results. You do not need to specify this parameter for the first request. You must specify the token that is obtained from the previous query as the value of NextPageToken.', example='16f01ad6175e4230ac42bb5182cd****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  pipelineId?: string(name='PipelineId', description='The ID of the MPS queue to which the job is submitted. To obtain the ID of an MPS queue, you can log on to the [MPS console](https://mps.console.aliyun.com/overview) and choose **Global Settings** > **MPS Queue and Callback** in the left-side navigation pane.', example='88c6ca184c0e424d5w5b665e2a12****', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  startOfJobCreatedTimeRange?: string(name='StartOfJobCreatedTimeRange', description='The beginning of the time range to query. Specify the time in the ISO 8601 standard in the `YYYY-MM-DDThh:mm:ssZ` format. The time must be in UTC.', example='2014-01-10T12:00:00Z', position='Query'),
  state?: string(name='State', description='The state of the transcoding job. Default value: **All**. Valid values:

*   **All**
*   **Submitted**
*   **Transcoding**
*   **TranscodeSuccess**
*   **TranscodeFail**
*   **TranscodeCancelled**', example='All', position='Query'),
}

model ListJobResponseBody = {
  jobList?: {
    job?: [ 
    {
      code?: string(name='Code', description='The error code returned if the job failed. This parameter is not returned if the job was successful.', example='InternalError'),
      creationTime?: string(name='CreationTime', description='The time when the job was created.', example='2014-01-10T12:00:00Z'),
      finishTime?: string(name='FinishTime', description='The time when the job was complete.', example='2014-01-10T12:20:25Z'),
      input?: {
        bucket?: string(name='Bucket', description='The name of the OSS bucket in which the job input is stored.', example='example-bucket'),
        location?: string(name='Location', description='The ID of the OSS region in which the job input is stored.', example='oss-cn-hangzhou'),
        object?: string(name='Object', description='The name of the OSS object that is used as the job input.', example='example.flv'),
      }(name='Input', description='The information about the job input.'),
      jobId?: string(name='JobId', description='The task ID.', example='31fa3c9ca8134fb4b0b0f7878301****'),
      MNSMessageResult?: {
        errorCode?: string(name='ErrorCode', description='The error code returned if the job failed. This parameter is not returned if the job was successful.', example='InvalidParameter.ResourceNotFound'),
        errorMessage?: string(name='ErrorMessage', description='The error message returned if the job failed. This parameter is not returned if the job was successful.', example='The resource operated “%s” cannot be found.'),
        messageId?: string(name='MessageId', description='The ID of the message returned if the job was successful.', example='123'),
      }(name='MNSMessageResult', description='The message sent by Message Service (MNS) to notify users of the job result.'),
      message?: string(name='Message', description='The error message returned if the job failed. This parameter is not returned if the job was successful.', example='The operation has failed due to some unknown error, exception or failure.'),
      output?: {
        audio?: {
          bitrate?: string(name='Bitrate', description='The audio bitrate of the output file.

*   Unit: Kbit/s.
*   Default value: **128**.', example='128'),
          channels?: string(name='Channels', description='The number of sound channels.

*   If the value of Codec is mp3, the value of this parameter can only be 1 or 2.
*   If the value of Codec is aac, the value of this parameter can only be 1, 2, 4, 5, 6, or 8.
*   Default value: 2.', example='2'),
          codec?: string(name='Codec', description='The audio codec.

*   Valid values: aac, mp3, vorbis, and flac.
*   Default value: **aac**.', example='aac'),
          profile?: string(name='Profile', description='The codec profile of the audio. Valid values if the value of Codec is aac: aaclow, aache, aachev2, aacld, and aaceld.', example='aaclow'),
          qscale?: string(name='Qscale', description='The level of quality control on the audio.', example='15'),
          samplerate?: string(name='Samplerate', description='The sampling rate.

*   Valid values: 22050, 32000, 44100, 48000, and 96000.
*   Unit: Hz.
*   Default value: 44100.
*   If the video container format is FLV and the audio codec is MP3, the value of this parameter cannot be 32000, 48000, or 96000. If the audio codec is MP3, the value of this parameter cannot be 96000.', example='44100'),
          volume?: {
            level?: string(name='Level', description='The volume adjustment range.

*   Unit: decibel.
*   Default value: **-20**.', example='-20'),
            method?: string(name='Method', description='The method that is used to adjust the volume. Valid values:

*   **auto**
*   **dynamic**
*   **linear**', example='auto'),
          }(name='Volume', description='The volume configurations.'),
        }(name='Audio', description='The audio configurations.'),
        audioStreamMap?: string(name='AudioStreamMap', description='The sequence number of the audio stream.

*   Format: `0:a:{Sequence number}`.
*   The sequence number is the index of the audio stream in the list and starts from 0. If no sequence number is specified, the default audio stream is used.', example='0:a:0'),
        clip?: {
          timeSpan?: {
            duration?: string(name='Duration', description='The duration of the clip.

*   Format: `hh:mm:ss[.SSS]` or `sssss[.SSS]`.
*   Valid values: `[00:00:00.000,23:59:59.999]` or `[0.000,86399.999]`.
*   Examples: 01:00:59.999 and 32000.23.', example='01:00:59.999'),
            seek?: string(name='Seek', description='The point in time when the clip starts.

*   Format: `hh:mm:ss[.SSS]` or `sssss[.SSS]`.
*   Valid values: `[00:00:00.000,23:59:59.999]` or `[0.000,86399.999]`.
*   Examples: 01:59:59.999 and 32000.23.', example='01:00:59.999'),
          }(name='TimeSpan', description='The time span of the clip.'),
        }(name='Clip', description='The information about the clip.'),
        container?: {
          format?: string(name='Format', description='The container format.

*   Default value: mp4.
*   Video formats include FLV, MP4, HLS (M3U8 + TS), and MPEG-DASH (MPD + fMP4).
*   Audio formats include MP3, MP4, Ogg, FLAC, and M4A.
*   Image formats include GIF and WebP.
*   If the container format is GIF, the video codec must be GIF.
*   If the container format is WebP, the video codec must be WebP.
*   If the container format is FLV, the video codec cannot be H.265.', example='flv'),
        }(name='Container', description='The container format configurations.'),
        deWatermark?: string(name='DeWatermark', description='The configurations of watermark blurring. The value is a JSON object. For more information, see [Parameter details](~~29253~~).', example='{"0": [{"l": 10,"t": 10,"w": 10,"h": 10},{"l": 100,"t": 0.1,"w": 10,"h": 10}],"128000": [],"250000": [{"l": 0.2,"t": 0.1,"w": 0.01,"h": 0.05}]}'),
        encryption?: {
          id?: string(name='Id', description='The encryption ID.', example='31fa3c9ca8134f9cec2b4b0b0f78****'),
          key?: string(name='Key', description='The key that is used to encrypt the video.', example='encryptionkey128'),
          keyType?: string(name='KeyType', description='The key encryption method. Valid values: Base64 and KMS.

>  For example, if the key is encryptionkey128, you can encrypt the key in the Base64 format or use Key Management Service (KMS) to encrypt the key.``````', example='Base64'),
          keyUri?: string(name='KeyUri', description='The URL that is used to request the key. The URL is Base64-encoded.', example='https://1161758785*****.cn-shanghai.fc.aliyuncs.com/2016-08-15/proxy/HLS-decyptServer/decyptServer/'),
          skipCnt?: string(name='SkipCnt', description='The number of unencrypted frames at the beginning of the video. Leaving these frames unencrypted enables video playback to quickly start.', example='3'),
          type?: string(name='Type', description='The encryption type. Only hls-aes-128 may be returned.', example='hls-aes-128'),
        }(name='Encryption', description='The encryption configurations. Only outputs in the M3U8 format are supported.'),
        m3U8NonStandardSupport?: {
          ts?: {
            md5Support?: boolean(name='Md5Support', description='Indicates whether the MD5 value of the TS file is included in the M3U8 file. Valid values:

*   **true**
*   **false**', example='true'),
            sizeSupport?: boolean(name='SizeSupport', description='Indicates whether the size of the TS file is included in the M3U8 file.

*   **true**
*   **false**', example='true'),
          }(name='TS', description='The non-standard support configurations for TS files. The value is a JSON object. For more information, see [Parameter details](~~29253~~).'),
        }(name='M3U8NonStandardSupport', description='The non-standard support configuration for M3U8. The value must be a JSON object. For more information, see [Parameter details](~~29253~~).'),
        mergeConfigUrl?: string(name='MergeConfigUrl', description='The URL of the merging configuration file. You can specify either MergeList or MergeConfigUrl when you submit the transcoding job.

*   The configuration file specified by MergeConfigUrl can contain up to 50 clips.
*   MergeConfigUrl indicates the URL of the configuration file for merging clips.
*   Make sure that the configuration file is stored as an object in OSS and that MPS can access the OSS object. For information about the file content, see the details of merging parameters.
*   Example of the content of the merging configuration file: `{"MergeList":[{"MergeURL":"http://exampleBucket****.oss-cn-hangzhou.aliyuncs.com/video_01.mp4"}]}`.', example='{"MergeList":[{"MergeURL":"http://exampleBucket****.oss-cn-hangzhou.aliyuncs.com/video_01.mp4"}]}'),
        mergeList?: {
          merge?: [ 
          {
            duration?: string(name='Duration', description='The duration of the clip.

*   Format: `hh:mm:ss[.SSS]` or `sssss[.SSS]`.
*   Examples: 01:59:59.999 and 32000.23.', example='01:59:59.999'),
            mergeURL?: string(name='MergeURL', description='The OSS URL of the clip.

*   Example: `http://example-bucket-****.oss-cn-hangzhou.aliyuncs.com/example-object.flv`.
*   The object must be URL-encoded by using the UTF-8 standard.', example='http://example-bucket.oss-cn-hangzhou.aliyuncs.com/example-object.flv'),
            roleArn?: string(name='RoleArn', description='The Alibaba Cloud Resource Name (ARN) of the Resource Access Management (RAM) role used for delegated authorization.', example='acs:ram::<your uid>:role/<your role name>'),
            start?: string(name='Start', description='The start point in time of the clip.

*   Format: `hh:mm:ss[.SSS]` or `sssss[.SSS]`.
*   Examples: 01:59:59.999 and 32000.23.'),
          }
        ](name='Merge')
        }(name='MergeList', description='The configurations for merging clips.'),
        muxConfig?: {
          gif?: {
            ditherMode?: string(name='DitherMode', description='The color dithering algorithm of the palette. Valid values: **sierra** and **bayer**.', example='bayer'),
            finalDelay?: string(name='FinalDelay', description='The duration for which the final frame is paused. Unit: centisecond.', example='0'),
            isCustomPalette?: string(name='IsCustomPalette', description='Indicates whether a custom palette is used. Valid values:

*   **true**
*   **false**', example='true'),
            loop?: string(name='Loop', description='The loop count.', example='0'),
          }(name='Gif', description='The transmuxing configurations for GIF.'),
          segment?: {
            duration?: string(name='Duration', description='The length of the segment. The value must be an integer. Unit: seconds.

*   Valid values: \\[1,10].
*   Default value: 10.', example='10'),
          }(name='Segment', description='The segment configurations. The value is a JSON object.'),
          webp?: {
            loop?: string(name='Loop', description='The loop count.', example='0'),
          }(name='Webp', description='The transmuxing configurations for WebP.'),
        }(name='MuxConfig', description='The transmuxing configurations.'),
        openingList?: {
          opening?: [ 
          {
            height?: string(name='Height', description='The height of the opening part. Valid values: values in the range of (0, 4096), -1, and full.

*   Default value: **-1**.
*   A value of -1 indicates that the height of the source of the opening part is retained.
*   A value of full indicates that the height of the opening part equals the height of the main part.', example='-1'),
            start?: string(name='Start', description='The amount of time after which the opening part is played. The value starts from 0.

*   Unit: seconds.
*   Default value: **0**.', example='0'),
            width?: string(name='Width', description='The width of the opening part. Valid values: values in the range of (0, 4096), -1, and full.

*   Default value: **-1**.
*   A value of -1 indicates that the width of the source of the opening part is retained.
*   A value of full indicates that the width of the opening part equals the width of the main part.', example='-1'),
            openUrl?: string(name='openUrl', description='The OSS URL of the opening part.', example='http://example-bucket-****.oss-cn-hangzhou.aliyuncs.com/opening_01.flv'),
          }
        ](name='Opening')
        }(name='OpeningList', description='The opening parts.'),
        outSubtitleList?: {
          outSubtitle?: [ 
          {
            map?: string(name='Map', description='The video track. Format: 0:{Stream}:{Stream sequence number}, that is, 0:v:{video_index}. The value of Stream is v, which indicates a video stream. The sequence number is the index of the video stream in the list and starts from 0.', example='0:v:0'),
            message?: string(name='Message', description='The error message returned if the job failed to be created. This parameter is not returned if the job was created.', example='The specified parameter “%s” cannot be null.'),
            outSubtitleFile?: {
              bucket?: string(name='Bucket', description='The name of the OSS bucket in which the input file is stored.', example='example-bucket-****'),
              location?: string(name='Location', description='The ID of the OSS region in which the output file is stored.', example='oss-cn-hangzhou'),
              object?: string(name='Object', description='The name of the OSS object that is used as the output file.', example='example-output.flv'),
              roleArn?: string(name='RoleArn', description='The ARN of the RAM role used for delegated authorization.', example='acs:ram::<your uid>:role/<your role name>'),
            }(name='OutSubtitleFile', description='The details of the output file.'),
            success?: boolean(name='Success', description='Indicates whether the job was created. Valid values:

*   **true**
*   **false**', example='true'),
          }
        ](name='OutSubtitle')
        }(name='OutSubtitleList', description='The output subtitles.'),
        outputFile?: {
          bucket?: string(name='Bucket', description='The name of the OSS bucket in which the input file is stored.', example='example-bucket'),
          location?: string(name='Location', description='The ID of the OSS region in which the output file is stored.', example='oss-cn-hangzhou'),
          object?: string(name='Object', description='The name of the OSS object that is used as the output file.', example='example-output.flv'),
          roleArn?: string(name='RoleArn', description='The ARN of the RAM role used for delegated authorization.', example='acs:ram::<your uid>:role/<your role name>'),
        }(name='OutputFile', description='The details of the output file.'),
        priority?: string(name='Priority', description='The priority of the job in the MPS queue to which the job is added.

*   A value of 10 indicates the highest priority.
*   Default value: **6**.', example='6'),
        properties?: {
          bitrate?: string(name='Bitrate', description='The bitrate of the video.', example='490'),
          duration?: string(name='Duration', description='The duration of the video.', example='17'),
          fileFormat?: string(name='FileFormat', description='The format of the video.', example='mp4'),
          fileSize?: string(name='FileSize', description='The size of the file.', example='1057273'),
          format?: {
            bitrate?: string(name='Bitrate', description='The total bitrate.', example='490.784'),
            duration?: string(name='Duration', description='The total duration.', example='17.234000'),
            formatLongName?: string(name='FormatLongName', description='The full name of the container format.', example='QuickTime / MOV'),
            formatName?: string(name='FormatName', description='The short name of the container format. Valid values: mov, mp4, m4a, 3gp, 3g2, and mj2.', example='mov'),
            numPrograms?: string(name='NumPrograms', description='The total number of program streams.', example='0'),
            numStreams?: string(name='NumStreams', description='The total number of media streams.', example='2'),
            size?: string(name='Size', description='The size of the file.', example='1057273'),
            startTime?: string(name='StartTime', description='The start time.', example='0.064000'),
          }(name='Format', description='The format information.'),
          fps?: string(name='Fps', description='The frame rate of the video.', example='30'),
          height?: string(name='Height', description='The height of the video.', example='1280'),
          streams?: {
            audioStreamList?: {
              audioStream?: [ 
              {
                bitrate?: string(name='Bitrate', description='The bitrate of the audio stream.', example='64.136'),
                channelLayout?: string(name='ChannelLayout', description='The output layout of the sound channels.', example='mono'),
                channels?: string(name='Channels', description='The number of sound channels.', example='1'),
                codecLongName?: string(name='CodecLongName', description='The full name of the codec.', example='AAC (Advanced Audio Coding)'),
                codecName?: string(name='CodecName', description='The short name of the codec.', example='aac'),
                codecTag?: string(name='CodecTag', description='The tag of the codec.', example='0x6134706d'),
                codecTagString?: string(name='CodecTagString', description='The tag string of the codec.', example='mp4a'),
                codecTimeBase?: string(name='CodecTimeBase', description='The codec time base.', example='1/32000'),
                duration?: string(name='Duration', description='The duration of the audio stream.', example='17.223562'),
                index?: string(name='Index', description='The sequence number of the audio stream. The value indicates the position of the audio stream in all audio streams.', example='1'),
                lang?: string(name='Lang', description='The language of the audio stream. For more information, see [FFmpeg documentation](https://www.ffmpeg.org/ffmpeg-all.html#Metadata) and [ISO 639](https://en.wikipedia.org/wiki/List_of_ISO\\_639-1\\_codes).', example='und'),
                numFrames?: string(name='NumFrames', description='The total number of frames.', example='30'),
                sampleFmt?: string(name='SampleFmt', description='The sampling format.', example='fltp'),
                samplerate?: string(name='Samplerate', description='The sampling rate of the audio stream.', example='32000'),
                startTime?: string(name='StartTime', description='The start time.', example='0.000000'),
                timebase?: string(name='Timebase', description='The time base of the audio stream.', example='1/32000'),
              }
            ](name='AudioStream')
            }(name='AudioStreamList', description='The audio streams.'),
            subtitleStreamList?: {
              subtitleStream?: [ 
              {
                index?: string(name='Index', description='The sequence number of the subtitle stream. The value indicates the position of the subtitle stream in all subtitle streams.', example='1'),
                lang?: string(name='Lang', description='The language of the subtitle stream.', example='und'),
              }
            ](name='SubtitleStream')
            }(name='SubtitleStreamList', description='The subtitle streams.'),
            videoStreamList?: {
              videoStream?: [ 
              {
                avgFPS?: string(name='AvgFPS', description='The average frame rate of the video stream.', example='30.0'),
                bitrate?: string(name='Bitrate', description='The bitrate of the video stream.', example='421.117'),
                codecLongName?: string(name='CodecLongName', description='The full name of the codec.', example='H.264 / AVC / MPEG-4 AVC / MPEG-4 part 10'),
                codecName?: string(name='CodecName', description='The short name of the codec.', example='h264'),
                codecTag?: string(name='CodecTag', description='The tag of the codec.', example='0x31637661'),
                codecTagString?: string(name='CodecTagString', description='The tag string of the codec.', example='avc1'),
                codecTimeBase?: string(name='CodecTimeBase', description='The codec time base.', example='1/60'),
                dar?: string(name='Dar', description='The display aspect ratio (DAR).', example='9:16'),
                duration?: string(name='Duration', description='The duration of the video stream.', example='17.233333'),
                fps?: string(name='Fps', description='The frame rate of the video stream.', example='30.0'),
                hasBFrames?: string(name='HasBFrames', description='Indicates whether the video stream contains B-frames.', example='2'),
                height?: string(name='Height', description='The height of the video stream in pixels.', example='1280'),
                index?: string(name='Index', description='The sequence number of the video stream. The value indicates the position of the video stream in all video streams.', example='0'),
                lang?: string(name='Lang', description='The language of the video stream. For more information, see [FFmpeg documentation](https://www.ffmpeg.org/ffmpeg-all.html#Metadata) and [ISO 639](https://en.wikipedia.org/wiki/List_of_ISO\\_639-1\\_codes).', example='und'),
                level?: string(name='Level', description='The codec level.', example='31'),
                networkCost?: {
                  avgBitrate?: string(name='AvgBitrate', description='The average bitrate of the video stream.', example='300'),
                  costBandwidth?: string(name='CostBandwidth', description='The maximum bandwidth that was consumed.', example='10'),
                  preloadTime?: string(name='PreloadTime', description='The amount of time consumed to preload the video stream.', example='8'),
                }(name='NetworkCost', description='The network bandwidth that was consumed.'),
                numFrames?: string(name='NumFrames', description='The total frame rate.', example='30'),
                pixFmt?: string(name='PixFmt', description='The pixel format of the video stream.', example='yuv420p'),
                profile?: string(name='Profile', description='The codec profile.', example='high'),
                sar?: string(name='Sar', description='The sample aspect ratio (SAR) of the video stream.', example='1:1'),
                startTime?: string(name='StartTime', description='The start time.', example='0.000000'),
                timebase?: string(name='Timebase', description='The time base of the video stream.', example='1/15360'),
                width?: string(name='Width', description='The width of the video stream in pixels.', example='720'),
              }
            ](name='VideoStream')
            }(name='VideoStreamList', description='The video streams.'),
          }(name='Streams', description='The stream information.'),
          width?: string(name='Width', description='The width of the video.', example='720'),
        }(name='Properties', description='The media properties.'),
        rotate?: string(name='Rotate', description='The rotation angle of the video.', example='90'),
        subtitleConfig?: {
          extSubtitleList?: {
            extSubtitle?: [ 
            {
              charEnc?: string(name='CharEnc', description='The character set used by the external subtitle.

*   Valid values: UTF-8, GBK, BIG5, and auto.
*   Default value: **auto**.

>  If this parameter is set to auto, the detected character set may not be the actual character set. We recommend that you set this parameter to another value.', example='auto'),
              fontName?: string(name='FontName', description='The font of the hardcoded subtitles converted from external subtitles.', example='"WenQuanYi Zen Hei", "Yuanti SC Regular", "SimSun"'),
              input?: {
                bucket?: string(name='Bucket', description='The name of the OSS bucket in which the input file is stored.', example='example-bucket-****'),
                location?: string(name='Location', description='The ID of the OSS region in which the input file is stored.', example='oss-cn-hangzhou'),
                object?: string(name='Object', description='The name of the OSS object that is used as the input file.', example='example-output.flv'),
              }(name='Input', description='The input subtitle file.

*   Files in the SRT or ASS format are supported. For more information, see [Parameter details](~~29253~~).
*   Example: `{"Bucket":"example-bucket","Location":"oss-cn-hangzhou","Object":"example.srt"}`.'),
            }
          ](name='ExtSubtitle')
          }(name='ExtSubtitleList', description='The external subtitles. The value is a JSON array.'),
          subtitleList?: {
            subtitle?: [ 
            {
              map?: string(name='Map', description='The sequence number of the video stream. The sequence number is the index of the video stream in the list and starts from 0. If you do not set the corresponding parameter in the request, the default video stream is selected.', example='0'),
            }
          ](name='Subtitle')
          }(name='SubtitleList', description='The subtitles.'),
        }(name='SubtitleConfig', description='The subtitle configurations.'),
        superReso?: {
          isHalfSample?: string(name='IsHalfSample', description='Indicates whether parameters related to the sampling rate are obtained. Valid values:

*   **true**
*   **false**', example='true'),
        }(name='SuperReso', description='The configurations for using the resolution of the source video.'),
        tailSlateList?: {
          tailSlate?: [ 
          {
            bgColor?: string(name='BgColor', description='The color of the bars that are added to the ending part if the size of the ending part is smaller than that of the main part. Default value: **White**. For more information, see [Background colors](https://docs-aliyun.cn-hangzhou.oss.aliyun-inc.com/assets/attach/29253/cn_zh/1502784952344/color.txt?spm=a2c4g.11186623.2.63.1df840f74IH4Eq\\&file=color.txt).', example='White'),
            blendDuration?: string(name='BlendDuration', description='The duration of the transition between the main part and the ending part. A fade transition is used: The last frame of the main part fades out, and the first frame of the ending part fades in. Unit: seconds. Default value: 0.', example='0'),
            height?: string(name='Height', description='The height of the ending part. Valid values: values in the range of (0, 4096), -1, and full.

*   A value of -1 indicates that the height of the source of the ending part is retained.
*   A value of full indicates that the height of the ending part equals the height of the main part.
*   Default value: -1.', example='-1'),
            isMergeAudio?: boolean(name='IsMergeAudio', description='Indicates whether the audio content of the ending part is merged. Valid values:

*   **true**
*   **false**', example='true'),
            start?: string(name='Start', description='The start time.', example='1'),
            tailUrl?: string(name='TailUrl', description='The OSS URL of the ending part.', example='http://example-bucket-****.oss-cn-hangzhou.aliyuncs.com/opening_01.flv'),
            width?: string(name='Width', description='The width of the ending part. Valid values: values in the range of (0, 4096), -1, and full.

*   A value of -1 indicates that the width of the source of the ending part is retained. A value of full indicates that the width of the ending part equals the width of the main part.
*   Default value: -1.', example='-1'),
          }
        ](name='TailSlate')
        }(name='TailSlateList', description='The ending parts.'),
        templateId?: string(name='TemplateId', description='The template ID.', example='S00000000-000010'),
        transConfig?: {
          adjDarMethod?: string(name='AdjDarMethod', description='The method of resolution adjustment. Default value: **none**. Valid values: rescale, crop, pad, and none.', example='none'),
          isCheckAudioBitrate?: string(name='IsCheckAudioBitrate', description='Indicates whether the audio bitrate is checked. If the bitrate of the output audio is higher than that of the input audio, the input bitrate is retained and the specified audio bitrate does not take effect. This parameter has a lower priority than IsCheckAudioBitrateFail. Valid values:

*   **true**

*   **false**

*   Default value:

    *   If this parameter is empty and the codec of the output audio is different from the codec of the input audio, the default value is false.
    *   If this parameter is empty and the codec of the output audio is the same as the codec of the input audio, the default value is true.', example='false'),
          isCheckAudioBitrateFail?: string(name='IsCheckAudioBitrateFail', description='Indicates whether the audio bitrate is checked. If the bitrate of the output audio is higher than that of the input audio, a transcoding failure is returned without transcoding the audio. This parameter has a higher priority than IsCheckAudioBitrate. Valid values:

*   **false**: The audio bitrate is checked.
*   **true**: The audio bitrate is not checked.', example='false'),
          isCheckReso?: string(name='IsCheckReso', description='Indicates whether the resolution is checked. If the output resolution is higher than the input resolution based on the width or height, the input resolution is retained. Valid values:

*   **true**
*   **false**
*   Default value: **false**.', example='false'),
          isCheckResoFail?: string(name='IsCheckResoFail', description='Indicates whether the resolution is checked. If the output resolution is higher than the input resolution based on the width or height, a transcoding failure is returned. Valid values:

*   **true**
*   **false**
*   Default value: **false**.', example='false'),
          isCheckVideoBitrate?: string(name='IsCheckVideoBitrate', description='Indicates whether the video bitrate is checked. If the bitrate of the output video is higher than that of the input video, the input bitrate is retained. Valid values:

*   **true**
*   **false**
*   Default value: **false**.', example='false'),
          isCheckVideoBitrateFail?: string(name='IsCheckVideoBitrateFail', description='Indicates whether the video bitrate is checked. If the bitrate of the output video is higher than that of the input video, a transcoding failure is returned without transcoding the video. This parameter has a higher priority than IsCheckVideoBitrate. Valid values:

*   **true**
*   **false**
*   Default value: false.', example='false'),
          transMode?: string(name='TransMode', description='The transcoding mode.

*   Valid values: onepass, twopass, and CBR.
*   Default value: onepass.', example='onepass'),
        }(name='TransConfig', description='The general transcoding configurations. If this parameter is specified in the request, the corresponding parameters in the specified transcoding template are overwritten.'),
        userData?: string(name='UserData', description='The custom data.', example='test-001'),
        video?: {
          bitrate?: string(name='Bitrate', description='The average bitrate of the video. Unit: Kbit/s.', example='428'),
          bitrateBnd?: {
            max?: string(name='Max', description='The maximum bitrate. Unit: Kbit/s.', example='1000'),
            min?: string(name='Min', description='The minimum bitrate. Unit: Kbit/s.', example='200'),
          }(name='BitrateBnd', description='The bitrate range of the video.'),
          bufsize?: string(name='Bufsize', description='The size of the buffer.', example='6000'),
          codec?: string(name='Codec', description='The video codec. Valid values: **H.264**, **H.265**, **GIF**, and **WEBP**.', example='H.264'),
          crf?: string(name='Crf', description='The constant rate factor. If this parameter is returned, the value of Bitrate is invalid. Default value: **26**.', example='26'),
          crop?: string(name='Crop', description='The video cropping mode. Valid values:

*   **border**: automatically detects and removes black borders.
*   A value in the width:height:left:top format: crops the videos based on the custom settings. Example: 1280:800:0:140.', example='border'),
          degrain?: string(name='Degrain', description='The strength of the independent noise reduction algorithm.', example='5'),
          fps?: string(name='Fps', description='The frame rate.

*   The value is 60 if the frame rate of the input video exceeds 60.
*   Default value: the frame rate of the input file.', example='25'),
          gop?: string(name='Gop', description='The maximum number of frames between two keyframes. Default value: 250.', example='250'),
          height?: string(name='Height', description='The height of the video.

*   Unit: pixel.
*   Default value: the height of the input video.', example='720'),
          maxFps?: string(name='MaxFps', description='The maximum frame rate.', example='60'),
          maxrate?: string(name='Maxrate', description='The maximum bitrate of the video. Unit: Kbit/s.', example='1000'),
          pad?: string(name='Pad', description='The black borders that are added to the video. Unit: pixel.

*   Format: width:height:left:top.
*   Example: 1280:800:0:140.', example='1280:800:0:140'),
          pixFmt?: string(name='PixFmt', description='The pixel format of the video. Valid values: standard pixel formats such as yuv420p and yuvj420p.', example='yuv420p'),
          preset?: string(name='Preset', description='The preset video algorithm. Default value: medium. Valid values:

*   **veryfast**
*   **fast**
*   **medium**
*   **slow**
*   **slower**', example='medium'),
          profile?: string(name='Profile', description='The codec profile. Valid values:

*   **baseline**: applicable to mobile devices.
*   **main**: applicable to standard-definition devices.
*   **high**: applicable to high-definition devices.
*   Default value: **high**.', example='high'),
          qscale?: string(name='Qscale', description='The level of quality control on the video.', example='15'),
          resoPriority?: string(name='ResoPriority', description='The priority of the resource.', example='1'),
          scanMode?: string(name='ScanMode', description='The scan mode. Valid values:

*   If this parameter is **empty**, the scan mode of the input file is used.
*   **auto**: automatic deinterlacing.
*   **progressive**: progressive scan.
*   **interlaced**: interlaced scan.
*   **By default**, this parameter is empty.

**Best practice**: Interlaced scan consumes less bandwidth than progressive scan, but the image quality is poor. Therefore, mainstream video production uses progressive scan.

*   If **progressive scan** or **interlaced scan** is used when the scan mode of the input file is neither of them, the transcoding job fails.
*   We recommend that you use **the scan mode of the input file** or **automatic deinterlacing** to improve compatibility.', example='interlaced'),
          width?: string(name='Width', description='The width of the video.

*   Unit: pixel.
*   Default value: the width of the input video.', example='1280'),
        }(name='Video', description='The video configurations.'),
        videoStreamMap?: string(name='VideoStreamMap', description='The sequence number of the video stream.

*   Format: 0:a:{Sequence number}. Example value: 0:a:0.
*   The sequence number is the index of the video stream in the list and starts from 0.
*   If no sequence number is specified, the default video stream is used.', example='0:a:0'),
        waterMarkConfigUrl?: string(name='WaterMarkConfigUrl', description='The URL of the watermark configuration file.', example='http://example.com/configure'),
        waterMarkList?: {
          waterMark?: [ 
          {
            dx?: string(name='Dx', description='The horizontal offset of the watermark image relative to the output video. If this parameter is specified in the request, the corresponding parameter in the watermark template is overwritten. Default value: 0. The value can be an integer or a decimal number.

*   An integer indicates the pixel value of the horizontal offset.

    *   Valid values: \\[8,4096].
    *   Unit: pixel.

*   A decimal number indicates the ratio of the horizontal offset to the width in the output video resolution.

    *   Valid values: (0,1).
    *   The decimal number can be accurate to four decimal places, such as 0.9999. Excess digits are automatically deleted.', example='100'),
            dy?: string(name='Dy', description='The vertical offset of the watermark image relative to the output video. If this parameter is specified in the request, the corresponding parameter in the watermark template is overwritten. The value can be an integer or a decimal number.

*   An integer indicates the pixel value of the vertical offset.

    *   Valid values: \\[8,4096].
    *   Unit: pixel.

*   A decimal indicates the ratio of the vertical offset to the height in the output video resolution.

    *   Valid values: (0,1).
    *   The decimal number can be accurate to four decimal places, such as 0.9999. Excess digits are automatically deleted.', example='100'),
            height?: string(name='Height', description='The height of the watermark. If this parameter is specified in the request, the corresponding parameter in the specified watermark template is overwritten. The value can be an integer or a decimal number.

*   An integer indicates the pixel value of the watermark height.

    *   Valid values: \\[8,4096].
    *   Unit: pixel.

*   A decimal indicates the ratio of the watermark height to the height in the output video resolution.

    *   Valid values: (0,1).
    *   The decimal number can be accurate to four decimal places, such as 0.9999. Excess digits are automatically deleted.', example='50'),
            inputFile?: {
              bucket?: string(name='Bucket', description='The name of the OSS bucket in which the input file is stored.', example='example-bucket'),
              location?: string(name='Location', description='The ID of the OSS region in which the input file is stored.', example='oss-cn-hangzhou'),
              object?: string(name='Object', description='The name of the Object Storage Service (OSS) object that is used as the input file.', example='example-logo-****.png'),
            }(name='InputFile', description='The watermark input file.'),
            referPos?: string(name='ReferPos', description='The position of the watermark.

*   **TopRight**
*   **TopLeft**
*   **BottomRight**
*   **BottomLeft**', example='TopRight'),
            type?: string(name='Type', description='The type of the watermark. If this parameter is specified in the request, the corresponding parameter in the watermark template is overwritten. For more information, see [Parameter details](~~29253~~). Valid values:

*   **Image**
*   **Text**', example='Image'),
            waterMarkTemplateId?: string(name='WaterMarkTemplateId', description='The ID of the watermark template.', example='88c6ca184c0e47098a5b665e2a12****'),
            width?: string(name='Width', description='The width of the watermark image. If this parameter is specified in the request, the corresponding parameter in the watermark template is overwritten. The value can be an integer or a decimal number.

*   An integer indicates the pixel value of the watermark width.

    *   Valid values: \\[8,4096].
    *   Unit: pixel.

*   A decimal indicates the ratio of the watermark width to the width in the output video resolution.

    *   Valid values: (0,1).
    *   The decimal number can be accurate to four decimal places, such as 0.9999. Excess digits are automatically deleted.', example='50'),
          }
        ](name='WaterMark')
        }(name='WaterMarkList', description='The watermarks.'),
      }(name='Output', description='The output of the job.'),
      percent?: long(name='Percent', description='The transcoding progress.', example='100'),
      pipelineId?: string(name='PipelineId', description='The ID of the MPS queue.', example='88c6ca184c0e47b665e2a1267971****'),
      state?: string(name='State', description='The state of the job. Valid values:

*   **Submitted**
*   **Transcoding**
*   **TranscodeSuccess**
*   **TranscodeFail**
*   **TranscodeCancelled**', example='TranscodeSuccess'),
    }
  ](name='Job')
  }(name='JobList', description='The transcoding jobs.'),
  nextPageToken?: string(name='NextPageToken', description='The pagination token. It can be used in the next request to retrieve a new page of results.', example='16f01ad6175e4230ac42bb5182cd****'),
  requestId?: string(name='RequestId', description='The request ID.', example='BC860F04-778A-472F-AB39-E1BF329C1EA8'),
}

model ListJobResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: ListJobResponseBody(name='body'),
}

/**
  * *   By default, the returned transcoding jobs are sorted by CreationTime in descending order.
  * *   You can call this operation to return transcoding jobs of the last 90 days. The jobs are returned based on the specified time range.
  * *   You can filter query results by configuring request parameters such as job status, creation time interval, and ApsaraVideo Media Processing (MPS) queue for transcoding.
  * *   By default, MPS does not allow you to access data across regions within the same account. Before you call this operation, make sure that the region that you specify is the same as the region of the transcoding jobs to be queried. Otherwise, this operation may fail to be called, or invalid information may be returned.
  * ### [](#qps)QPS limits
  * You can call this API operation up to 100 times per second per account. Requests that exceed this limit are dropped, and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limits](~~342832~~).
  *
 */
async function listJob(request: ListJobRequest): ListJobResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'ListJob', 'POST', '/', 'json', false, 'json', request);
}

model ListMediaWorkflowExecutionsRequest {
  inputFileURL?: string(name='InputFileURL', description='The Object Storage Service (OSS) URL of the input file of the media workflow. The URL complies with RFC 3986 and is encoded in UTF-8, with reserved characters being percent-encoded. For more information, see [URL encoding](~~423796~~).', example='http://example-****.cn-hangzhou.aliyuncs.com/test****.flv', position='Query'),
  maximumPageSize?: long(name='MaximumPageSize', description='The maximum number of media workflow execution instances to return. Valid values: `[1,100]`. Default value: **10**.', example='1', minimum=1, maximum=100, position='Query'),
  mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the media workflow whose execution instances you want to query. To obtain the workflow ID, you can log on to the **ApsaraVideo Media Processing (MPS) console** and choose **Workflows** > **Workflow Settings**.', example='43b7335a4b1d4fe883670036affb****', position='Query'),
  mediaWorkflowName?: string(name='MediaWorkflowName', description='The name of the media workflow. To obtain the workflow name, you can log on to the **MPS console** and choose **Workflows** > **Workflow Settings**.', example='example-mediaworkflow-****', position='Query'),
  nextPageToken?: string(name='NextPageToken', description='The pagination token that is used in the next request to retrieve a new page of results. The value is a UUID that contains 32 characters. When you request the first page of query results, leave the NextPageToken parameter empty. When you request more query results, specify the value of the NextPageToken parameter returned in the query results on the previous page.', example='39f8e0bc005e4f309379701645f4****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model ListMediaWorkflowExecutionsResponseBody = {
  mediaWorkflowExecutionList?: {
    mediaWorkflowExecution?: [ 
    {
      activityList?: {
        activity?: [ 
        {
          code?: string(name='Code', description='The error code returned if the request failed.

*   The specific error code appears if the state of the activity is **Fail**.
*   This parameter is not returned if the state of the activity is **Success**.', example='null'),
          endTime?: string(name='EndTime', description='The end time of the activity.', example='2016-04-01T06:54:00Z'),
          jobId?: string(name='JobId', description='The ID of the job generated when the activity is executed. We recommend that you keep this ID for subsequent operation calls.', example='2376030d9d0849399cd20e20c876****'),
          MNSMessageResult?: {
            errorCode?: string(name='ErrorCode', description='The error code returned if the job failed. If the job was successful, this parameter is not returned.', example='The Topic/Queue config is empty, not send message'),
            errorMessage?: string(name='ErrorMessage', description='The error message returned if the job failed. If the job was successful, this parameter is not returned.', example='MessageConfigEmpty'),
            messageId?: string(name='MessageId', description='The ID of the success message. If the job failed, this parameter is not returned.', example='4f3bc83233de4e2f81c7dade443e****'),
          }(name='MNSMessageResult', description='The message sent by Message Service (MNS) to notify the user of the job result.'),
          message?: string(name='Message', description='The error message returned if the request failed.

*   The detailed error message appears if the state of the activity is **Fail**.
*   This parameter is not returned if the state of the activity is **Success**.', example='null'),
          name?: string(name='Name', description='The name of the media workflow activity.

> The name of an activity in a media workflow is unique.', example='Act-2'),
          startTime?: string(name='StartTime', description='The start time of the activity.', example='2016-04-01T06:53:45Z'),
          state?: string(name='State', description='The status of the activity. Valid values:

*   **Running**: The activity is being executed.
*   **Fail**: The activity failed to be executed.
*   **Skipped**: The activity was skipped.
*   **Success**: The activity was successfully executed.

> For example, the high-definition and standard-definition transcoding activities are to be run after the analysis activity is complete. The system determines the activity to run based on the analysis result. If the definition of the input video content is insufficient, the high-definition transcoding activity may be skipped.', example='Success'),
          type?: string(name='Type', description='The type of the media workflow activity. Valid values: Start, Snapshot, Transcode, Analysis, and Report. For more information, see [Methods supported for media workflows](~~68494~~).', example='Start'),
        }
      ](name='Activity')
      }(name='ActivityList', description='The activities that are executed in the media workflow.'),
      creationTime?: string(name='CreationTime', description='The time when the media workflow was created.', example='2016-04-01T06:53:43Z'),
      input?: {
        inputFile?: {
          bucket?: string(name='Bucket', description='The name of the OSS bucket in which the input media file is stored.', example='example-bucket-****'),
          location?: string(name='Location', description='The OSS region in which the input file resides.', example='cn-shanghai'),
          object?: string(name='Object', description='The name of the OSS object that is used as the input media file.', example='example-mediaWorkflow-****/example-object-****/example.mp4'),
        }(name='InputFile', description='The information about the storage location of the input file of the media workflow in OSS.'),
        userData?: string(name='UserData', description='The custom data.', example='example data'),
      }(name='Input', description='The custom data of the media workflow.'),
      mediaId?: string(name='MediaId', description='The ID of the media file. A media file contains all the information about a media workflow.', example='512046582a924698a41e0f8b0d2b****'),
      mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the media workflow.', example='43b7335a4b1d4fe883670036affb****'),
      name?: string(name='Name', description='The name of the media workflow.', example='example-mediaworkflow-****'),
      runId?: string(name='RunId', description='The ID of the execution instance.', example='48e33690ac19445488c706924321****'),
      state?: string(name='State', description='The status of the media workflow. Valid values:

*   **running**: The execution is in progress.
*   **Completed**: The execution is complete.

> A value of Completed indicates that the execution is complete. For the information about whether each activity, such as Transcode or Snapshot, is successful, check the status of the activity.

*   **Fail**: The execution failed.
*   **Success**: The execution was successful.', example='Success'),
    }
  ](name='MediaWorkflowExecution')
  }(name='MediaWorkflowExecutionList', description='The details of the media workflows.'),
  nextPageToken?: string(name='NextPageToken', description='The returned value of NextPageToken is a pagination token, which can be used in the next request to retrieve a new page of results.', example='39f8e0bc005e4f309379701645f4****'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='D1D5C080-8E2F-5030-8AB4-13092F17631B'),
}

model ListMediaWorkflowExecutionsResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: ListMediaWorkflowExecutionsResponseBody(name='body'),
}

/**
  * This operation returns execution instances only in the recent 90 days.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function listMediaWorkflowExecutions(request: ListMediaWorkflowExecutionsRequest): ListMediaWorkflowExecutionsResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'ListMediaWorkflowExecutions', 'POST', '/', 'json', false, 'json', request);
}

model QueryAnalysisJobListRequest {
  analysisJobIds: string(name='AnalysisJobIds', description='The template analysis job ID list.', example='bb558c1cc25b45309aab5be44d19****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model QueryAnalysisJobListResponseBody = {
  analysisJobList?: {
    analysisJob?: [ 
    {
      analysisConfig?: {
        propertiesControl?: {
          crop?: {
            height?: string(name='Height', description='The height of the video image after the margins are cropped out.

>  This parameter is invalid if the Mode parameter is set to Auto or None.', example='8'),
            left?: string(name='Left', description='The left margin to be cropped out.

>  This parameter is invalid if the Mode parameter is set to Auto or None.', example='8'),
            mode?: string(name='Mode', description='The cropping mode. Valid values:

*   **Auto**: Cropping was automatically run.
*   **Force**: Cropping was forced to run.
*   **None**: Cropping was forced not to run.
*   This parameter is required if the value of the Crop parameter is not an empty JSON {}.', example='Auto'),
            top?: string(name='Top', description='The top margin to be cropped out.

>  This parameter is invalid if the Mode parameter is set to Auto or None.', example='8'),
            width?: string(name='Width', description='The width of the video image after the margins are cropped out.

>  This parameter is invalid if the Mode parameter is set to Auto or None.', example='8'),
          }(name='Crop', description='The cropping configuration of the video image.'),
          deinterlace?: string(name='Deinterlace', description='Specifies whether deinterlacing was forced to run. Valid values:

*   **Auto**: Deinterlacing was automatically run.
*   **Force**: Deinterlacing was forced to run.
*   **None**: Deinterlacing was forced not to run.', example='Auto'),
        }(name='PropertiesControl', description='The control on the attributes of the job output.'),
        qualityControl?: {
          methodStreaming?: string(name='MethodStreaming', description='The playback mode. Valid values:

*   **network**: online playback.
*   **local**: playback on on-premises devices.
*   Default value: **network**.', example='network'),
          rateQuality?: string(name='RateQuality', description='The quality level of the job output. Default value: **25**.', example='25'),
        }(name='QualityControl', description='The quality control on the job output.'),
      }(name='AnalysisConfig', description='The job configurations.'),
      code?: string(name='Code', description='The error code returned if the job fails.', example='InvalidParameter.ResourceNotFound'),
      creationTime?: string(name='CreationTime', description='The time when the job was created.', example='2014-01-10T12:00:00Z'),
      id?: string(name='Id', description='The ID of the template analysis job.', example='57f6aa3f84824309bcba67231b406****'),
      inputFile?: {
        bucket?: string(name='Bucket', description='The name of the OSS bucket in which the input file is stored.', example='example-bucket'),
        location?: string(name='Location', description='The region in which OSS resides.', example='oss-cn-hangzhou'),
        object?: string(name='Object', description='The name of the Object Storage Service (OSS) object that is used as the input file.', example='example.flv'),
      }(name='InputFile', description='The information about the job input.'),
      MNSMessageResult?: {
        errorCode?: string(name='ErrorCode', description='The error code returned if the job failed. This parameter is not returned if the job was successful.', example='InvalidParameter.ResourceNotFound'),
        errorMessage?: string(name='ErrorMessage', description='The error message returned if the job failed. This parameter is not returned if the job was successful.', example='The resource operated \\"PipelineId\\" cannot be found'),
        messageId?: string(name='MessageId', description='The ID of the message returned if the job was successful. This parameter is not returned if the job failed.', example='3ca84a39a9024f19853b21be9cf9****'),
      }(name='MNSMessageResult', description='The message sent by Message Service (MNS) to notify users of the job result.'),
      message?: string(name='Message', description='The error message returned if the job failed.', example='The resource operated \\"PipelineId\\" cannot be found'),
      percent?: long(name='Percent', description='The transcoding progress.', example='86'),
      pipelineId?: string(name='PipelineId', description='The ID of the MPS queue to which the job was submitted.', example='bb558c1cc25b45309aab5be44d19****'),
      priority?: string(name='Priority', description='The priority of the job in the ApsaraVideo Media Processing (MPS) queue to which the job was submitted.

*   Valid values: **1 to 10**. The value 10 indicates the highest priority.
*   Default value: **10**.', example='8'),
      state?: string(name='State', description='The job status.

*   **Submitted**: The job was submitted.
*   **Analyzing**: The job is in progress.
*   **Success**: The job was successful.
*   **Fail**: The job failed.', example='Success'),
      templateList?: {
        template?: [ 
        {
          audio?: {
            bitrate?: string(name='Bitrate', description='The audio bitrate of the output file.

*   Valid values: **8 to 1000**.
*   Unit: Kbit/s.
*   Default value: **128**.', example='128'),
            channels?: string(name='Channels', description='The number of sound channels. Default value: **2**.', example='2'),
            codec?: string(name='Codec', description='The audio codec. Default value: **aac**. Valid values:

*   **aac**
*   **mp3**
*   **vorbis**
*   **flac**', example='aac'),
            profile?: string(name='Profile', description='The codec profile of the audio. Valid values when the **Codec** parameter is set to **aac**:

*   **aac_low**
*   **aac_he**
*   **aac_he_v2**
*   **aac_ld**
*   **aac_eld**', example='aac_low'),
            qscale?: string(name='Qscale', description='The level of quality control on the audio.', example='15'),
            samplerate?: string(name='Samplerate', description='The sampling rate.

*   Unit: Hz.
*   Default value: **44100**.', example='44100'),
          }(name='Audio', description='The audio codec configurations.'),
          container?: {
            format?: string(name='Format', description='The container format.', example='flv'),
          }(name='Container', description='The container format configurations.'),
          id?: string(name='Id', description='The transcoding template ID.', example='S00000000-00****'),
          muxConfig?: {
            gif?: {
              finalDelay?: string(name='FinalDelay', description='The interval between two consecutive loops for the GIF format. Unit: 0.01 second. For example, a value of 500 indicates 5 seconds.', example='0'),
              loop?: string(name='Loop', description='The number of loops for the GIF or WebP format. Default value: 0.', example='0'),
            }(name='Gif', description='The transmuxing configurations for the GIF format.'),
            segment?: {
              duration?: string(name='Duration', description='The segment length. Unit: seconds.', example='10'),
            }(name='Segment', description='The segment configurations.'),
          }(name='MuxConfig', description='The transmuxing configurations.'),
          name?: string(name='Name', description='The name of the template.', example='FLV-UD'),
          state?: string(name='State', description='The state of the template. Valid values:

*   **Normal**
*   **Deleted**', example='Normal'),
          transConfig?: {
            transMode?: string(name='TransMode', description='The transcoding mode. Default value: **onepass**. Valid values:

*   **onepass**
*   **twopass**
*   **CBR**', example='onepass'),
          }(name='TransConfig', description='The general transcoding configurations.'),
          video?: {
            bitrate?: string(name='Bitrate', description='The average bitrate of the video. Unit: Kbit/s.', example='1000'),
            bitrateBnd?: {
              max?: string(name='Max', description='The upper limit of the total bitrate. Unit: Kbit/s.', example='1500'),
              min?: string(name='Min', description='The lower limit of the total bitrate. Unit: Kbit/s.', example='800'),
            }(name='BitrateBnd', description='The average bitrate range of the video.'),
            bufsize?: string(name='Bufsize', description='The buffer size.

*   Unit: KB.
*   Default value: **6000**.', example='6000'),
            codec?: string(name='Codec', description='The codec. Valid values: H.264 and H.265. Default value: **H.264**.', example='H.264'),
            crf?: string(name='Crf', description='The constant rate factor.

*   Default value when the the Codec parameter is set to H.264: **23**, default value when the the Codec parameter is set to H.265: **26**.
*   If this parameter is specified, the value of the Bitrate parameter becomes invalid.', example='26'),
            degrain?: string(name='Degrain', description='The strength of the independent noise reduction algorithm.', example='5'),
            fps?: string(name='Fps', description='The frame rate of the video.

*   The value is 60 if the frame rate of the input file exceeds 60.
*   Default value: **the frame rate of the input video**.', example='25'),
            gop?: string(name='Gop', description='The maximum number of frames between two keyframes. Default value: **250**.', example='250'),
            height?: string(name='Height', description='The height of the video.

*   Unit: pixel.
*   Default value: **the height of the input video**.', example='720'),
            maxrate?: string(name='Maxrate', description='The maximum bitrate of the video.

*   Valid values: **10 to 50000**.
*   Unit: Kbit/s.', example='2000'),
            pixFmt?: string(name='PixFmt', description='The pixel format of the video. Valid values: standard pixel formats such as yuv420p and yuvj420p.', example='yuv420p'),
            preset?: string(name='Preset', description='The preset video algorithm. Valid values: veryfast, fast, medium, slow, and slower. Default value: **medium**.', example='medium'),
            profile?: string(name='Profile', description='The codec profile. Valid values:

*   **baseline**: applicable to mobile devices.
*   **main**: applicable to standard-definition devices.
*   **high**: applicable to high-definition devices.
*   Default value: **high**.', example='high'),
            qscale?: string(name='Qscale', description='The level of quality control on the video.', example='15'),
            scanMode?: string(name='ScanMode', description='The scan mode. Valid values:

*   **interlaced**
*   **progressive**', example='interlaced'),
            width?: string(name='Width', description='The width of the video.

*   Unit: pixel.
*   Default value: **the width of the input video**.', example='1280'),
          }(name='Video', description='The video codec configurations.'),
        }
      ](name='Template')
      }(name='TemplateList', description='The matched preset templates.'),
      userData?: string(name='UserData', description='The custom data.', example='testid-001'),
    }
  ](name='AnalysisJob')
  }(name='AnalysisJobList', description='The IDs of template analysis jobs.'),
  nonExistAnalysisJobIds?: {
    string?: [ string ](name='String')
  }(name='NonExistAnalysisJobIds', description='The message sent by Message Service (MNS) to notify the user of the job result.'),
  requestId?: string(name='RequestId', description='The status of the job. Valid values:

*   **Submitted**: The job has been submitted.
*   **Analyzing**: The job is being run.
*   **Success**: The job is successful.
*   **Fail**: The job fails.', example='5CA6E020-4102-4FFF-AA56-5ED7ECD811A1'),
}

model QueryAnalysisJobListResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: QueryAnalysisJobListResponseBody(name='body'),
}

/**
  * The time when the job was created.
  *
 */
async function queryAnalysisJobList(request: QueryAnalysisJobListRequest): QueryAnalysisJobListResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'QueryAnalysisJobList', 'POST', '/', 'json', false, 'json', request);
}

model QueryCopyrightExtractJobRequest {
  jobId: string(name='JobId', example='2288c6ca184c0e47098a5b665e2a12****', position='Query'),
}

model QueryCopyrightExtractJobResponseBody = {
  data?: {
    message?: string(name='Message', example='example water mark'),
  }(name='Data'),
  message?: string(name='Message', example='ok'),
  requestId?: string(name='RequestId', description='Id of the request', example='F24EAE86-5356-528E-A2B1-FEDE269F42DD'),
  statusCode?: long(name='StatusCode', example='200'),
}

model QueryCopyrightExtractJobResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: QueryCopyrightExtractJobResponseBody(name='body'),
}

async function queryCopyrightExtractJob(request: QueryCopyrightExtractJobRequest): QueryCopyrightExtractJobResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'QueryCopyrightExtractJob', 'POST', '/', 'json', false, 'json', request);
}

model QueryCopyrightJobRequest {
  createTimeEnd?: long(name='CreateTimeEnd', example='1627357325', position='Query'),
  createTimeStart?: long(name='CreateTimeStart', example='1627357322', position='Query'),
  jobId?: string(name='JobId', example='2a0697e35a7342859f733a9190c4****', position='Query'),
  level?: long(name='Level', example='2', position='Query'),
  pageNumber?: long(name='PageNumber', example='0', position='Query'),
  pageSize?: long(name='PageSize', example='1', position='Query'),
}

model QueryCopyrightJobResponseBody = {
  data?: [ 
    {
      callback?: string(name='Callback', example='http://callbacktest.com/callback'),
      gmtCreate?: long(name='GmtCreate', example='1627357322'),
      gmtModified?: long(name='GmtModified', example='1627357328'),
      input?: string(name='Input', example='{"Bucket":"ivison-test","Location":"oss-cn-shanghai","Object":"gambling.mp4"}'),
      jobId?: string(name='JobId', example='bfb786c639894f4d80648792021e****'),
      level?: long(name='Level', example='2'),
      message?: string(name='Message', example='平头哥半导体(上海)'),
      output?: string(name='Output', example='{"Bucket":"ivison-test","Location":"oss-cn-shanghai","Object":"out.mp4"}'),
      result?: string(name='Result', example='{"Code":"success","Message":"ok"}'),
      status?: string(name='Status', example='success'),
      userData?: string(name='UserData', example='123'),
      userId?: long(name='UserId', example='1346693***'),
    }
  ](name='Data'),
  message?: string(name='Message', example='ok'),
  requestId?: string(name='RequestId', description='Id of the request', example='5CA6E020-4102-4FFF-AA56-5ED7ECD811A1'),
  statusCode?: long(name='StatusCode', example='200'),
}

model QueryCopyrightJobResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: QueryCopyrightJobResponseBody(name='body'),
}

async function queryCopyrightJob(request: QueryCopyrightJobRequest): QueryCopyrightJobResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'QueryCopyrightJob', 'POST', '/', 'json', false, 'json', request);
}

model QueryFpDBDeleteJobListRequest {
  jobIds?: string(name='JobIds', description='The IDs of the jobs of clearing or deleting a media fingerprint library. You can obtain the job IDs from the response parameters of the [SubmitFpDBDeleteJob](~~209341~~) operation. Separate multiple job IDs with commas (,). If you leave this parameter empty, the system returns the latest 20 jobs that are submitted.', example='2288c6ca184c0e47098a5b665e2a12****,78dc866518b843259669df58ed30****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model QueryFpDBDeleteJobListResponseBody = {
  fpDBDeleteJobList?: {
    fpDBDeleteJob?: [ 
    {
      code?: string(name='Code', description='The error code returned if the job fails. This parameter is not returned if the job is successful.', example='ServiceUnavailable'),
      creationTime?: string(name='CreationTime', description='The time when the job was created.', example='2020-06-30T00:33:18Z'),
      delType?: string(name='DelType', description='The type of the operation.', example='Purge'),
      finishTime?: string(name='FinishTime', description='The time when the job was complete.', example='2020-06-30T00:34:02Z'),
      fpDBId?: string(name='FpDBId', description='The ID of the media fingerprint library.', example='88c6ca184c0e47098a5b665e2a12****'),
      id?: string(name='Id', description='The ID of the job.', example='25bacf2824614bcf9273dc0744db****'),
      message?: string(name='Message', description='The error message returned if the job fails. This parameter is not returned if the job is successful.', example='The request has failed due to a temporary failure of the server.'),
      pipelineId?: string(name='PipelineId', description='The ID of the ApsaraVideo Media Processing (MPS) queue to which the job was submitted.', example='fb712a6890464059b1b2ea7c8647****'),
      status?: string(name='Status', description='The status of the job. Valid values:

*   **Queuing**: The job is waiting in the queue.
*   **Analysing**: The job is in progress.
*   **Success**: The job is successful.
*   **Fail**: The job fails.', example='Success'),
      userData?: string(name='UserData', description='The user-defined data.', example='example data'),
    }
  ](name='FpDBDeleteJob')
  }(name='FpDBDeleteJobList', description='The jobs of deleting a media fingerprint library. For more information, see the "FpDBDeleteJob" section of the [Data types](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/datatypes) topic.'),
  nonExistIds?: {
    string?: [ string ](name='String')
  }(name='NonExistIds', description='The IDs of the jobs that do not exist.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='4247B23C-26DE-529F-8D9F-FD6811AE979B'),
}

model QueryFpDBDeleteJobListResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: QueryFpDBDeleteJobListResponseBody(name='body'),
}

/**
  * You can call this operation to query the specified jobs of clearing or deleting a media fingerprint library based on the job IDs. If you do not specify job IDs, the system returns the latest 20 jobs that are submitted.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function queryFpDBDeleteJobList(request: QueryFpDBDeleteJobListRequest): QueryFpDBDeleteJobListResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'QueryFpDBDeleteJobList', 'POST', '/', 'json', false, 'json', request);
}

model QueryFpFileDeleteJobListRequest {
  jobIds?: string(name='JobIds', description='The IDs of the jobs of deleting media files from a media fingerprint library. You can obtain the job IDs from the response parameters of the [SubmitFpFileDeleteJob](~~209274~~) operation. Separate multiple job IDs with commas (,). If you leave this parameter empty, the system returns the latest 20 jobs that are submitted.', example='d98459323c024947a104f6a50cbf****,c2dc694696f1441591c5012a73c1****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model QueryFpFileDeleteJobListResponseBody = {
  fpFileDeleteJobList?: {
    fpFileDeleteJob?: [ 
    {
      code?: string(name='Code', description='The error code returned if the job fails. This parameter is not returned if the job is successful.', example='ServiceUnavailable'),
      creationTime?: string(name='CreationTime', description='The time when the job was created.', example='2020-06-30T00:33:18Z'),
      fileIds?: string(name='FileIds', description='The ID of the file.', example='41e6536e4f2250e2e9bf26cdea19****'),
      finishTime?: string(name='FinishTime', description='The time when the job was complete.', example='2020-06-30T00:34:02Z'),
      fpDBId?: string(name='FpDBId', description='The ID of the media fingerprint library.', example='88c6ca184c0e47098a5b665e2a12****'),
      id?: string(name='Id', description='The ID of the job.', example='25bacf2824614bcf9273dc0744db****'),
      message?: string(name='Message', description='The error message returned if the job fails. This parameter is not returned if the job is successful.', example='The request has failed due to a temporary failure of the server.'),
      pipelineId?: string(name='PipelineId', description='The ID of the ApsaraVideo Media Processing (MPS) queue to which the job was submitted.', example='fb712a6890464059b1b2ea7c8647****'),
      status?: string(name='Status', description='The status of the job. Valid values: Valid values:

*   **Queuing**: The job is waiting in the queue.
*   **Analysing**: The job is in progress.
*   **Success**: The job is successful.
*   **Fail**: The job fails.', example='Success'),
      userData?: string(name='UserData', description='The user-defined data.', example='example data'),
    }
  ](name='FpFileDeleteJob')
  }(name='FpFileDeleteJobList', description='The jobs of deleting media files from a media fingerprint library. For more information, see the "FpFileDeleteJob" section of the [Data types](~~93555~~) topic.'),
  nonExistIds?: {
    string?: [ string ](name='String')
  }(name='NonExistIds', description='The response parameters.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='D127C68E-F1A1-4CE5-A874-8FF724881A12'),
}

model QueryFpFileDeleteJobListResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: QueryFpFileDeleteJobListResponseBody(name='body'),
}

/**
  * You can call this operation to query the specified jobs of deleting media files from a media fingerprint library based on the job IDs. If you do not specify job IDs, the system returns the latest 20 jobs that are submitted.
  * ### QPS limit
  * You can call this operation up to 500 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function queryFpFileDeleteJobList(request: QueryFpFileDeleteJobListRequest): QueryFpFileDeleteJobListResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'QueryFpFileDeleteJobList', 'POST', '/', 'json', false, 'json', request);
}

model QueryFpShotJobListRequest {
  endOfJobCreatedTimeRange?: string(name='EndOfJobCreatedTimeRange', description='The end of the time range within which the jobs to be queried were created. 

*   Specify the time in the ISO 8601 standard in the
*   YYYY-MM-DDThh:mm:ssZ format. The time must be in UTC.', example='2022-02-14T02:16:07Z', position='Query'),
  jobIds?: string(name='JobIds', description='The ID of the media fingerprint analysis job that you want to query. To view the job ID, log on to the [ApsaraVideo Media Processing (MPS) console](https://mps.console.aliyun.com/overview), click **Tasks** in the left-side navigation pane, and then click the **Video DNA** tab on the Tasks page. You can query up to 10 media fingerprint analysis jobs at a time. Separate multiple job IDs with commas (,).', example='88c6ca184c0e47098a5b665e2a12****', position='Query'),
  maximumPageSize?: long(name='MaximumPageSize', description='The maximum number of entries to return on each page. 

*   Default value: **10**.
*   Valid values: **1 to 100**.', example='10', minimum=1, maximum=100, position='Query'),
  nextPageToken?: string(name='NextPageToken', description='The token that is used to retrieve the next page of the query results. You do not need to specify this parameter in the first request. The response to the first request contains this parameter, which you add to the next request.', example='16f01ad6175e4230ac42bb5182cd****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  pipelineId?: string(name='PipelineId', description='The ID of the MPS queue. To view the ID of the MPS queue, log on to the [MPS console](https://mps.console.aliyun.com/overview) and choose **Global Settings** > **Pipelines** in the left-side navigation pane.', example='b11c171cced04565b1f38f1ecc39****', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  startOfJobCreatedTimeRange?: string(name='StartOfJobCreatedTimeRange', description='The beginning of the time range within which the jobs to be queried were created. 

*   Specify the time in the ISO 8601 standard in the
*   YYYY-MM-DDThh:mm:ssZ format. The time must be in UTC.', example='2021-12-22T03:48:05Z', position='Query'),
  state?: string(name='State', description='The status of the jobs to be queried. Valid values:

*   **All**: all jobs.
*   **Queuing**: the jobs that are being queued.
*   **Analysing**: the jobs that are in progress.
*   **Fail**: failed jobs.
*   **Success**: successful jobs.', example='All', position='Query'),
}

model QueryFpShotJobListResponseBody = {
  fpShotJobList?: {
    fpShotJob?: [ 
    {
      code?: string(name='Code', description='The error code returned if the job fails.', example='InvalidParameter.UUIDFormatInvalid'),
      creationTime?: string(name='CreationTime', description='The time when the job was created.', example='2017-01-10T12:00:00Z'),
      duration?: int32(name='Duration', description='The length of the input file.
Unit: seconds.', example='5'),
      fileId?: string(name='FileId', description='The ID of the uploaded file.', example='ebb51ee30f0b49aba959823fa991****'),
      finishTime?: string(name='FinishTime', description='The time when the job was complete.', example='0'),
      fpShotConfig?: {
        fpDBId?: string(name='FpDBId', description='The ID of the media fingerprint library.', example='2288c6ca184c0e47098a5b665e2a12****'),
        primaryKey?: string(name='PrimaryKey', description='The unique primary key of the video.', example='3ca84a39a9024f19853b21be9cf9****'),
        saveType?: string(name='SaveType', description='The storage type. Valid values:

*   **nosave**: The fingerprints of the job input are not saved to the media fingerprint library.
*   **save**: The fingerprints of the job input are saved to the media fingerprint library only if the job input is not duplicated with media content in the media fingerprint library.
*   **forcesave**: The fingerprints of the job input are forcibly saved to the media fingerprint library.', example='save'),
      }(name='FpShotConfig', description='The configurations of the job.'),
      fpShotResult?: {
        audioFpShots?: {
          fpShot?: [ 
          {
            fpShotSlices?: {
              fpShotSlice?: [ 
              {
                duplication?: {
                  duration?: string(name='Duration', description='The duration of the similar audio clip in the audio file that has similar fingerprints to the input audio in the audio fingerprint library.', example='3'),
                  start?: string(name='Start', description='The start point in time of the similar audio clip in the audio file that has similar fingerprints to the input audio in the audio fingerprint library.', example='0'),
                }(name='Duplication', description='The start point in time and duration of the similar audio clip in the audio file that has similar fingerprints to the input audio in the audio fingerprint library.'),
                input?: {
                  duration?: string(name='Duration', description='The duration of the similar audio clip in the input audio.', example='5'),
                  start?: string(name='Start', description='The start point in time of the similar audio clip in the input audio.', example='0'),
                }(name='Input', description='The start point in time and duration of the similar audio clip in the input audio.'),
                similarity?: string(name='Similarity', description='The similarity of the input audio against the audio file that has similar fingerprints to the input audio in the audio fingerprint library.', example='0'),
              }
            ](name='FpShotSlice')
            }(name='FpShotSlices', description='The audio files that have similar fingerprints to the input audio in the audio fingerprint library.'),
            primaryKey?: string(name='PrimaryKey', description='The unique primary key of the input audio.', example='498ac941373341599c4777c8d884****'),
            similarity?: string(name='Similarity', description='The overall similarity of the input audio against audio files that have similar fingerprints to the input audio in the audio fingerprint library.', example='0'),
          }
        ](name='FpShot')
        }(name='AudioFpShots', description='The audio fingerprint analysis results.'),
        fpShots?: {
          fpShot?: [ 
          {
            fpShotSlices?: {
              fpShotSlice?: [ 
              {
                duplication?: {
                  duration?: string(name='Duration', description='The duration of the similar video clip in the video file that has similar fingerprints to the input video in the video fingerprint library.', example='48'),
                  start?: string(name='Start', description='The start point in time of the similar video clip in the video file that has similar fingerprints to the input video in the video fingerprint library.', example='1260'),
                }(name='Duplication', description='The start point in time and duration of the similar video clip in the video file that has similar fingerprints to the input video in the video fingerprint library.'),
                input?: {
                  duration?: string(name='Duration', description='The duration of the similar video clip in the input video.', example='48'),
                  start?: string(name='Start', description='The start point in time of the similar video clip in the input video.', example='46'),
                }(name='Input', description='The start time and duration of the similar video clip in the input video.'),
                similarity?: string(name='Similarity', description='The similarity of the input video clip against the video file that has similar fingerprints to the input video in the video fingerprint library.', example='0'),
              }
            ](name='FpShotSlice')
            }(name='FpShotSlices', description='The video files that have similar fingerprints to the input video in the video fingerprint library.'),
            primaryKey?: string(name='PrimaryKey', description='The unique primary key of the input video.', example='498ac941373341599c4777c8d884****'),
            similarity?: string(name='Similarity', description='The overall similarity of the input video against video files that have similar fingerprints to the input video in the video fingerprint library.

>  The overall similarity is the average value of the similarities of the input video clips with the clips of the video that has a similar fingerprint. If multiple video files that have similar fingerprints to the input video exist in the video fingerprint library, the similarities of the input video against multiple similar video clips are returned.', example='0.8914769887924194'),
          }
        ](name='FpShot')
        }(name='FpShots', description='The video fingerprint analysis results.'),
        textFpShots?: {
          textFpShot?: [ 
          {
            primaryKey?: string(name='PrimaryKey', description='The unique primary key of the input text.', example='3e34ac649945b53a1b0f863ce030****'),
            similarity?: string(name='Similarity', description='The similarity of the input text against text snippets that have similar fingerprints to the input text in the text fingerprint library.', example='1.0'),
            textFpShotSlices?: {
              textFpShotSlice?: [ 
              {
                duplicationText?: string(name='DuplicationText', description='The text snippet that has similar fingerprints to the input text in the text fingerprint library.', example='It\\"s snowy today.'),
                inputFragment?: {
                  duration?: string(name='Duration', description='The duration of the similar text snippet in the input text.', example='3'),
                  start?: string(name='Start', description='The start time of the similar text snippet in the input text.', example='0'),
                }(name='InputFragment', description='The start point in time and duration of the similar text snippet in the input text.'),
                inputText?: string(name='InputText', description='The input text for text fingerprint analysis.', example='It\\"s snowy today.'),
                similarity?: string(name='Similarity', description='The similarity of the input text against the text snippet that has similar fingerprints to the input text in the text fingerprint library.', example='1.0'),
              }
            ](name='TextFpShotSlice')
            }(name='TextFpShotSlices', description='The text snippets that have similar fingerprints to the input text in the text fingerprint library.'),
          }
        ](name='TextFpShot')
        }(name='TextFpShots', description='The text fingerprint analysis results.'),
      }(name='FpShotResult', description='The results of the media fingerprint analysis job.'),
      id?: string(name='Id', description='The ID of the job.', example='88c6ca184c0e47098a5b665e2a12****'),
      input?: string(name='Input', description='The information about the job input.', example='{"Bucket":"oss-test","Location":"oss-cn-beijing","Object":"test.mp4"}'),
      inputFile?: {
        bucket?: string(name='Bucket', description='The OSS bucket in which the job input resides.', example='oss-test'),
        location?: string(name='Location', description='The OSS region in which the job input resides.', example='oss-cn-beijing'),
        object?: string(name='Object', description='The Object Storage Service (OSS) object that is used as the job input.', example='test.mp4'),
      }(name='InputFile', description='The information about the job input.'),
      message?: string(name='Message', description='The error message returned if the job fails. This parameter is not returned if the job is successful.', example='The parameter \\"Id\\" is invalid.A uuid must:1)be comprised of chracters[a-f],numbers[0-9];2)be 32 characters long'),
      pipelineId?: string(name='PipelineId', description='The ID of the MPS queue to which the analysis job is submitted.', example='88c6ca184c0e47098a5b665e2a12****'),
      state?: string(name='State', description='The status of the job. Valid values:

*   **Queuing**: The job is waiting in the queue.
*   **Analysing**: The job is in progress.
*   **Success**: The job is successful.
*   **Fail**: The job fails.', example='Success'),
      userData?: string(name='UserData', description='The custom data.', example='testid-001'),
    }
  ](name='FpShotJob')
  }(name='FpShotJobList', description='The information about media fingerprint analysis jobs.'),
  nextPageToken?: string(name='NextPageToken', description='The token that is used to retrieve the next page of the query results. The value is a 32-bit UUID. If the returned query results cannot be displayed within one page, this parameter is returned. The value of this parameter is updated for each query.', example='b11c171cced04565b1f38f1ecc39****'),
  nonExistIds?: {
    string?: [ string ](name='String')
  }(name='NonExistIds', description='The IDs of the jobs that do not exist.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='25818875-5F78-4A13-BEF6-D7393642CA58'),
}

model QueryFpShotJobListResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: QueryFpShotJobListResponseBody(name='body'),
}

/**
  * *   After a media fingerprint analysis job is submitted, the media fingerprinting service compares the fingerprints of the job input with those of the media files in the media fingerprint library. You can call this operation to query the job results.
  * *   You can query the results of a text fingerprint analysis job only in the China (Shanghai) region.
  * ### [](#qps)QPS limits
  * You can call this API operation up to 100 times per second per account. Requests that exceed this limit are dropped, and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limits](~~342832~~).
  *
 */
async function queryFpShotJobList(request: QueryFpShotJobListRequest): QueryFpShotJobListResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'QueryFpShotJobList', 'POST', '/', 'json', false, 'json', request);
}

model QueryIProductionJobRequest {
  jobId?: string(name='JobId', example='88c6ca184c0e432bbf5b665e2a15****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model QueryIProductionJobResponseBody = {
  functionName?: string(name='FunctionName', example='ImageCartoonize'),
  input?: string(name='Input', example='oss://example-****.oss-cn-hangzhou.aliyuncs.com/example.mp4'),
  jobId?: string(name='JobId', example='88c6ca184c0e432bbf5b665e2a15****'),
  jobParams?: string(name='JobParams', example='{mode:"gif"}'),
  output?: string(name='Output', example='oss://example-****.oss-cn-hangzhou.aliyuncs.com/iproduction/{source}-{timestamp}-{sequenceId}.srt'),
  pipelineId?: string(name='PipelineId', example='39f8e0bc005e4f309379701645f4****'),
  requestId?: string(name='RequestId', example='D127C68E-F1A1-4CE5-A874-8FF724881A12'),
  result?: string(name='Result', example='{"Code":"Success","Data":"{\\"result\\":[{\\"file\\":\\"iproduction/test-result.jpg\\"},{\\"file\\":\\"iproduction/test-origin.jpg\\"}]}","Message":"Successful."}'),
  state?: string(name='State', example='Success'),
  userData?: string(name='UserData', example='null'),
}

model QueryIProductionJobResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: QueryIProductionJobResponseBody(name='body'),
}

async function queryIProductionJob(request: QueryIProductionJobRequest): QueryIProductionJobResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'QueryIProductionJob', 'POST', '/', 'json', false, 'json', request);
}

model QueryJobListRequest {
  jobIds?: string(name='JobIds', description='The IDs of transcoding jobs. Separate multiple IDs with commas (,). You can query a maximum of 10 transcoding jobs at a time. You can log on to the [ApsaraVideo Media Processing (MPS) console](https://mps.console.aliyun.com/overview) and click **Tasks** in the left-side navigation pane to obtain job IDs. Alternatively, you can obtain job IDs from the response to the [SubmitJobs](~~29226~~) operation.

>  If you do not set the JobIds parameter, the `InvalidParameter` error code is returned.', example='bb558c1cc25b45309aab5be44d19****,d1ce4d3efcb549419193f50f1fcd****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model QueryJobListResponseBody = {
  jobList?: {
    job?: [ 
    {
      code?: string(name='Code', description='The error code returned if the job failed. If the job was successful, this parameter is not returned.', example='InvalidParameter.NullValue'),
      creationTime?: string(name='CreationTime', description='The time when the job was created.', example='2014-01-10T12:00:00Z'),
      finishTime?: string(name='FinishTime', description='The time when the job was complete.', example='2014-01-10T12:20:25Z'),
      input?: {
        bucket?: string(name='Bucket', description='The name of the OSS bucket in which the input file is stored.', example='exampleBucket'),
        location?: string(name='Location', description='The OSS region in which the input file resides.', example='oss-cn-shanghai'),
        object?: string(name='Object', description='The name of the OSS object that is used as the input file.', example='video_01.mp4'),
      }(name='Input', description='The information about the job input.'),
      jobId?: string(name='JobId', description='The job ID.', example='31fa3c9ca8134fb4b0b0f7878301****'),
      MNSMessageResult?: {
        errorCode?: string(name='ErrorCode', description='The error code returned if the job failed. This parameter is not returned if the job was successful.', example='InvalidParameter.ResourceNotFound'),
        errorMessage?: string(name='ErrorMessage', description='The error message returned if the job failed. This parameter is not returned if the job was successful.', example='The resource operated “%s” cannot be found.'),
        messageId?: string(name='MessageId', description='The ID of the message returned if the job was successful.', example='123'),
      }(name='MNSMessageResult', description='The message sent by Message Service (MNS) to notify users of the job result.'),
      message?: string(name='Message', description='The error message returned if the job failed. If the job was successful, this parameter is not returned.', example='The specified parameter "%s" cannot be null.'),
      output?: {
        audio?: {
          bitrate?: string(name='Bitrate', description='The audio bitrate of the output file.

*   Unit: Kbit/s.
*   Default value: **128**.', example='128'),
          channels?: string(name='Channels', description='The number of sound channels.

*   Valid values: 1, 2, 3, 4, 5, 6, 7, and 8.
*   Default value: **2**.', example='2'),
          codec?: string(name='Codec', description='The audio codec.

*   Valid values: aac, mp3, vorbis, and flac.
*   Default value: **aac**.', example='aac'),
          profile?: string(name='Profile', description='The codec profile of the audio. Valid values when the value of Codec is aac: aaclow, aache, aachev2, aacld, and aaceld.', example='aaclow'),
          qscale?: string(name='Qscale', description='The level of quality control on the audio.', example='15'),
          samplerate?: string(name='Samplerate', description='The sampling rate.

*   Valid values: 22050, 32000, 44100, 48000, and 96000.
*   Unit: Hz.
*   Default value: 44100.

>  If the video container format is FLV and the audio codec is MP3, the value of this parameter cannot be 32000, 48000, or 96000. If the audio codec is MP3, the value of this parameter cannot be 96000.', example='44100'),
          volume?: {
            level?: string(name='Level', description='The volume adjustment range. Default value: -20. Unit: dB.', example='-20'),
            method?: string(name='Method', description='The method that is used to adjust the volume. Valid values:

*   **auto**
*   **dynamic**
*   **linear**', example='auto'),
          }(name='Volume', description='The volume configurations.'),
        }(name='Audio', description='The audio configurations.

>  If this parameter is specified in the request, the corresponding parameters in the specified transcoding template are overwritten.'),
        audioStreamMap?: string(name='AudioStreamMap', description='The sequence number of the audio stream.

*   Format: 0:a:{Sequence number}. Example: 0:a:0.
*   The sequence number is the index of the audio stream in the list and starts from 0.
*   If no sequence number is specified, the default audio stream is used.', example='0:a:0'),
        clip?: {
          timeSpan?: {
            duration?: string(name='Duration', description='The duration of the clip.

*   Format: `hh:mm:ss[.SSS]`.
*   Example: 01:00:59.999.

Or

*   Format: `sssss[.SSS]`.
*   Example: 32000.23.', example='01:00:59.999'),
            seek?: string(name='Seek', description='The point in time when the clip starts.

*   Format: `hh:mm:ss[.SSS]`.
*   Example: 01:59:59.999.

Or

*   Format: `sssss[.SSS]`.
*   Example: 32000.23.', example='01:59:59.999'),
          }(name='TimeSpan', description='The time span of the clip.'),
        }(name='Clip', description='The information about clips.'),
        container?: {
          format?: string(name='Format', description='The container format.

*   Default value: mp4.
*   Video formats include FLV, MP4, HLS (M3U8 + TS), and MPEG-DASH (MPD + fMP4).
*   Audio formats include MP3, MP4, Ogg, FLAC, and M4A.
*   Image formats include GIF and WebP. If the container format is GIF, the video codec must be GIF.
*   If the container format is WebP, the video codec must be WebP.
*   If the container format is FLV, the video codec cannot be H.265.', example='mp4'),
        }(name='Container', description='The container format configurations.'),
        deWatermark?: string(name='DeWatermark', description='The configurations of watermark blurring. The value is a JSON object. For more information, see the DeWatermark section of the [Parameter details](~~29253~~) topic.', example='{"0":[{"l":10,"t":10,"w":10,"h":10},{"l":100,"t":0.1,"w":10,"h":10}],"128000":[],"250000":[{"l":0.2,"t":0.1,"w":0.01,"h":0.05}]}'),
        encryption?: {
          id?: string(name='Id', description='The encryption ID.', example='31fa3c9ca8134f9cec2b4b0b0f78****'),
          key?: string(name='Key', description='The key that is used to encrypt the video.', example='encryptionkey128'),
          keyType?: string(name='KeyType', description='The key encryption method. Valid values: Base64 and KMS.

>  For example, if the key is `encryptionkey128`, the key can be encrypted as `Base64("encryptionkey128")` or `KMS(Base64("encryptionkey128")` depending on the encryption method used.', example='Base64'),
          keyUri?: string(name='KeyUri', description='The URL that is used to request the key. The URL is Base64-encoded.', example='https://1161758785*****.cn-shanghai.fc.aliyuncs.com/2016-08-15/proxy/HLS-decyptServer/decyptServer/'),
          skipCnt?: string(name='SkipCnt', description='The number of unencrypted frames at the beginning of the video. Leaving these frames unencrypted enables video playback to quickly start.', example='3'),
          type?: string(name='Type', description='The encryption type. Only **hls-aes-128** may be returned.', example='hls-aes-128'),
        }(name='Encryption', description='The encryption configurations. The encrypted video file is generated in the M3U8 format.'),
        extendData?: string(name='ExtendData', description='The custom fields.', example='testid-002'),
        m3U8NonStandardSupport?: {
          ts?: {
            md5Support?: boolean(name='Md5Support', description='Indicates whether the output of the MD5 value of the TS file is supported in the M3U8 file. Valid values:

*   **true**
*   **false**', example='true'),
            sizeSupport?: boolean(name='SizeSupport', description='Indicates whether the output of the size of the TS file is supported in the M3U8 file. Valid values:

*   **true**
*   **false**', example='true'),
          }(name='TS', description='The non-standard support configurations for TS files. The value is a JSON object. For more information, see the TS section of the [Parameter details](~~29253~~) topic.'),
        }(name='M3U8NonStandardSupport', description='The non-standard support configurations for M3U8. The value is a JSON object. For more information, see the M3U8NonStandardSupport section of the [Parameter details](~~29253~~) topic.'),
        mergeConfigUrl?: string(name='MergeConfigUrl', description='The URL of the merging configuration file. Only one of MergeList and MergeConfigUrl takes effect.

*   The configuration file specified by MergeConfigUrl can contain up to 50 clips.
*   MergeConfigUrl indicates the URL of the configuration file for merging clips. Make sure that the configuration file is stored as an object in OSS and that MPS can access the OSS object. For information about the file content, see the details about merging parameters.
*   Example of the content of the merging configuration file: `{"MergeList":[{"MergeURL":"http://exampleBucket****.oss-cn-hangzhou.aliyuncs.com/video_01.mp4"}]}`.', example='https://ceshi-***.oss-cn-shanghai.aliyuncs.com/ccc/p0903q9wkkb.m3u8'),
        mergeList?: {
          merge?: [ 
          {
            duration?: string(name='Duration', description='The duration of the clip.

*   Format: `hh:mm:ss[.SSS]` or `sssss[.SSS]`.
*   Examples: 01:59:59.999 and 32000.23.', example='01:59:59.999'),
            mergeURL?: string(name='MergeURL', description='The OSS URL of the clip.

*   Example: `http://example-bucket-.oss-cn-hangzhou.aliyuncs.com/example-object.flv`.
*   The object must be URL-encoded by using the UTF-8 standard. For more information, see [URL encoding](~~423796~~).', example='http://example-bucket.oss-cn-hangzhou.aliyuncs.com/example-object.flv'),
            roleArn?: string(name='RoleArn', description='The Alibaba Cloud Resource Name (ARN) of the Resource Access Management (RAM) role used for delegated authorization.', example='acs:ram::<your uid>:role/<your role name>'),
            start?: string(name='Start', description='The start point in time of the clip.

*   Format: `hh:mm:ss[.SSS]` or `sssss[.SSS]`.
*   Examples: 01:59:59.999 and 32000.23.', example='01:59:59.999'),
          }
        ](name='Merge')
        }(name='MergeList', description='The configurations of clip merging. Up to four clips can be merged.'),
        multiSpeedInfo?: {
          code?: string(name='Code', description='The error code returned if high-speed transcoding is not enabled.', example='Boost.NotNeedSpeed'),
          downgradePolicy?: string(name='DowngradePolicy', description='The downgrade policy if high-speed transcoding is not supported.', example='NormalSpeed'),
          duration?: double(name='Duration', description='The duration of the output video.', example='21.0'),
          enable?: string(name='Enable', description='Indicates whether high-speed transcoding is enabled.', example='true'),
          message?: string(name='Message', description='The error message returned if high-speed transcoding is not enabled.', example='success'),
          realSpeed?: double(name='RealSpeed', description='The actual transcoding speed.', example='6.576886940181647'),
          settingSpeed?: int32(name='SettingSpeed', description='The speed setting.', example='30'),
          timeCost?: double(name='TimeCost', description='The amount of time consumed.', example='3.193'),
        }(name='MultiSpeedInfo', description='The information about the high-speed transcoding job. This information is available only for jobs that are submitted by using an MPS queue for high-speed transcoding. This does not support MPS queues for high-speed transcoding of an earlier version.'),
        muxConfig?: {
          gif?: {
            ditherMode?: string(name='DitherMode', description='The color dithering algorithm of the palette. Valid values: sierra and bayer.', example='bayer'),
            finalDelay?: string(name='FinalDelay', description='The duration for which the final frame is paused. Unit: centisecond.', example='0'),
            isCustomPalette?: string(name='IsCustomPalette', description='Indicates whether a custom palette is used. Valid values:

*   **true**
*   **false**', example='false'),
            loop?: string(name='Loop', description='The loop count.', example='0'),
          }(name='Gif', description='The transmuxing configurations for GIF.'),
          segment?: {
            duration?: string(name='Duration', description='The segment length. Unit: seconds.', example='20'),
          }(name='Segment', description='The segment configurations. The value is a JSON object.'),
          webp?: {
            loop?: string(name='Loop', description='The loop count.', example='0'),
          }(name='Webp', description='The transmuxing configurations for WebP.'),
        }(name='MuxConfig', description='The transmuxing configurations. The transmuxing configurations. If this parameter is specified in the request, the corresponding parameters in the specified transcoding template are overwritten.'),
        openingList?: {
          opening?: [ 
          {
            height?: string(name='Height', description='The height of the opening part.

*   Valid values: values in the range of (0,4096), -1, and full.
*   A value of -1 indicates that the original height of the opening part is retained.
*   A value of full indicates that the height of the opening part equals the height of the main part.
*   Default value: **-1**.', example='-1'),
            start?: string(name='Start', description='The amount of time after which the opening part is played.

*   The value starts from 0.
*   Unit: seconds.
*   Default value: **0**.', example='0'),
            width?: string(name='Width', description='The width of the opening part.

*   Valid values: values in the range of (0,4096), -1, and full.
*   A value of -1 indicates that the original width of the opening part is retained.
*   A value of full indicates that the width of the opening part equals the width of the main part.
*   Default value: **-1**.', example='-1'),
            openUrl?: string(name='openUrl', description='The OSS URL of the opening part.', example='http://example.oss-cn-shanghai.aliyuncs.com/t5.mp4'),
          }
        ](name='Opening')
        }(name='OpeningList', description='The opening parts. The value is a JSON object.'),
        outSubtitleList?: {
          outSubtitle?: [ 
          {
            map?: string(name='Map', description='The video track. Format: `0:{Stream}:{Stream sequence number}`, which is `0:v:{video_index}`. The value of Stream is v, which indicates a video stream. The sequence number is the index of the video stream in the list and starts from 0.', example='0:v:0'),
            message?: string(name='Message', description='The error message returned if the job failed to be created. This parameter is not returned if the job was created.', example='The specified parameter “%s” cannot be null.'),
            outSubtitleFile?: {
              bucket?: string(name='Bucket', description='The name of the OSS bucket in which the output caption is stored.', example='exampleBucket'),
              location?: string(name='Location', description='The OSS region in which the output caption resides.', example='oss-cn-hangzhou'),
              object?: string(name='Object', description='The name of the OSS object that is used as the output caption.', example='example.flv'),
              roleArn?: string(name='RoleArn', description='The ARN of the RAM role used for delegated authorization.', example='acs:ram::<your uid>:role/<your role name>'),
            }(name='OutSubtitleFile', description='The details of the output caption.'),
            success?: boolean(name='Success', description='Indicates whether the job was successful. Valid values:

*   **true**: The job was successful.
*   **false**: The job failed.', example='true'),
          }
        ](name='OutSubtitle')
        }(name='OutSubtitleList', description='The output captions.'),
        outputFile?: {
          bucket?: string(name='Bucket', description='The name of the OSS bucket in which the output file is stored.', example='example-bucket'),
          location?: string(name='Location', description='The OSS region in which the output file resides.', example='oss-cn-hangzhou'),
          object?: string(name='Object', description='The name of the OSS object that is used as the output file.', example='example-output.flv'),
          roleArn?: string(name='RoleArn', description='The ARN of the RAM role used for delegated authorization.', example='acs:ram::<your uid>:role/<your role name>'),
        }(name='OutputFile', description='The details of the output file.'),
        priority?: string(name='Priority', description='The priority of the job in the ApsaraVideo Media Processing (MPS) queue to which the job is added.

*   A value of 10 indicates the highest priority.
*   Default value: **6**.', example='6'),
        properties?: {
          bitrate?: string(name='Bitrate', description='The video bitrate.', example='490'),
          duration?: string(name='Duration', description='The video duration.', example='17'),
          fileFormat?: string(name='FileFormat', description='The video format.', example='mp4'),
          fileSize?: string(name='FileSize', description='The size of the media file.', example='1057273'),
          format?: {
            bitrate?: string(name='Bitrate', description='The total bitrate.', example='490.784'),
            duration?: string(name='Duration', description='The total duration.', example='17.234000'),
            formatLongName?: string(name='FormatLongName', description='The full name of the container format.', example='QuickTime / MOV'),
            formatName?: string(name='FormatName', description='The short name of the container format. Valid values: mov, mp4, m4a, 3gp, 3g2, and mj2.', example='mov'),
            numPrograms?: string(name='NumPrograms', description='The total number of program streams.', example='0'),
            numStreams?: string(name='NumStreams', description='The total number of media streams.', example='2'),
            size?: string(name='Size', description='The size of the media file.', example='1057273'),
            startTime?: string(name='StartTime', description='The start time.', example='-0.064000'),
          }(name='Format', description='The format information.'),
          fps?: string(name='Fps', description='The frame rate of the video.', example='30'),
          height?: string(name='Height', description='The video height.', example='1280'),
          sourceLogos?: {
            sourceLogo?: [ 
            {
              source?: string(name='Source', description='The keyword.', example='example'),
            }
          ](name='SourceLogo')
          }(name='SourceLogos', description='The non-engine layer keywords.'),
          streams?: {
            audioStreamList?: {
              audioStream?: [ 
              {
                bitrate?: string(name='Bitrate', description='The bitrate of the audio stream.', example='64.136'),
                channelLayout?: string(name='ChannelLayout', description='The output layout of the sound channels.', example='mono'),
                channels?: string(name='Channels', description='The number of sound channels.', example='1'),
                codecLongName?: string(name='CodecLongName', description='The full name of the codec.', example='AAC (Advanced Audio Coding)'),
                codecName?: string(name='CodecName', description='The short name of the codec.', example='aac'),
                codecTag?: string(name='CodecTag', description='The tag of the codec.', example='0x6134706d'),
                codecTagString?: string(name='CodecTagString', description='The tag string of the codec.', example='mp4'),
                codecTimeBase?: string(name='CodecTimeBase', description='The codec time base.', example='1/32000'),
                duration?: string(name='Duration', description='The duration of the audio stream.', example='17.223562'),
                index?: string(name='Index', description='The sequence number of the audio stream. The value indicates the position of the audio stream in all audio streams.', example='1'),
                lang?: string(name='Lang', description='The language of the audio stream. For more information, see [FFmpeg documentation](https://www.ffmpeg.org/ffmpeg-all.html#Metadata) and [ISO 639](https://en.wikipedia.org/wiki/List_of_ISO\\_639-1\\_codes).', example='und'),
                numFrames?: string(name='NumFrames', description='The total number of frames.', example='50'),
                sampleFmt?: string(name='SampleFmt', description='The sampling format.', example='fltp'),
                samplerate?: string(name='Samplerate', description='The sampling rate.', example='32000'),
                startTime?: string(name='StartTime', description='The start time.', example='0.064000'),
                timebase?: string(name='Timebase', description='The time base of the audio stream.', example='1/32000'),
              }
            ](name='AudioStream')
            }(name='AudioStreamList', description='The audio streams.'),
            subtitleStreamList?: {
              subtitleStream?: [ 
              {
                index?: string(name='Index', description='The sequence number of the caption stream. The value indicates the position of the caption stream in all caption streams.', example='1'),
                lang?: string(name='Lang', description='The language of the caption stream. For more information, see [FFmpeg documentation](https://www.ffmpeg.org/ffmpeg-all.html#Metadata) and [ISO 639](https://en.wikipedia.org/wiki/List_of_ISO\\_639-1\\_codes).', example='eng'),
              }
            ](name='SubtitleStream')
            }(name='SubtitleStreamList', description='The caption streams.'),
            videoStreamList?: {
              videoStream?: [ 
              {
                avgFPS?: string(name='AvgFPS', description='The average frame rate of the video stream.', example='30.0'),
                bitrate?: string(name='Bitrate', description='The video bitrate.', example='421.117'),
                codecLongName?: string(name='CodecLongName', description='The full name of the codec.', example='H.264 / AVC / MPEG-4 AVC / MPEG-4 part 10'),
                codecName?: string(name='CodecName', description='The short name of the codec.', example='h264'),
                codecTag?: string(name='CodecTag', description='The tag of the codec.', example='0x31637661'),
                codecTagString?: string(name='CodecTagString', description='The tag string of the codec.', example='avc1'),
                codecTimeBase?: string(name='CodecTimeBase', description='The codec time base.', example='1/60'),
                dar?: string(name='Dar', description='The display aspect ratio (DAR) of the video stream.', example='9:16'),
                duration?: string(name='Duration', description='The duration of the video stream.', example='17.233333'),
                fps?: string(name='Fps', description='The frame rate of the video stream.', example='30.0'),
                hasBFrames?: string(name='HasBFrames', description='Indicates whether the video stream contains bidirectional frames (B-frames).', example='2'),
                height?: string(name='Height', description='The height of the video stream in pixels.', example='1280'),
                index?: string(name='Index', description='The sequence number of the video stream. The value indicates the position of the video stream in all video streams.', example='0'),
                lang?: string(name='Lang', description='The language of the video stream. For more information, see [FFmpeg documentation](https://www.ffmpeg.org/ffmpeg-all.html#Metadata) and [ISO 639](https://en.wikipedia.org/wiki/List_of_ISO\\_639-1\\_codes).', example='eng'),
                level?: string(name='Level', description='The codec level.', example='31'),
                networkCost?: {
                  avgBitrate?: string(name='AvgBitrate', description='The average bitrate.', example='300'),
                  costBandwidth?: string(name='CostBandwidth', description='The maximum bandwidth that was consumed.', example='10'),
                  preloadTime?: string(name='PreloadTime', description='The amount of time consumed to preload the video stream.', example='8'),
                }(name='NetworkCost', description='The network bandwidth that was consumed.'),
                numFrames?: string(name='NumFrames', description='The total number of frames.', example='30'),
                pixFmt?: string(name='PixFmt', description='The pixel format of the video stream.', example='yuv420p'),
                profile?: string(name='Profile', description='The codec profile.', example='high'),
                sar?: string(name='Sar', description='The sample aspect ratio (SAR) of the video stream.', example='1:1'),
                startTime?: string(name='StartTime', description='The start time.', example='0.000000'),
                timebase?: string(name='Timebase', description='The time base of the video stream.', example='1/15360'),
                width?: string(name='Width', description='The width of the video stream in pixels.', example='720'),
                bitsPerRawSample?: string(name='bitsPerRawSample', description='The number of binary bits used by each sample or pixel.', example='8'),
                colorPrimaries?: string(name='colorPrimaries', description='The primary colors.', example='bt709'),
                colorTransfer?: string(name='colorTransfer', description='The color transfer configuration.', example='bt709'),
              }
            ](name='VideoStream')
            }(name='VideoStreamList', description='The video streams.'),
          }(name='Streams', description='The stream information.'),
          width?: string(name='Width', description='The video width.', example='720'),
        }(name='Properties', description='The media properties.'),
        rotate?: string(name='Rotate', description='The rotation angle of the video.', example='90'),
        subtitleConfig?: {
          extSubtitleList?: {
            extSubtitle?: [ 
            {
              charEnc?: string(name='CharEnc', description='The character set used by the external caption.

*   Valid values: UTF-8, GBK, BIG5, and auto.
*   Default value: **auto**.

>  If the value of CharEnc is auto, the detected character set may not be the actual character set. We recommend that you set this parameter to another value.', example='auto'),
              fontName?: string(name='FontName', description='The font of the hardcoded captions converted from external captions. Default value: SimSum. For more information, see [Fonts](~~59950~~).', example='"WenQuanYi Zen Hei", "Yuanti SC Regular", "SimSun"'),
              input?: {
                bucket?: string(name='Bucket', description='The name of the OSS bucket in which the input caption file is stored.', example='example-bucket-****'),
                location?: string(name='Location', description='The OSS region in which the input caption file resides.', example='oss-cn-hangzhou'),
                object?: string(name='Object', description='The name of the OSS object that is used as the input caption file.', example='example-output.flv'),
              }(name='Input', description='The input caption file.

*   SRT and ASS files are supported. For more information, see the Input section of the [Parameter details](~~29253~~) topic.
*   Example: `{"Bucket":"example-bucket","Location":"oss-cn-hangzhou","Object":"example.srt"}`.'),
            }
          ](name='ExtSubtitle')
          }(name='ExtSubtitleList', description='The external captions.'),
          subtitleList?: {
            subtitle?: [ 
            {
              map?: string(name='Map', description='The audio track. Format: `0:{Stream}:{Stream sequence number}`, which is `0:a:{audio_index}`. The value of Stream is a, which indicates an audio stream. The sequence number is the index of the audio stream in the list and starts from 0.', example='0:a:0'),
            }
          ](name='Subtitle')
          }(name='SubtitleList', description='The captions.'),
        }(name='SubtitleConfig', description='The caption configurations.'),
        superReso?: {
          isHalfSample?: string(name='IsHalfSample', description='Indicates whether parameters related to the sampling rate are obtained. Valid values:

*   **true**
*   **false**', example='true'),
        }(name='SuperReso', description='The configurations for using the resolution of the source video.'),
        tailSlateList?: {
          tailSlate?: [ 
          {
            bgColor?: string(name='BgColor', description='The color of the bars that are added to the ending part if the size of the ending part is smaller than that of the main part. Default value: White. For more information, see [Parameter details](~~29253~~).', example='White'),
            blendDuration?: string(name='BlendDuration', description='The duration of the transition between the main part and the ending part. A fade transition is used: The last frame of the main part fades out, and the first frame of the ending part fades in. Unit: seconds. Default value: 0.', example='0'),
            height?: string(name='Height', description='The height of the ending part.

*   Valid values: values in the range of (0,4096), -1, and full.
*   A value of -1 indicates that the original height of the ending part is retained.
*   A value of full indicates that the height of the ending part equals the height of the main part.
*   Default value: -1.', example='-1'),
            isMergeAudio?: boolean(name='IsMergeAudio', description='Indicates whether the audio content of the ending part is merged. Valid values:

*   **true**
*   **false**', example='true'),
            start?: string(name='Start', description='The time when the ending part is played.', example='00000.00'),
            tailUrl?: string(name='TailUrl', description='The OSS URL of the ending part.', example='http://example-bucket-****.oss-cn-hangzhou.aliyuncs.com/opening_01.flv'),
            width?: string(name='Width', description='The width of the ending part. Valid values: values in the range of (0,4096), -1, and full.

*   A value of -1 indicates that the original width of the ending part is retained.
*   A value of full indicates that the width of the ending part equals the width of the main part.
*   Default value: -1.', example='-1'),
          }
        ](name='TailSlate')
        }(name='TailSlateList', description='The ending parts.'),
        templateId?: string(name='TemplateId', description='The template ID.', example='S00000001-200010'),
        transConfig?: {
          adjDarMethod?: string(name='AdjDarMethod', description='The method of resolution adjustment. Default value: **none**. Valid values: rescale, crop, pad, and none.', example='none'),
          isCheckAudioBitrate?: string(name='IsCheckAudioBitrate', description='Indicates whether the audio bitrate is checked. If the bitrate of the output audio is higher than that of the input audio, the input bitrate is retained and the specified audio bitrate does not take effect. This parameter has a lower priority than IsCheckAudioBitrateFail. Valid values:

*   **true**

*   **false**

*   Default value:

    *   If this parameter is empty and the codec of the output audio is different from that of the input audio, the default value is false.
    *   If this parameter is empty and the codec of the output audio is the same as that of the input audio, the default value is true.', example='false'),
          isCheckAudioBitrateFail?: string(name='IsCheckAudioBitrateFail', description='Indicates whether the audio bitrate is checked. If the bitrate of the output audio is higher than that of the input audio, the input audio is not transcoded and a transcoding failure is returned. This parameter has a higher priority than IsCheckAudioBitrate. Valid values:

*   **true**
*   **false**
*   Default value: **false**.', example='false'),
          isCheckReso?: string(name='IsCheckReso', description='Indicates whether the resolution is checked. If the output resolution is higher than the input resolution based on the width or height, the input resolution is retained. Valid values:

*   **true**:
*   **false**
*   Default value: **false**.', example='false'),
          isCheckResoFail?: string(name='IsCheckResoFail', description='Indicates whether the resolution is checked. If the output resolution is higher than the input resolution based on the width or height, a transcoding failure is returned. Valid values:

*   **true**
*   **false**
*   Default value: **false**.', example='false'),
          isCheckVideoBitrate?: string(name='IsCheckVideoBitrate', description='Indicates whether the video bitrate is checked. If the bitrate of the output video is higher than that of the input video, the input bitrate is retained. Valid values:

*   **true**
*   **false**
*   Default value: **false**.', example='false'),
          isCheckVideoBitrateFail?: string(name='IsCheckVideoBitrateFail', description='Indicates whether the video bitrate is checked. If the bitrate of the output video is higher than that of the input video, the input video is not transcoded and a transcoding failure is returned. This parameter has a higher priority than IsCheckVideoBitrate. Valid values:

*   **true**
*   **false**
*   Default value: **false**.', example='false'),
          transMode?: string(name='TransMode', description='The transcoding mode.

*   Valid values: onepass, twopass, and CBR.
*   Default value: **onepass**.', example='onepass'),
        }(name='TransConfig', description='The general transcoding configurations.

>  If this parameter is specified in the request, the corresponding parameters in the specified transcoding template are overwritten.'),
        userData?: string(name='UserData', description='The custom data.', example='testid-001'),
        video?: {
          bitrate?: string(name='Bitrate', description='The average bitrate of the video. Unit: Kbit/s.', example='500'),
          bitrateBnd?: {
            max?: string(name='Max', description='The maximum bitrate.', example='1000'),
            min?: string(name='Min', description='The minimum bitrate.', example='300'),
          }(name='BitrateBnd', description='The average bitrate range of the video.'),
          bufsize?: string(name='Bufsize', description='The buffer size.

*   Unit: KB.
*   Default value: **6000**.', example='6000'),
          codec?: string(name='Codec', description='The video codec.

*   Valid values: H.264 and H.265.
*   Default value: H.264.', example='H.264'),
          crf?: string(name='Crf', description='The constant rate factor.

*   Default value when the value of Codec is H.264: **23**, default value when the value of Codec is H.265: **26**.
*   If the value of this parameter is returned, the value of Bitrate becomes invalid.', example='26'),
          crop?: string(name='Crop', description='The method of video cropping. Valid values:

*   **border**: automatically detects and removes borders.
*   A value in the width:height:left:top format: The video image is cropped based on custom settings.', example='1280:800:0:140'),
          degrain?: string(name='Degrain', description='The strength of the independent noise reduction algorithm.', example='5'),
          fps?: string(name='Fps', description='The frame rate of the video.

*   Unit: frames per second.
*   The value is 60 if the frame rate of the input file exceeds 60.
*   Default value: the frame rate of the input video.', example='25'),
          gop?: string(name='Gop', description='The maximum interval between keyframes or the maximum number of frames in a frame group. Unit: seconds.

*   Default value: **250**.
*   If the maximum number of frames is returned, the value does not contain a unit.', example='250'),
          height?: string(name='Height', description='The height of the video.

*   Unit: pixel.
*   Default value: the height of the input video.', example='720'),
          maxFps?: string(name='MaxFps', description='The maximum frame rate.', example='60'),
          maxrate?: string(name='Maxrate', description='The maximum bitrate of the video. Unit: Kbit/s.', example='3000'),
          pad?: string(name='Pad', description='The black bars that are added to the video.

*   Unit: pixel.
*   Format: width:height:left:top.', example='1280:800:0:140'),
          pixFmt?: string(name='PixFmt', description='The pixel format of the video. Valid values: standard pixel formats such as yuv420p and yuvj420p.', example='yuv420p'),
          preset?: string(name='Preset', description='The preset video algorithm. Default value: **medium**. Valid values:

*   **veryfast**
*   **fast**
*   **medium**
*   **slow**
*   **slower**', example='medium'),
          profile?: string(name='Profile', description='The codec profile of the video. Valid values: baseline, main, and high.

>  If multiple definitions are involved, we recommend that you use baseline for the lowest definition to ensure normal playback on low-end devices, and use main or high for other definitions.

*   **baseline**: applicable to mobile devices.
*   **main**: applicable to standard-definition devices.
*   **high**: applicable to high-definition devices.
*   Default value: **high**.', example='high'),
          qscale?: string(name='Qscale', description='The level of quality control on the video.', example='15'),
          resoPriority?: string(name='ResoPriority', description='The resource priority.', example='1'),
          scanMode?: string(name='ScanMode', description='The scan mode. Valid values:

*   If this parameter is left **empty**, the scan mode of the input video is used.
*   **auto**: automatic deinterlacing.
*   **progressive**: progressive scan.
*   **interlaced**: interlaced scan.
*   **By default**, this parameter is left empty.

**Best practice**: The interlaced scan mode saves data traffic than the progressive scan mode but provides poor image quality. Therefore, the progressive scan mode is commonly used in mainstream video production.

*   If **progressive** or **interlaced** is used when the scan mode of the input video is neither of them, the transcoding job fails.
*   We recommend that you use **the scan mode of the input video** or **automatic deinterlacing** for higher compatibility.', example='interlaced'),
          width?: string(name='Width', description='The width of the video.

*   Unit: pixel.
*   Default value: the width of the input video.', example='1280'),
        }(name='Video', description='The video configurations.'),
        videoStreamMap?: string(name='VideoStreamMap', description='The sequence number of the video stream. The sequence number is the index of the video stream in the list and starts from 0. If no sequence number is specified, the default video stream is used.', example='0'),
        waterMarkConfigUrl?: string(name='WaterMarkConfigUrl', description='The URL of the watermark configuration file.', example='http://example.com/configure'),
        waterMarkList?: {
          waterMark?: [ 
          {
            dx?: string(name='Dx', description='The horizontal offset of the watermark image relative to the output video. If this parameter is specified in the request, the corresponding parameter in the watermark template is overwritten. Default value: 0. The value can be an integer or a decimal number.

*   An integer indicates the pixel value of the horizontal offset.

    *   Valid values: **\\[8,4096]**.
    *   Unit: pixel.

*   A decimal number indicates the ratio of the horizontal offset to the width in the output video resolution.

    *   Valid values: (0,1).
    *   The decimal number can be accurate to four decimal places, such as 0.9999. Excessive digits are automatically discarded.', example='100'),
            dy?: string(name='Dy', description='The vertical offset of the watermark image relative to the output video. If this parameter is specified in the request, the corresponding parameter in the watermark template is overwritten. The value can be an integer or a decimal number.

*   An integer indicates the pixel value of the vertical offset.

    *   Valid values: **\\[8,4096]**.
    *   Unit: pixel.

*   A decimal number indicates the ratio of the vertical offset to the height in the output video resolution.

    *   Valid values: **(0,1)**.
    *   The decimal number can be accurate to four decimal places, such as 0.9999. Excessive digits are automatically discarded.', example='100'),
            height?: string(name='Height', description='The height of the watermark image. If this parameter is specified in the request, the corresponding parameter in the watermark template is overwritten. The value can be an integer or a decimal number.

*   An integer indicates the pixel value of the watermark height.

    *   Valid values: **\\[8,4096]**.
    *   Unit: pixel.

*   A decimal number indicates the ratio of the watermark height to the height in the output video resolution.

    *   Valid values: **(0,1)**.
    *   The decimal number can be accurate to four decimal places, such as 0.9999. Excessive digits are automatically discarded.', example='50'),
            inputFile?: {
              bucket?: string(name='Bucket', description='The name of the OSS bucket in which the input file is stored.', example='example-bucket'),
              location?: string(name='Location', description='The OSS region in which the input file resides.', example='oss-cn-hangzhou'),
              object?: string(name='Object', description='The name of the Object Storage Service (OSS) object that is used as the input file.', example='example-logo-****.png'),
            }(name='InputFile', description='The watermark input file. PNG images and MOV files are supported.'),
            referPos?: string(name='ReferPos', description='The position of the watermark. If this parameter is specified in the request, the corresponding parameter in the watermark template is overwritten. Valid values:

*   TopRight
*   TopLeft
*   BottomRight
*   BottomLeft', example='TopRight'),
            type?: string(name='Type', description='The type of the watermark. If this parameter is specified in the request, the corresponding parameter in the watermark template is overwritten. For more information, see [Parameter details](~~29253~~). Valid values:

*   Image
*   Text', example='Image'),
            waterMarkTemplateId?: string(name='WaterMarkTemplateId', description='The ID of the watermark template.', example='88c6ca184c0e47098a5b665e2a12****'),
            width?: string(name='Width', description='The width of the watermark image. If this parameter is specified in the request, the corresponding parameter in the watermark template is overwritten. The value can be an integer or a decimal number.

*   An integer indicates the pixel value of the watermark width.

    *   Valid values: **\\[8,4096]**.
    *   Unit: pixel.

*   A decimal number indicates the ratio of the watermark width to the width in the output video resolution.

    *   Valid values: **(0,1)**.
    *   The decimal number can be accurate to four decimal places, such as 0.9999. Excessive digits are automatically discarded.', example='50'),
          }
        ](name='WaterMark')
        }(name='WaterMarkList', description='The watermarks.'),
      }(name='Output', description='The job output.'),
      percent?: long(name='Percent', description='The transcoding progress.', example='100'),
      pipelineId?: string(name='PipelineId', description='The ID of the MPS queue that is used to run the job.', example='88c6ca184c0e47b665e2a1267971****'),
      state?: string(name='State', description='The job state. Valid values:

*   **Submitted**: The job was submitted.
*   **Transcoding**: Transcoding is in process.
*   **TranscodeSuccess**: The job was successful.
*   **TranscodeFail**: The job failed.
*   **TranscodeCancelled**: The job was canceled.', example='TranscodeSuccess'),
      submitTime?: string(name='SubmitTime', description='The time when the job was submitted.', example='2021-03-04T06:44:43Z'),
    }
  ](name='Job')
  }(name='JobList', description='The transcoding jobs.'),
  nonExistJobIds?: {
    string?: [ string ](name='String')
  }(name='NonExistJobIds', description='The list of nonexistent job IDs. If all queried job IDs exist, the response does not contain this parameter.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='197ADF44-104C-514C-9F92-D8924CB34E2A'),
}

model QueryJobListResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: QueryJobListResponseBody(name='body'),
}

/**
  * *   By default, returned jobs are sorted in descending order by CreationTime.
  * *   You can call this operation to query up to 10 transcoding jobs at a time.
  * *   If you do not set the JobIds parameter, the `InvalidParameter` error code is returned.
  * ## QPS limit
  * You can call this API operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
 */
async function queryJobList(request: QueryJobListRequest): QueryJobListResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'QueryJobList', 'POST', '/', 'json', false, 'json', request);
}

model QueryMediaCensorJobDetailRequest {
  jobId: string(name='JobId', description='The ID of the content moderation job. You can obtain the job ID from the response parameters of the [SubmitMediaCensorJob](~~91774~~) operation.', example='2288c6ca184c0e47098a5b665e2a12****', position='Query'),
  maximumPageSize?: long(name='MaximumPageSize', description='The maximum number of entries to return on each page.

*   Default value: **30**.
*   Valid values: **1 to 300**.', example='30', position='Query'),
  nextPageToken?: string(name='NextPageToken', description='The pagination token that is used in the next request to retrieve a new page of results. You do not need to specify this parameter for the first request. The token of the next page is returned after you call this operation to query the results of a content moderation job for the first time.', example='ae0fd49c0840e14daf0d66a75b83****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model QueryMediaCensorJobDetailResponseBody = {
  mediaCensorJobDetail?: {
    audioCensorResult?: {
      audioDetailResultList?: {
        audioDetailResult?: [ 
        {
          endTime?: int32(name='EndTime'),
          label?: string(name='Label'),
          startTime?: int32(name='StartTime'),
          text?: string(name='Text'),
        }
      ](name='AudioDetailResult')
      }(name='AudioDetailResultList'),
      label?: string(name='Label'),
      suggestion?: string(name='Suggestion'),
    }(name='AudioCensorResult'),
    barrageCensorResult?: {
      label?: string(name='Label', description='The labels of the moderation result. Multiple labels are separated with commas (,). Valid values:

*   **spam**: spam
*   **ad**: ads
*   **abuse**: abuse
*   **flood**: excessive junk content
*   **contraband**: prohibited content
*   **meaningless**: meaningless content
*   **normal**: normal content', example='normal'),
      rate?: string(name='Rate', description='The score.', example='99.91'),
      scene?: string(name='Scene', description='The moderation scenario. Valid values: The value is **antispam**.', example='antispam'),
      suggestion?: string(name='Suggestion', description='The recommended subsequent operation. Valid values:

*   **pass**: The content passes the moderation.
*   **review**: The content needs to be manually reviewed again.
*   **block**: The content needs to be blocked.', example='pass'),
    }(name='BarrageCensorResult', description='The moderation result of live comments.'),
    code?: string(name='Code', description='The error code returned if the job fails. This parameter is not returned if the job is successful.', example='InvalidParameter.ResourceNotFound'),
    coverImageCensorResults?: {
      coverImageCensorResult?: [ 
      {
        bucket?: string(name='Bucket', description='The OSS bucket that stores the video thumbnail.', example='bucket-out-test-****'),
        location?: string(name='Location', description='The OSS region in which the video thumbnail resides.', example='oss-cn-shanghai'),
        object?: string(name='Object', description='The Object Storage Service (OSS) object that is used as the video thumbnail.', example='test/ai/censor/v2/vme-****.jpg'),
        results?: {
          result?: [ 
          {
            label?: string(name='Label', description='The labels of the moderation result. Multiple labels are separated with commas (,).

*   Valid values in the pornographic content moderation scenario:

    *   **normal**: normal content
    *   **sexy**: sexy content
    *   **porn**: pornographic content

*   Valid values in the terrorist content moderation scenario:

    *   **narmal**: normal content
    *   **bloody**: bloody content
    *   **explosion**: explosion and smoke
    *   **outfit**: special costume
    *   **logo**: special logo
    *   **weapon**: weapon
    *   **politics**: political content
    *   **violence**: violence
    *   **crowd**: crowd
    *   **parade**: parade
    *   **carcrash**: car accident
    *   **flag**: flag
    *   **location**: landmark
    *   **others**: other content

*   Valid values in the ad moderation scenario:

    *   **normal**: normal content
    *   **ad**: other ads
    *   **politics**: political content in text
    *   **porn**: pornographic content in text
    *   **abuse**: abuse in text
    *   **terrorism**: terrorist content in text
    *   **contraband**: prohibited content in text
    *   **spam**: spam in text
    *   **npx**: illegal ads
    *   **qrcode**: QR code
    *   **programCode**: mini program code

*   Valid values in the live moderation scenario:

    *   **normal**: normal content
    *   **meaningless**: meaningless content, such as a black or white screen
    *   **PIP**: picture-in-picture
    *   **smoking**: smoking
    *   **drivelive**: live broadcasting in a running vehicle

*   Valid values in the logo moderation scenario:

    *   **normal**: normal content
    *   **TV**: controlled TV station logo
    *   **trademark**: trademark', example='normal'),
            rate?: string(name='Rate', description='The score. Valid values: **0 to 100**.', example='100'),
            scene?: string(name='Scene', description='The moderation scenario. Valid values:

*   **porn**: pornographic content moderation
*   **terrorism**: terrorist content moderation
*   **ad**: ad violation moderation
*   **live**: undesirable scene moderation
*   **logo**: logo moderation', example='porn'),
            suggestion?: string(name='Suggestion', description='The recommended subsequent operation. Valid values:

*   **pass**: The content passes the moderation.
*   **review**: The content needs to be manually reviewed again.
*   **block**: The content needs to be blocked.', example='pass'),
          }
        ](name='Result')
        }(name='Results', description='The moderation results.'),
      }
    ](name='CoverImageCensorResult')
    }(name='CoverImageCensorResults', description='The moderation results of thumbnails.'),
    creationTime?: string(name='CreationTime', description='The time when the job was created.', example='2018-09-13T16:32:24Z'),
    descCensorResult?: {
      label?: string(name='Label', description='The labels of the moderation result. Valid values:

*   **spam**: spam
*   **ad**: ads
*   **abuse**: abuse
*   **flood**: excessive junk content
*   **contraband**: prohibited content
*   **meaningless**: meaningless content
*   **normal**: normal content', example='normal'),
      rate?: string(name='Rate', description='The score.', example='100'),
      scene?: string(name='Scene', description='The moderation scenario. Valid values: The value is **antispam**.', example='antispam'),
      suggestion?: string(name='Suggestion', description='The recommended subsequent operation. Valid values:

*   **pass**: The content passes the moderation.
*   **review**: The content needs to be manually reviewed again.
*   **block**: The content needs to be blocked.', example='review'),
    }(name='DescCensorResult', description='The moderation result of the description.'),
    finishTime?: string(name='FinishTime', description='The time when the job was complete.', example='2018-09-21'),
    input?: {
      bucket?: string(name='Bucket', description='The name of the OSS bucket in which the input media file is stored.', example='bucket-test-in-****'),
      location?: string(name='Location', description='The OSS region in which the input media file resides.', example='oss-cn-shanghai'),
      object?: string(name='Object', description='The name of the OSS object that is used as the input media file.', example='test/ai/censor/test-****.mp4'),
    }(name='Input', description='The information about the job input.'),
    jobId?: string(name='JobId', description='The ID of the content moderation job.', example='f8f166eea7a44e9bb0a4aecf9543****'),
    message?: string(name='Message', description='The error message returned if the job fails. This parameter is not returned if the job is successful.', example='The resource operated cannot be found'),
    pipelineId?: string(name='PipelineId', description='The ID of the ApsaraVideo Media Processing (MPS) queue to which the content moderation job is submitted.', example='c5b30b7c0d0e4a0abde1d5f9e751****'),
    state?: string(name='State', description='The status of the job. Valid values:', example='Success'),
    suggestion?: string(name='Suggestion', description='The overall result of the job. Valid values:

*   **pass**: The content passes the moderation.
*   **review**: The content needs to be manually reviewed again.
*   **block**: The content needs to be blocked.

If the moderation result of one type of the moderated content is review, the overall result is review. If the moderation result of one type of the moderated content is block, the overall result is block.', example='block'),
    titleCensorResult?: {
      label?: string(name='Label', description='The labels of the moderation result. Valid values:

*   **normal**: normal content
*   **spam**: spam
*   **ad**: ads
*   **abuse**: abuse
*   **flood**: excessive junk content
*   **contraband**: prohibited content
*   **meaningless**: meaningless content', example='meaningless'),
      rate?: string(name='Rate', description='The score.', example='99.91'),
      scene?: string(name='Scene', description='The moderation scenario. Valid values: The value is **antispam**.', example='antispam'),
      suggestion?: string(name='Suggestion', description='The recommended subsequent operation. Valid values:

*   **pass**: The content passes the moderation.
*   **review**: The content needs to be manually reviewed again.
*   **block**: The content needs to be blocked.', example='block'),
    }(name='TitleCensorResult', description='The moderation result of the title.'),
    userData?: string(name='UserData', description='The custom data.', example='example userdata ****'),
    vensorCensorResult?: {
      censorResults?: {
        censorResult?: [ 
        {
          label?: string(name='Label', description='The labels of the moderation result. Multiple labels are separated with commas (,). Valid values:

*   Valid values in the pornographic content moderation scenario:

    *   **porn**: pornographic content
    *   **sexy**: sexy content
    *   **normal**: normal content

*   Valid values in the terrorist content moderation scenario:

    *   **normal**: normal content
    *   **bloody**: bloody content
    *   **explosion**: explosion and smoke
    *   **outfit**: special costume
    *   **logo**: special logo
    *   **weapon**: weapon
    *   **politics**: political content
    *   **violence**: violence
    *   **crowd**: crowd
    *   **parade**: parade
    *   **carcrash**: car accident
    *   **flag**: flag
    *   **location**: landmark
    *   **others**: other content

*   Valid values in the ad moderation scenario:

    *   **normal**: normal content
    *   **ad**: other ads
    *   **politics**: political content in text
    *   **porn**: pornographic content in text
    *   **abuse**: abuse in text
    *   **terrorism**: terrorist content in text
    *   **contraband**: prohibited content in text
    *   **spam**: spam in text
    *   **npx**: illegal ads
    *   **qrcode**: QR code
    *   **programCode**: mini program code

*   Valid values in the live moderation scenario:

    *   **normal**: normal content
    *   **meaningless**: meaningless content, such as a black or white screen
    *   **PIP**: picture-in-picture
    *   **smoking**: smoking
    *   **drivelive**: live broadcasting in a running vehicle

*   Valid values in the logo moderation scenario:

    *   **normal**: normal content
    *   **TV**: controlled TV station logo
    *   **trademark**: trademark', example='meaningless'),
          rate?: string(name='Rate', description='The score.', example='100'),
          scene?: string(name='Scene', description='The moderation scenario. Valid values:

*   **porn**: pornographic content moderation
*   **terrorism**: terrorist content moderation
*   **ad**: ad violation moderation
*   **live**: undesirable scene moderation
*   **logo**: logo moderation', example='terrorism'),
          suggestion?: string(name='Suggestion', description='The recommended subsequent operation. Valid values:

*   **pass**: The content passes the moderation.
*   **review**: The content needs to be manually reviewed again.
*   **block**: The content needs to be blocked.', example='review'),
        }
      ](name='CensorResult')
      }(name='CensorResults', description='A collection of the moderation results. The information includes the summary of various scenarios such as pornographic content and terrorist content.'),
      nextPageToken?: string(name='NextPageToken', description='The returned value of NextToken is a pagination token, which can be used in the next request to retrieve a new page of results.', example='ea04afcca7cd4e80b9ece8fbb251****'),
      videoTimelines?: {
        videoTimeline?: [ 
        {
          censorResults?: {
            censorResult?: [ 
            {
              label?: string(name='Label', description='The labels of the moderation result. Multiple labels are separated with commas (,). Valid values:

*   Valid values in the pornographic content moderation scenario:

    *   **porn**: pornographic content
    *   **sexy**: sexy content
    *   **normal**: normal content

*   Valid values in the terrorist content moderation scenario:

    *   **normal**: normal content
    *   **bloody**: bloody content
    *   **explosion**: explosion and smoke
    *   **outfit**: special costume
    *   **logo**: special logo
    *   **weapon**: weapon
    *   **politics**: political content
    *   **violence**: violence
    *   **crowd**: crowd
    *   **parade**: parade
    *   **carcrash**: car accident
    *   **flag**: flag
    *   **location**: landmark
    *   **others**: other content

*   Valid values in the ad moderation scenario:

    *   **normal**: normal content
    *   **ad**: other ads
    *   **politics**: political content in text
    *   **porn**: pornographic content in text
    *   **abuse**: abuse in text
    *   **terrorism**: terrorist content in text
    *   **contraband**: prohibited content in text
    *   **spam**: spam in text
    *   **npx**: illegal ads
    *   **qrcode**: QR code
    *   **programCode**: mini program code

*   Valid values in the live moderation scenario:

    *   **normal**: normal content
    *   **meaningless**: meaningless content, such as a black or white screen
    *   **PIP**: picture-in-picture
    *   **smoking**: smoking
    *   **drivelive**: live broadcasting in a running vehicle

*   Valid values in the logo moderation scenario:

    *   **normal**: normal content
    *   **TV**: controlled TV station logo
    *   **trademark**: trademark', example='porn,ad'),
              rate?: string(name='Rate', description='The score.', example='99.99'),
              scene?: string(name='Scene', description='The moderation scenario. Valid values:

*   **porn**: pornographic content moderation
*   **terrorism**: terrorist content moderation
*   **ad**: ad violation moderation
*   **live**: undesirable scene moderation
*   **logo**: logo moderation', example='porn'),
              suggestion?: string(name='Suggestion', description='The recommended subsequent operation. Valid values:

*   **pass**: The content passes the moderation.
*   **review**: The content needs to be manually reviewed again.
*   **block**: The content needs to be blocked.', example='block'),
            }
          ](name='CensorResult')
          }(name='CensorResults', description='The moderation results that include information such as labels and scores.'),
          object?: string(name='Object', description='The one or more OSS objects that are generated as the output snapshots.

> In the example, {Count} is a placeholder. The OSS objects that are generated as output snapshots are named `output00001-****.jpg`, `output00002-****.jpg`, and so on.', example='output{Count}.jpg'),
          timestamp?: string(name='Timestamp', description='The position in the video. Format: `hh:mm:ss[.SSS]`.', example='00:02:59.999'),
        }
      ](name='VideoTimeline')
      }(name='VideoTimelines', description='The moderation results that are sorted in ascending order by time.'),
    }(name='VensorCensorResult', description='The moderation results of the video.'),
    videoCensorConfig?: {
      bizType?: string(name='BizType', description='The custom business type. Default value: common.', example='common'),
      outputFile?: {
        bucket?: string(name='Bucket', description='The OSS bucket that stores the output snapshots.', example='test-bucket-****'),
        location?: string(name='Location', description='The region in which the OSS bucket that stores the output snapshot resides.', example='oss-cn-shanghai'),
        object?: string(name='Object', description='The one or more OSS objects that are generated as the output snapshots.

> In the example, {Count} is a placeholder. The OSS objects that are generated as output snapshots are named `output00001-****.jpg`, `output00002-****.jpg`, and so on.', example='output{Count}.jpg'),
      }(name='OutputFile', description='The information about output snapshots.'),
      videoCensor?: string(name='VideoCensor', description='Indicates whether the video content needs to be moderated. Default value: **true** Valid values:

*   **true**: The video content needs to be moderated.
*   **false**: The video content does not need to be moderated.', example='true'),
    }(name='VideoCensorConfig', description='The video moderation configurations.'),
  }(name='MediaCensorJobDetail', description='The results of the content moderation job.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='B42299E6-F71F-465F-8FE9-4FC2E3D3C2CA'),
}

model QueryMediaCensorJobDetailResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: QueryMediaCensorJobDetailResponseBody(name='body'),
}

/**
  * In the content moderation results, the moderation results of the video are sorted in ascending order by time into a timeline. If the video is long, the content moderation results are paginated, and the first page is returned. You can call this operation again to query the remaining moderation results of the video.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function queryMediaCensorJobDetail(request: QueryMediaCensorJobDetailRequest): QueryMediaCensorJobDetailResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'QueryMediaCensorJobDetail', 'POST', '/', 'json', false, 'json', request);
}

model QueryMediaCensorJobListRequest {
  endOfJobCreatedTimeRange?: string(name='EndOfJobCreatedTimeRange', description='The end of the time range within which the jobs to be queried were created.

*   Specify the time in the ISO 8601 standard in the YYYY-MM-DDThh:mm:ssZ format.
*   The time must be in UTC.', example='2022-02-14T02:16:07Z', position='Query'),
  jobIds?: string(name='JobIds', description='The ID of the content moderation job. You can call the [SubmitMediaCensorJob](~~91779~~) operation to query the ID of the content moderation job. Separate multiple IDs with commas (,).', example='fa9c34be3bcf42919ac4d1775239****,78dc866518b843259669df58ed30****', position='Query'),
  maximumPageSize?: long(name='MaximumPageSize', description='The maximum number of entries to return on each page.

*   Default value: **30**.
*   Valid values: **1 to 300**.', example='20', minimum=1, maximum=100, position='Query'),
  nextPageToken?: string(name='NextPageToken', description='The pagination token that is used in the next request to retrieve a new page of results. You do not need to specify this parameter for the first request. The returned value of NextPageToken is a pagination token, which can be used in the next request to retrieve a new page of results.', example='79aff3eee82242e092899db5f669****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  pipelineId?: string(name='PipelineId', description='The ID of the ApsaraVideo Media Processing (MPS) queue that is used to run the job. To obtain the ID of the MPS queue, perform the following steps: Log on to the [**MPS console**](https://mps.console.aliyun.com/overview). In the left-side navigation pane, choose **Global Settings** > **Pipelines**.', example='c5b30b7c0d0e4a0abde1d5f9e751****', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  startOfJobCreatedTimeRange?: string(name='StartOfJobCreatedTimeRange', description='The beginning of the time range within which the jobs to be queried were created.

*   Specify the time in the ISO 8601 standard in the YYYY-MM-DDThh:mm:ssZ format.
*   The time must be in UTC.', example='2021-12-22T03:48:05Z', position='Query'),
  state?: string(name='State', description='The status of the jobs to be queried. Valid values:

*   **All**: all jobs.
*   **Queuing**: the jobs that are being queued.
*   **Analysing**: the jobs that are in progress.
*   **Fail**: failed jobs.
*   **Success**: successful jobs.', example='All', position='Query'),
}

model QueryMediaCensorJobListResponseBody = {
  mediaCensorJobList?: {
    mediaCensorJob?: [ 
    {
      audioCensorResult?: {
        label?: string(name='Label'),
        suggestion?: string(name='Suggestion'),
      }(name='AudioCensorResult'),
      barrageCensorResult?: {
        label?: string(name='Label', description='The labels of the moderation result. Separate multiple labels with commas (,). Valid values:

*   **spam**: spam
*   **ad**: ads
*   **abuse**: abuse
*   **flood**: excessive junk content
*   **contraband**: prohibited content
*   **meaningless**: meaningless content
*   **normal**: normal content', example='normal'),
        rate?: string(name='Rate', description='The score.', example='99.91'),
        scene?: string(name='Scene', description='The moderation scenario. The value is **antispam**.', example='antispam'),
        suggestion?: string(name='Suggestion', description='The recommended subsequent operation. Valid values:

*   **pass**: The content passes the moderation.
*   **review**: The content needs to be manually reviewed again.
*   **block**: The content needs to be blocked.', example='pass'),
      }(name='BarrageCensorResult', description='The moderation result of live comments.'),
      code?: string(name='Code', description='The error code returned if the job fails. This parameter is not returned if the job is successful.', example='InvalidParameter.ResourceNotFound'),
      coverImageCensorResults?: {
        coverImageCensorResult?: [ 
        {
          bucket?: string(name='Bucket', description='The OSS bucket that stores the video thumbnail.', example='example-Bucket-****'),
          location?: string(name='Location', description='The OSS region in which the video thumbnail resides.', example='oss-cn-shanghai'),
          object?: string(name='Object', description='The Object Storage Service (OSS) object that is used as the video thumbnail.', example='test/ai/censor/v2/vme-****.jpg'),
          results?: {
            result?: [ 
            {
              label?: string(name='Label', description='The labels of the moderation result. Separate multiple labels with commas (,).

*   Valid values in the pornographic content moderation scenario:

    *   **normal**: normal content
    *   **sexy**: sexy content
    *   **porn**: pornographic content

*   Valid values in the terrorist content moderation scenario:

    *   **normal**: normal content
    *   **bloody**: bloody content
    *   **explosion**: explosion and smoke
    *   **outfit**: special costume
    *   **logo**: special logo
    *   **weapon**: weapon
    *   **politics**: political content
    *   **violence**: violence
    *   **crowd**: crowd
    *   **parade**: parade
    *   **carcrash**: car accident
    *   **flag**: flag
    *   **location**: landmark
    *   **others**: other content

*   Valid values in the ad moderation scenario:

    *   **normal**: normal content
    *   **ad**: other ads
    *   **politics**: political content in text
    *   **porn**: pornographic content in text
    *   **abuse**: abuse in text
    *   **terrorism**: terrorist content in text
    *   **contraband**: prohibited content in text
    *   **spam**: spam in text
    *   **npx**: illegal ads
    *   **qrcode**: QR code
    *   **programCode**: mini program code

*   Valid values in the live moderation scenario:

    *   **normal**: normal content
    *   **meaningless**: meaningless content, such as a black or white screen
    *   **PIP**: picture-in-picture
    *   **smoking**: smoking
    *   **drivelive**: live broadcasting in a running vehicle

*   Valid values in the logo moderation scenario:

    *   **normal**: normal content
    *   **TV**: controlled TV station logo
    *   **trademark**: trademark', example='normal'),
              rate?: string(name='Rate', description='The score. Valid values: 0 to 100.', example='100'),
              scene?: string(name='Scene', description='The moderation scenario. Valid values:

*   **porn**: pornographic content moderation
*   **terrorism**: terrorist content moderation
*   **ad**: ad violation moderation
*   **live**: undesirable scene moderation
*   **logo**: special logo moderation', example='live'),
              suggestion?: string(name='Suggestion', description='The overall result of the job. Valid values:

*   **pass**: The content passes the moderation.
*   **review**: The content needs to be manually reviewed again.
*   **block**: The content needs to be blocked.

> If the moderation result of any type of the moderated content is review, the overall result is review. If the moderation result of any type of the moderated content is block, the overall result is block.', example='pass'),
            }
          ](name='Result')
          }(name='Results', description='The moderation results of the content moderation jobs.'),
        }
      ](name='CoverImageCensorResult')
      }(name='CoverImageCensorResults', description='The moderation results of thumbnails.'),
      creationTime?: string(name='CreationTime', description='The time when the job was created.', example='2021-11-04T07:25:48Z'),
      descCensorResult?: {
        label?: string(name='Label', description='The labels of the moderation result. Separate multiple labels with commas (,). Valid values:

*   **spam**: spam
*   **ad**: ads
*   **abuse**: abuse
*   **flood**: excessive junk content
*   **contraband**: prohibited content
*   **meaningless**: meaningless content
*   **normal**: normal content', example='ad'),
        rate?: string(name='Rate', description='The score.', example='100'),
        scene?: string(name='Scene', description='The moderation scenario. The value is **antispam**.', example='antispam'),
        suggestion?: string(name='Suggestion', description='The recommended subsequent operation. Valid values:

*   **pass**: The content passes the moderation.
*   **review**: The content needs to be manually reviewed again.
*   **block**: The content needs to be blocked.', example='pass'),
      }(name='DescCensorResult', description='The moderation result of the description.'),
      finishTime?: string(name='FinishTime', description='The time when the job was completed.', example='2021-11-04T07:25:48Z'),
      input?: {
        bucket?: string(name='Bucket', description='The name of the OSS bucket in which the input file is stored.', example='bucket-test-in-****'),
        location?: string(name='Location', description='The OSS region in which the input file resides.', example='oss-cn-shanghai'),
        object?: string(name='Object', description='The name of the OSS object that is used as the input file.', example='test/ai/censor/test-****.mp4'),
      }(name='Input', description='The information about the job input.'),
      jobId?: string(name='JobId', description='The ID of the content moderation job.', example='f8f166eea7a44e9bb0a4aecf9543****'),
      message?: string(name='Message', description='The error message returned if the job fails. This parameter is not returned if the job is successful.', example='The resource operated cannot be found'),
      pipelineId?: string(name='PipelineId', description='The ID of the MPS queue that is used to run the job.', example='c5b30b7c0d0e4a0abde1d5f9e751****'),
      state?: string(name='State', description='The status of the job.', example='Success'),
      suggestion?: string(name='Suggestion', description='The recommended subsequent operation. Valid values:

*   **pass**: The content passes the moderation.
*   **review**: The content needs to be manually reviewed again.
*   **block**: The content needs to be blocked.', example='pass'),
      titleCensorResult?: {
        label?: string(name='Label', description='The labels of the moderation result. Separate multiple labels with commas (,).

*   **normal**: normal content
*   **spam**: spam
*   **ad**: ads
*   **abuse**: abuse content
*   **flood**: excessive junk content
*   **contraband**: prohibited content
*   **meaningless**: meaningless content', example='meaningless'),
        rate?: string(name='Rate', description='The score.', example='99.91'),
        scene?: string(name='Scene', description='The moderation scenario. The value is **antispam**.', example='antispam'),
        suggestion?: string(name='Suggestion', description='The recommended subsequent operation. Valid values:

*   **pass**: The content passes the moderation.
*   **review**: The content needs to be manually reviewed again.
*   **block**: The content needs to be blocked.', example='block'),
      }(name='TitleCensorResult', description='The moderation result of the title.'),
      userData?: string(name='UserData', description='The custom data.', example='example userdata ****'),
      vensorCensorResult?: {
        censorResults?: {
          censorResult?: [ 
          {
            label?: string(name='Label', description='The labels of the moderation result. Separate multiple labels with commas (,).

*   Valid values in the pornographic content moderation scenario:

    *   **porn**: pornographic content
    *   **sexy**: sexy content
    *   **normal**: normal content

*   Valid values in the terrorist content moderation scenario:

    *   **normal**: normal content
    *   **bloody**: bloody content
    *   **explosion**: explosion and smoke
    *   **outfit**: special costume
    *   **logo**: special logo
    *   **weapon**: weapon
    *   **politics**: political content
    *   **violence**: violence
    *   **crowd**: crowd
    *   **parade**: parade
    *   **carcrash**: car accident
    *   **flag**: flag
    *   **location**: landmark
    *   **others**: other content

*   Valid values in the ad moderation scenario:

    *   **normal**: normal content
    *   **ad**: other ads
    *   **politics**: political content in text
    *   **porn**: pornographic content in text
    *   **abuse**: abuse in text
    *   **terrorism**: terrorist content in text
    *   **contraband**: prohibited content in text
    *   **spam**: spam in text
    *   **npx**: illegal ads
    *   **qrcode**: QR code
    *   **programCode**: mini program code

*   Valid values in the live moderation scenario:

    *   **normal**: normal content
    *   **meaningless**: meaningless content, such as a black or white screen
    *   **PIP**: picture-in-picture
    *   **smoking**: smoking
    *   **drivelive**: live broadcasting in a running vehicle

*   Valid values in the logo moderation scenario:

    *   **normal**: normal content
    *   **TV**: controlled TV station logo
    *   **trademark**: trademark', example='meaningless'),
            rate?: string(name='Rate', description='The score.', example='100'),
            scene?: string(name='Scene', description='The moderation scenario. Valid values:

*   **porn**: pornographic content moderation
*   **terrorism**: terrorist content moderation
*   **ad**: ad violation moderation
*   **live**: undesirable scene moderation
*   **logo**: special logo moderation', example='terrorism'),
            suggestion?: string(name='Suggestion', description='The recommended subsequent operation. Valid values:

*   **pass**: The content passes the moderation.
*   **review**: The content needs to be manually reviewed again.
*   **block**: The content needs to be blocked.', example='review'),
          }
        ](name='CensorResult')
        }(name='CensorResults', description='A collection of the moderation results. The information includes the summary about various scenarios such as pornographic content and terrorist content.'),
        nextPageToken?: string(name='NextPageToken', description='The pagination token that is used in the next request to retrieve a new page of results.', example='ea04afcca7cd4e80b9ece8fbb251****'),
        videoTimelines?: {
          videoTimeline?: [ 
          {
            censorResults?: {
              censorResult?: [ 
              {
                label?: string(name='Label', description='The labels of the moderation result. Separate multiple labels with commas (,). Valid values:

*   Valid values in the pornographic content moderation scenario:

    *   **porn**: pornographic content
    *   **sexy**: sexy content
    *   **normal**: normal content

*   Valid values in the terrorist content moderation scenario:

    *   **normal**: normal content
    *   **bloody**: bloody content
    *   **explosion**: explosion and smoke
    *   **outfit**: special costume
    *   **logo**: special logo
    *   **weapon**: weapon
    *   **politics**: political content
    *   **violence**: violence
    *   **crowd**: crowd
    *   **parade**: parade
    *   **carcrash**: car accident
    *   **flag**: flag
    *   **location**: landmark
    *   **others**: other content

*   Valid values in the ad moderation scenario:

    *   **normal**: normal content
    *   **ad**: other ads
    *   **politics**: political content in text
    *   **porn**: pornographic content in text
    *   **abuse**: abuse in text
    *   **terrorism**: terrorist content in text
    *   **contraband**: prohibited content in text
    *   **spam**: spam in text
    *   **npx**: illegal ads
    *   **qrcode**: QR code
    *   **programCode**: mini program code

*   Valid values in the live moderation scenario:

    *   **normal**: normal content
    *   **meaningless**: meaningless content, such as a black or white screen
    *   **PIP**: picture-in-picture
    *   **smoking**: smoking
    *   **drivelive**: live broadcasting in a running vehicle

*   Valid values in the logo moderation scenario:

    *   **normal**: normal content
    *   **TV**: controlled TV station logo
    *   **trademark**: trademark', example='normal'),
                rate?: string(name='Rate', description='The score.', example='99.99'),
                scene?: string(name='Scene', description='The moderation scenario. Valid values:

*   **porn**: pornographic content moderation
*   **terrorism**: terrorist content moderation
*   **ad**: ad violation moderation
*   **live**: undesirable scene moderation
*   **logo**: special logo moderation', example='pron'),
                suggestion?: string(name='Suggestion', description='The recommended subsequent operation. Valid values:

*   **pass**: The content passes the moderation.
*   **review**: The content needs to be manually reviewed again.
*   **block**: The content needs to be blocked.', example='block'),
              }
            ](name='CensorResult')
            }(name='CensorResults', description='The moderation results that include information such as labels and scores.'),
            object?: string(name='Object', description='The OSS object that is generated as the output snapshot.

> In the example, {Count} is a placeholder. The OSS objects that are generated as output snapshots are named `output00001-****.jpg, output00002-****.jpg`, and so on.', example='output{Count}.jpg'),
            timestamp?: string(name='Timestamp', description='The position in the video. Format: `hh:mm:ss[.SSS]`.', example='00:02:59.999'),
          }
        ](name='VideoTimeline')
        }(name='VideoTimelines', description='The moderation results that are sorted in ascending order by time.'),
      }(name='VensorCensorResult', description='The moderation results of the video.'),
      videoCensorConfig?: {
        bizType?: string(name='BizType', description='The moderation template. Default value: common. The default value indicates that the default template is used.

> If the moderation template is not specified, the default value common is returned. If a custom moderation template that is created by submitting a ticket is specified, a user ID is returned.', example='common'),
        outputFile?: {
          bucket?: string(name='Bucket', description='The OSS bucket that stores the output snapshot.', example='test-bucket-****'),
          location?: string(name='Location', description='The OSS region in which the OSS bucket for storing the output snapshot resides.', example='oss-cn-shanghai'),
          object?: string(name='Object', description='The one or more OSS objects that are generated as the output snapshots.

> In the example, {Count} is a placeholder. The OSS objects that are generated as output snapshots are named `output00001-****.jpg, output00002-****.jpg`, and so on.', example='output{Count}.jpg'),
        }(name='OutputFile', description='The information about output snapshots.'),
        videoCensor?: string(name='VideoCensor', description='Indicates whether the video content needs to be moderated. Default value: true. Valid values:

*   **true**: The video content needs to be moderated.
*   **false**: The video content does not need to be moderated.', example='true'),
      }(name='VideoCensorConfig', description='The video moderation configurations.'),
    }
  ](name='MediaCensorJob')
  }(name='MediaCensorJobList', description='The content moderation jobs.'),
  nextPageToken?: string(name='NextPageToken', description='The token that is used to retrieve the next page of the query results. The value is a UUID that contains 32 characters. If the returned query results cannot be displayed within one page, this parameter is returned. The value of this parameter is updated for each query.', example='9b1a42bc6e8d46e6a1383b7e7f01****'),
  nonExistIds?: {
    string?: [ string ](name='String')
  }(name='NonExistIds', description='The IDs of the jobs that do not exist. This parameter is not returned if all specified jobs are found.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='D1D5C080-8E2F-5030-8AB4-13092F17631B'),
}

model QueryMediaCensorJobListResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: QueryMediaCensorJobListResponseBody(name='body'),
}

/**
  * You can call this operation to query only the content moderation jobs within three months.
  * ### QPS limit
  * You can call this operation up to 50 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function queryMediaCensorJobList(request: QueryMediaCensorJobListRequest): QueryMediaCensorJobListResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'QueryMediaCensorJobList', 'POST', '/', 'json', false, 'json', request);
}

model QueryMediaInfoJobListRequest {
  mediaInfoJobIds: string(name='MediaInfoJobIds', description='The IDs of the media information analysis jobs.

*   You can query up to 10 jobs at a time. Separate multiple IDs with commas (,).
*   You can obtain the details from the response parameters of the [SubmitMediaInfoJob](~~602827~~) operation.

>  If you do not specify the JobIds parameter, the **InvalidParameter** error code is returned.', example='23ca1d184c0e4341e5b665e2a12****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model QueryMediaInfoJobListResponseBody = {
  mediaInfoJobList?: {
    mediaInfoJob?: [ 
    {
      async?: boolean(name='Async', description='Indicates whether the job is in asynchronous mode.', example='true'),
      code?: string(name='Code', description='The error code returned if the job fails.', example='InvalidParameter.JsonObjectFormatInvalid'),
      creationTime?: string(name='CreationTime', description='The time when the job was created.', example='2014-01-10T12:00:00Z'),
      input?: {
        bucket?: string(name='Bucket', description='The OSS bucket in which the input file is stored.', example='example-bucket'),
        location?: string(name='Location', description='The ID of the OSS region.', example='oss-cn-hangzhou'),
        object?: string(name='Object', description='The name of the Object Storage Service (OSS) object that is used as the input file.', example='example.mp4'),
      }(name='Input', description='The information about the job input.'),
      jobId?: string(name='JobId', description='The ID of the job.', example='23ca1d184c0e4341e5b665e2a12****'),
      MNSMessageResult?: {
        errorCode?: string(name='ErrorCode', description='The error code returned if the job failed. This parameter is not returned if the job is successful.', example='InvalidParameter.JsonObjectFormatInvalid'),
        errorMessage?: string(name='ErrorMessage', description='The error message returned if the job failed. This parameter is not returned if the job is successful.', example='The parameter \\"Input\\" does not conform to the JSON Object specification'),
        messageId?: string(name='MessageId', description='The ID of the message returned if the job was successful. This parameter is not returned if the job fails.', example='123'),
      }(name='MNSMessageResult', description='The message sent by MNS to notify you of the job result.'),
      message?: string(name='Message', description='The error message returned if the job fails.', example='The parameter ”*” does not conform to the JSON Object specification'),
      pipelineId?: string(name='PipelineId', description='The ID of the MPS queue.', example='88c6ca184c0e432bbf5b665e2a15****'),
      properties?: {
        bitrate?: string(name='Bitrate', description='The bitrate of the media file.', example='1630.045'),
        duration?: string(name='Duration', description='The duration of the media file.', example='17.226000'),
        fileFormat?: string(name='FileFormat', description='The format of the input media file.', example='QuickTime/MOV'),
        fileSize?: string(name='FileSize', description='The size of the image file.', example='3509895'),
        format?: {
          bitrate?: string(name='Bitrate', description='The total bitrate.', example='1630.045'),
          duration?: string(name='Duration', description='The total duration.', example='17.226000'),
          formatLongName?: string(name='FormatLongName', description='The full name of the container format.', example='QuickTime/MOV'),
          formatName?: string(name='FormatName', description='The short name of the container format.', example='mov'),
          numPrograms?: string(name='NumPrograms', description='The total number of program streams.', example='2'),
          numStreams?: string(name='NumStreams', description='The total number of media streams.', example='1'),
          size?: string(name='Size', description='The size of the image file.', example='3509895'),
          startTime?: string(name='StartTime', description='The start time.', example='0.042000'),
        }(name='Format', description='The format information.'),
        fps?: string(name='Fps', description='The frame rate of the media file.', example='25'),
        height?: string(name='Height', description='The height of the video. Unit: pixel.', example='720'),
        streams?: {
          audioStreamList?: {
            audioStream?: [ 
            {
              bitrate?: string(name='Bitrate', description='The bitrate of the media file.', example='1536000'),
              channelLayout?: string(name='ChannelLayout', description='The number of sound channels.', example='5.1(side)'),
              channels?: string(name='Channels', description='The output layout of the sound channels.', example='2'),
              codecLongName?: string(name='CodecLongName', description='The full name of the encoding format.', example='DCA (DTS Coherent Acoustics)'),
              codecName?: string(name='CodecName', description='The short name of the encoding format. Valid values:

*   **acc**
*   **mp3**
*   **mp4**
*   **ogg**
*   **flac**', example='acc'),
              codecTag?: string(name='CodecTag', description='The tag of the encoding format.', example='0x0000'),
              codecTagString?: string(name='CodecTagString', description='The tag string of the encoding format.', example='[0][0][0][0]'),
              codecTimeBase?: string(name='CodecTimeBase', description='The codec time base.', example='1/48000'),
              duration?: string(name='Duration', description='The duration of the media file.', example='123'),
              index?: string(name='Index', description='The sequence number of the audio stream. The value indicates the position of the audio stream in all audio streams.', example='1'),
              lang?: string(name='Lang', description='The language. For more information, see [FFmpeg documentation](https://www.ffmpeg.org/ffmpeg-all.html?spm=a2c4g.11186623.2.66.243851cd2SntfN#Metadata).', example='eng'),
              numFrames?: string(name='NumFrames', description='The total number of frames.', example='123'),
              sampleFmt?: string(name='SampleFmt', description='The sampling format.', example='fltp'),
              samplerate?: string(name='Samplerate', description='The sampling rate.', example='48000'),
              startTime?: string(name='StartTime', description='The start time.', example='0.042000'),
              timebase?: string(name='Timebase', description='The time base.', example='1/1000'),
            }
          ](name='AudioStream')
          }(name='AudioStreamList', description='The information about each audio stream.'),
          subtitleStreamList?: {
            subtitleStream?: [ 
            {
              codecLongName?: string(name='CodecLongName', description='The full name of the encoding format.', example='ASS (Advanced SSA) subtitle'),
              codecName?: string(name='CodecName', description='The short name of the encoding format. Valid values:

*   **srt**
*   **ass**', example='ass'),
              codecTag?: string(name='CodecTag', description='The tag of the encoding format.', example='0x0000'),
              codecTagString?: string(name='CodecTagString', description='The tag string of the encoding format.', example='[0][0][0][0]'),
              codecTimeBase?: string(name='CodecTimeBase', description='The codec time base.', example='0/1'),
              duration?: string(name='Duration', description='The duration. Unit: seconds.', example='1370.116000'),
              index?: string(name='Index', description='The sequence number of the subtitle stream. The value indicates the position of the subtitle stream in all subtitle streams.', example='3'),
              lang?: string(name='Lang', description='The language.', example='eng'),
              startTime?: string(name='StartTime', description='The start time.', example='0.000000'),
              timebase?: string(name='Timebase', description='The time base.', example='1/1000'),
            }
          ](name='SubtitleStream')
          }(name='SubtitleStreamList', description='The information about each subtitle stream.'),
          videoStreamList?: {
            videoStream?: [ 
            {
              avgFPS?: string(name='AvgFPS', description='The average frame rate.', example='24000/1001'),
              bitrate?: string(name='Bitrate', description='The bitrate of the media file.', example='30541090'),
              codecLongName?: string(name='CodecLongName', description='The full name of the encoding format.', example='H.264/AVC/MPEG-4 AVC/MPEG-4 part 10'),
              codecName?: string(name='CodecName', description='The short name of the encoding format. Valid values:

*   **h264**
*   **h265**
*   **gif**
*   **webp**', example='h264'),
              codecTag?: string(name='CodecTag', description='The tag of the encoding format.', example='0x0000'),
              codecTagString?: string(name='CodecTagString', description='The tag string of the encoding format.', example='[0][0][0][0]'),
              codecTimeBase?: string(name='CodecTimeBase', description='The codec time base.', example='1001/48000'),
              colorPrimaries?: string(name='ColorPrimaries', description='The level of color reconstruction.', example='700'),
              colorRange?: string(name='ColorRange', description='The color range.', example='700'),
              colorTransfer?: string(name='ColorTransfer', description='The color channel.', example='R255 G83 B170'),
              dar?: string(name='Dar', description='The display aspect ratio (DAR).', example='16:9'),
              duration?: string(name='Duration', description='The duration of the media file.', example='100'),
              fps?: string(name='Fps', description='The frame rate of the media file.', example='25'),
              hasBFrames?: string(name='HasBFrames', description='Indicates whether the video stream contains bidirectional frames (B-frames). A value of 1 indicates that the video stream contains B-frames. A value of 0 indicates that the video stream does not contain B-frames.', example='0'),
              height?: string(name='Height', description='The height of the video stream in pixels.', example='1080'),
              index?: string(name='Index', description='The sequence number of the video stream. The value indicates the position of the video stream in all video streams.', example='1'),
              lang?: string(name='Lang', description='The language.', example='eng'),
              level?: string(name='Level', description='The codec level.', example='41'),
              networkCost?: {
                avgBitrate?: string(name='AvgBitrate', description='The average bitrate of the video stream.', example='300.34'),
                costBandwidth?: string(name='CostBandwidth', description='The maximum bandwidth that is consumed.', example='10'),
                preloadTime?: string(name='PreloadTime', description='The time consumed to preload the video.', example='8'),
              }(name='NetworkCost', description='The network bandwidth that is consumed.'),
              numFrames?: string(name='NumFrames', description='The total number of frames.', example='100'),
              pixFmt?: string(name='PixFmt', description='The pixel format.', example='yuv420p'),
              profile?: string(name='Profile', description='The codec profile.', example='High'),
              rotate?: string(name='Rotate', description='The rotation angle of the video.', example='180'),
              sar?: string(name='Sar', description='The sample aspect ratio (SAR).', example='1:1'),
              startTime?: string(name='StartTime', description='The start time.', example='0.042000'),
              timebase?: string(name='Timebase', description='The time base.', example='1/1000'),
              width?: string(name='Width', description='The width of the video in pixels.', example='1920'),
            }
          ](name='VideoStream')
          }(name='VideoStreamList', description='The information about each video stream.'),
        }(name='Streams', description='The media streams that are contained in the input media file.'),
        width?: string(name='Width', description='The width of the video. Unit: pixel.', example='1280'),
      }(name='Properties', description='The information about the input file. For more information, see [AliyunProperties](~~29251~~).'),
      state?: string(name='State', description='The status of the job.

*   **Analyzing**: The job is being run.
*   **Success**: The job is successful.
*   **Fail**: The job fails.', example='Success'),
      userData?: string(name='UserData', description='The custom data.', example='testid-001'),
    }
  ](name='MediaInfoJob')
  }(name='MediaInfoJobList', description='The details of each returned media information analysis job.'),
  nonExistMediaInfoJobIds?: {
    string?: [ string ](name='String')
  }(name='NonExistMediaInfoJobIds', description='Nonexistent media information analysis jobs.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='46A04AA5-B119-41BB-B750-7C5327AC3E7A'),
}

model QueryMediaInfoJobListResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: QueryMediaInfoJobListResponseBody(name='body'),
}

/**
  * *   In asynchronous mode, the media information can be retrieved only after the Message Service (MNS) callback of **submitting a media information job** is returned. If you have not retrieved the media information for a long period, the job may have failed.
  * *   You can call this operation to query up to 10 media information analysis jobs at a time.
  * *   By default, returned jobs are sorted in descending order by CreationTime.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function queryMediaInfoJobList(request: QueryMediaInfoJobListRequest): QueryMediaInfoJobListResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'QueryMediaInfoJobList', 'POST', '/', 'json', false, 'json', request);
}

model QueryMediaListRequest {
  includeMediaInfo?: boolean(name='IncludeMediaInfo', description='Specifies whether to include media information in the returned result.

*   Valid values: true and false.
*   Default value: **false**.', example='true', position='Query'),
  includePlayList?: boolean(name='IncludePlayList', description='Specifies whether to include playback information in the returned result.

*   Valid values: true and false.
*   Default value: **false**.', example='true', position='Query'),
  includeSnapshotList?: boolean(name='IncludeSnapshotList', description='Specifies whether to include snapshot information in the returned result.

*   Valid values: true and false.
*   Default value: **false**.', example='true', position='Query'),
  includeSummaryList?: boolean(name='IncludeSummaryList', description='Specifies whether to include summaries in the returned result.

*   Valid values: true and false.
*   Default value: **false**.', example='true', position='Query'),
  mediaIds: string(name='MediaIds', description='The IDs of the media files. To obtain the ID of a media file, you can perform the following operations in the ApsaraVideo Media Processing (MPS) console: In the left-side navigation pane, choose **Media Management** > **Media List**. Find the required video and click Manage. The ID of the video is displayed on the Basics tab. Separate multiple IDs with commas (,). You can query up to 10 media files at a time.', example='3e1cd21131a94525be55acf65888****,e26cfa29e784402388463f61dbec****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model QueryMediaListResponseBody = {
  mediaList?: {
    media?: [ 
    {
      bitrate?: string(name='Bitrate', description='The bitrate.', example='2659.326'),
      cateId?: long(name='CateId', description='The ID of the category to which the media file belongs.', example='1'),
      censorState?: string(name='CensorState', description='The review status of the media file. Valid values:

*   **Initiated**: The media file is uploaded but not reviewed.
*   **Pass**: The media file is uploaded and passes the review.', example='Initiated'),
      coverURL?: string(name='CoverURL', description='The OSS URL of the thumbnail.', example='http://example-bucket1-****.oss-cn-hangzhou.aliyuncs.com//example-****.png'),
      creationTime?: string(name='CreationTime', description='The time when the media file was created.', example='2016-09-14T08:30:33Z'),
      description?: string(name='Description', description='The description.', example='This is description ****'),
      duration?: string(name='Duration', description='The duration.', example='7.965000'),
      file?: {
        state?: string(name='State', description='The status of the input file. Valid values:

*   **Normal**: normal
*   **Deleted**: deleted', example='Normal'),
        URL?: string(name='URL', description='The OSS URL of the input file.', example='http://example-bucket-****.oss-cn-hangzhou.aliyuncs.com//example-****.mp4'),
      }(name='File', description='The details of the input file.'),
      format?: string(name='Format', description='The encoding format. Valid values: mov, mp4, m4a, 3gp, 3g2, and mj2.', example='mov'),
      fps?: string(name='Fps', description='The frame rate.', example='25.0'),
      height?: string(name='Height', description='The height of the media file.', example='1080'),
      mediaId?: string(name='MediaId', description='The ID of the media file.', example='3e1cd21131a94525be55acf65888****'),
      mediaInfo?: {
        format?: {
          bitrate?: string(name='Bitrate', description='The bitrate.', example='2659.326'),
          duration?: string(name='Duration', description='The total duration.', example='7.965000'),
          formatLongName?: string(name='FormatLongName', description='The full name of the container format.', example='QuickTime/MOV'),
          formatName?: string(name='FormatName', description='The short name of the container format. Valid values: mov, mp4, m4a, 3gp, 3g2, and mj2.', example='mov'),
          numPrograms?: string(name='NumPrograms', description='The total number of program streams.', example='2'),
          numStreams?: string(name='NumStreams', description='The total number of media streams.', example='2'),
          size?: string(name='Size', description='The size of the file.', example='2647692'),
          startTime?: string(name='StartTime', description='The start time.', example='0.000000'),
        }(name='Format', description='The format information.'),
        streams?: {
          audioStreamList?: {
            audioStream?: [ 
            {
              bitrate?: string(name='Bitrate', description='The bitrate.', example='160.008'),
              channelLayout?: string(name='ChannelLayout', description='The output layout of the sound channels.', example='stereo'),
              channels?: string(name='Channels', description='The number of sound channels.', example='2'),
              codecLongName?: string(name='CodecLongName', description='The full name of the encoding format.', example='AAC(Advanced Audio Coding)'),
              codecName?: string(name='CodecName', description='The short name of the encoding format. Valid values: H264, mov, aac, avc, and mpeg.', example='mov'),
              codecTag?: string(name='CodecTag', description='The tag of the encoding format.', example='0x6134706d'),
              codecTagString?: string(name='CodecTagString', description='The tag string of the encoding format.', example='mp4a'),
              codecTimeBase?: string(name='CodecTimeBase', description='The codec time base.', example='1/44100'),
              duration?: string(name='Duration', description='The duration.', example='182.591995'),
              index?: string(name='Index', description='The sequence number of the audio stream. The value indicates the position of the audio stream in all audio streams.', example='1'),
              lang?: string(name='Lang', description='The language. For more information, see [FFmpeg documentation](https://www.ffmpeg.org/ffmpeg-all.html#Metadata).', example='und'),
              numFrames?: string(name='NumFrames', description='The total number of frames.', example='100'),
              sampleFmt?: string(name='SampleFmt', description='The sampling format.', example='fltp'),
              samplerate?: string(name='Samplerate', description='The sampling rate.', example='44100'),
              startTime?: string(name='StartTime', description='The start time.', example='0.000000'),
              timebase?: string(name='Timebase', description='The time base.', example='1/44100'),
            }
          ](name='AudioStream')
          }(name='AudioStreamList', description='The list of audio streams.'),
          subtitleStreamList?: {
            subtitleStream?: [ 
            {
              index?: string(name='Index', description='The sequence number of the subtitle stream. The value indicates the position of the subtitle stream in all subtitle streams.', example='3'),
              lang?: string(name='Lang', description='The language. For more information, see [FFmpeg documentation](https://www.ffmpeg.org/ffmpeg-all.html#Metadata).', example='und'),
            }
          ](name='SubtitleStream')
          }(name='SubtitleStreamList', description='The list of subtitle streams.'),
          videoStreamList?: {
            videoStream?: [ 
            {
              avgFPS?: string(name='AvgFPS', description='The average frame rate.', example='29.97003'),
              bitrate?: string(name='Bitrate', description='The bitrate.', example='2659.326'),
              codecLongName?: string(name='CodecLongName', description='The full name of the encoding format.', example='QuickTime/MOV'),
              codecName?: string(name='CodecName', description='The short name of the encoding format. Valid values: mov, mp4, m4a, 3gp, 3g2, and mj2.', example='mov'),
              codecTag?: string(name='CodecTag', description='The tag of the encoding format.', example='0x31637661'),
              codecTagString?: string(name='CodecTagString', description='The tag string of the encoding format.', example='avc1'),
              codecTimeBase?: string(name='CodecTimeBase', description='The codec time base.', example='1001/60000'),
              dar?: string(name='Dar', description='The display aspect ratio (DAR).', example='16:9'),
              duration?: string(name='Duration', description='The duration.', example='182.683000'),
              fps?: string(name='Fps', description='The frame rate.', example='29.97003'),
              hasBFrames?: string(name='HasBFrames', description='Indicates whether the video stream contains bidirectional frames (B-frames). A value of **1** indicates that the video stream contains B-frames. A value of **2** indicates that the video stream does not contain B-frames.', example='2'),
              height?: string(name='Height', description='The latter number in the video resolution. The number indicates the video height.', example='1080'),
              index?: string(name='Index', description='The sequence number of the video stream. The value indicates the position of the video stream in all video streams.', example='5'),
              lang?: string(name='Lang', description='The language. For more information, see [FFmpeg documentation](https://www.ffmpeg.org/ffmpeg-all.html#Metadata).', example='und'),
              level?: string(name='Level', description='The codec level.', example='40'),
              networkCost?: {
                avgBitrate?: string(name='AvgBitrate', description='The average bitrate.', example='2659.326'),
                costBandwidth?: string(name='CostBandwidth', description='The maximum bandwidth that was consumed.', example='100'),
                preloadTime?: string(name='PreloadTime', description='The amount of preload time.', example='0.01'),
              }(name='NetworkCost', description='The network bandwidth consumption.'),
              numFrames?: string(name='NumFrames', description='The total number of frames.', example='12'),
              pixFmt?: string(name='PixFmt', description='The pixel format.', example='yuv420p'),
              profile?: string(name='Profile', description='The codec profile.', example='High'),
              rotate?: string(name='Rotate', description='The video rotation angle.', example='90'),
              sar?: string(name='Sar', description='The sample aspect ratio (SAR).', example='1:1'),
              startTime?: string(name='StartTime', description='The start time.', example='0.000000'),
              timebase?: string(name='Timebase', description='The time base.', example='1/30000'),
              width?: string(name='Width', description='The former number in the video resolution. The number indicates the video width.', example='100'),
            }
          ](name='VideoStream')
          }(name='VideoStreamList', description='The list of video streams.'),
        }(name='Streams', description='The stream information.'),
      }(name='MediaInfo', description='The media information.'),
      playList?: {
        play?: [ 
        {
          activityName?: string(name='ActivityName', description='The name of the workflow activity.', example='example-activity-****'),
          bitrate?: string(name='Bitrate', description='The bitrate of the media file.', example='2659.326'),
          duration?: string(name='Duration', description='The duration of the media file.', example='7.965000'),
          encryption?: string(name='Encryption', description='Indicates whether the media file is encrypted. Valid values:

*   **0**: The media file is not encrypted.
*   **1**: The media file is encrypted.', example='0'),
          file?: {
            state?: string(name='State', description='The status of the file. Valid values:

*   **Normal**: normal
*   **Deleted**: deleted', example='Normal'),
            URL?: string(name='URL', description='The Object Storage Service (OSS) URL of the output file.', example='http://example-bucket-****.oss-cn-hangzhou.aliyuncs.com//example-****.mp4'),
          }(name='File', description='The playback file.'),
          format?: string(name='Format', description='The encoding format of the media file. Valid values: mov, mp4, m4a, 3gp, 3g2, and mj2.', example='mp4'),
          fps?: string(name='Fps', description='The frame rate of the media file.', example='25.0'),
          height?: string(name='Height', description='The height.', example='1080'),
          mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the workflow that generates the playback file.', example='93ab850b4f6f44eab54b6e91d24d****'),
          mediaWorkflowName?: string(name='MediaWorkflowName', description='The name of the workflow that generates the playback file.', example='example-mediaworkflow-****'),
          size?: string(name='Size', description='The size of the media file.', example='2647692'),
          width?: string(name='Width', description='The width of the media file.', example='760'),
        }
      ](name='Play')
      }(name='PlayList', description='The playlist.'),
      publishState?: string(name='PublishState', description='The publishing status of the media file. Valid values:

- **Initiated**: The media file is in the initial state.
- **UnPublish**: The media file has not been published, and the playback permission on the OSS object is Private.
- **Published**: The media file has been published, and the playback permission on the OSS object is Default.
- **Deleted**: The media file has been deleted.', example='Published'),
      runIdList?: {
        runId?: [ string ](name='RunId')
      }(name='RunIdList', description='The ID of the instance.'),
      size?: string(name='Size', description='The size of the file.', example='2647692'),
      snapshotList?: {
        snapshot?: [ 
        {
          activityName?: string(name='ActivityName', description='The name of the workflow activity that generates the snapshot.', example='example-activity1-****'),
          count?: string(name='Count', description='The number of snapshots. This parameter is valid only when the value of the **Type** parameter is **Sequence**.', example='5'),
          file?: {
            state?: string(name='State', description='The status of the file. Valid values:

- **Normal**: normal
- **Deleted**: deleted', example='Normal'),
            URL?: string(name='URL', description='The OSS URL of the snapshot.', example='http://example1-bucket1-****.oss-cn-hangzhou.aliyuncs.com//example111-****.png'),
          }(name='File', description='The snapshot.'),
          mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the workflow that generates the snapshot.', example='6cc3aa66d1cb4bb2adf14e726c0a****'),
          mediaWorkflowName?: string(name='MediaWorkflowName', description='The name of the workflow that generates the snapshot.', example='example-workflow-****'),
          type?: string(name='Type', description='The type of the snapshot. Valid values:

- **Single**
- **Sequence**', example='Sequence'),
        }
      ](name='Snapshot')
      }(name='SnapshotList', description='The list of snapshots.'),
      summaryList?: {
        summary?: [ 
        {
          activityName?: string(name='ActivityName', description='The name of the workflow activity.', example='example-activity-****'),
          file?: {
            state?: string(name='State', description='The status of the file. Valid values:

*   **Normal**: normal
*   **Deleted**: deleted', example='Normal'),
            URL?: string(name='URL', description='The OSS URL of the input file.', example='http://example-bucket-****.oss-cn-hangzhou.aliyuncs.com//example-****.mp4'),
          }(name='File', description='The information about the input file.'),
          mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the workflow that generates the summary.', example='93ab850b4f6f44eab54b6e91d24d****'),
          mediaWorkflowName?: string(name='MediaWorkflowName', description='The name of the workflow that generates the summary.', example='example-mediaworkflow-****'),
          type?: string(name='Type', description='The type of the summary. Valid values:

*   **Video**: video
*   **Gif**: dynamic image', example='Video'),
        }
      ](name='Summary')
      }(name='SummaryList', description='The list of video summaries.'),
      tags?: {
        tag?: [ string ](name='Tag')
      }(name='Tags', description='The tags of the media file.'),
      title?: string(name='Title', description='The title.', example='example-title-****'),
      width?: string(name='Width', description='The width.', example='1920'),
    }
  ](name='Media')
  }(name='MediaList', description='The list of media files.'),
  nonExistMediaIds?: {
    mediaId?: [ string ](name='MediaId')
  }(name='NonExistMediaIds', description='The IDs of the media files that do not exist. This parameter is not returned when all specified media files exist.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='283DC68C-146F-4489-A2A1-2F88F1472A56'),
}

model QueryMediaListResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: QueryMediaListResponseBody(name='body'),
}

/**
  * You can call this operation to query up to 10 media files at a time.
  * ## QPS limit
  * You can call this operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
 */
async function queryMediaList(request: QueryMediaListRequest): QueryMediaListResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'QueryMediaList', 'POST', '/', 'json', false, 'json', request);
}

model QueryMediaListByURLRequest {
  fileURLs: string(name='FileURLs', description='The OSS URLs of the media files. To obtain the OSS URL of a media file, you can perform the following operations in the ApsaraVideo Media Processing (MPS) console: In the left-side navigation pane, choose **Media Management** > **Media List**. Find the media file whose OSS URL you want to view and click **Manage** in the **Actions** column. The OSS URL of the media file is displayed on the **Obtain Encoding URL** tab. Separate multiple URLs with commas (,). You can query up to 10 media files at a time.

*   The URL complies with RFC 3986 and is encoded in UTF-8, with reserved characters being percent-encoded. The value can be up to 3,200 bytes in size. For more information, see [URL encoding](~~423796~~).
*   Only OSS HTTP URLs are supported. Alibaba Cloud CDN URLs and HTTPS URLs are not supported.', example='http://example-bucket-****.oss-cn-shanghai.aliyuncs.com/example.mp4', position='Query'),
  includeMediaInfo?: boolean(name='IncludeMediaInfo', description='Specifies whether to include media information in the returned result.

*   Valid values: true and false.

*   Default value: **false**.

> To obtain detailed information about the media files, set this parameter to true.', example='true', position='Query'),
  includePlayList?: boolean(name='IncludePlayList', description='Specifies whether to include playback information in the returned result.

*   Valid values: true and false.
*   Default value: **false**.', example='true', position='Query'),
  includeSnapshotList?: boolean(name='IncludeSnapshotList', description='Specifies whether to include snapshot information in the returned result.

*   Valid values: true and false.
*   Default value: **false**.', example='true', position='Query'),
  includeSummaryList?: boolean(name='IncludeSummaryList', description='Specifies whether to include summaries in the returned result.

*   Valid values: true and false.
*   Default value: **false**.', example='true', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model QueryMediaListByURLResponseBody = {
  mediaList?: {
    media?: [ 
    {
      bitrate?: string(name='Bitrate', description='The bitrate.', example='593.192'),
      cateId?: long(name='CateId', description='The ID of the category to which the media file belongs.', example='123'),
      censorState?: string(name='CensorState', description='The review status of the media file. Valid values:

*   **Initiated**: The media file is uploaded but not reviewed.
*   **Pass**: The media file is uploaded and passes the review.', example='Initiated'),
      coverURL?: string(name='CoverURL', description='The OSS URL of the thumbnail.', example='http://example-bucket1-****.oss-cn-hangzhou.aliyuncs.com//example-****.png'),
      creationTime?: string(name='CreationTime', description='The time when the media file was created.', example='2021-07-14T13:05:00Z'),
      description?: string(name='Description', description='The description.', example='This is description ****'),
      duration?: string(name='Duration', description='The duration.', example='79.204000'),
      file?: {
        state?: string(name='State', description='The status of the media file. Valid values:

*   **Normal**: The file is normal.
*   **Deleted**: The file is deleted.', example='Normal'),
        URL?: string(name='URL', description='The OSS URL of the input file.', example='http://example-bucket-****.oss-cn-hangzhou.aliyuncs.com//example-****.mp4'),
      }(name='File', description='The details of the input file.'),
      format?: string(name='Format', description='The encoding format. Valid values: mov, mp4, m4a, 3gp, 3g2, and mj2.', example='mov'),
      fps?: string(name='Fps', description='The frame rate.', example='15.0'),
      height?: string(name='Height', description='The height of the queried media file.', example='360'),
      mediaId?: string(name='MediaId', description='The ID of the media file.', example='52d7e98b05e648199612290bb819****'),
      mediaInfo?: {
        format?: {
          bitrate?: string(name='Bitrate', description='The bitrate.', example='593.192'),
          duration?: string(name='Duration', description='The duration.', example='79.204000'),
          formatLongName?: string(name='FormatLongName', description='The full name of the encoding format.', example='QuickTime/MOV'),
          formatName?: string(name='FormatName', description='The short name of the container format. Valid values: mov, mp4, m4a, 3gp, 3g2, and mj2.', example='mov'),
          numPrograms?: string(name='NumPrograms', description='The total number of program streams.', example='0'),
          numStreams?: string(name='NumStreams', description='The total number of media streams.', example='2'),
          size?: string(name='Size', description='The size.', example='5872904'),
          startTime?: string(name='StartTime', description='The start time.', example='0.000000'),
        }(name='Format', description='The format information.'),
        streams?: {
          audioStreamList?: {
            audioStream?: [ 
            {
              bitrate?: string(name='Bitrate', description='The bitrate.', example='76.356'),
              channelLayout?: string(name='ChannelLayout', description='The output layout of the sound channels.', example='stereo'),
              channels?: string(name='Channels', description='The number of sound channels.', example='2'),
              codecLongName?: string(name='CodecLongName', description='The full name of the encoding format.', example='AAC (Advanced Audio Coding)'),
              codecName?: string(name='CodecName', description='The short name of the encoding format. Valid values: H264, mov, aac, avc, and mpeg.', example='aac'),
              codecTag?: string(name='CodecTag', description='The tag of the encoding format.', example='0x6134706d'),
              codecTagString?: string(name='CodecTagString', description='The tag string of the encoding format.', example='mp4a'),
              codecTimeBase?: string(name='CodecTimeBase', description='The codec time base.', example='1/44100'),
              duration?: string(name='Duration', description='The duration of the media file.', example='79.203265'),
              index?: string(name='Index', description='The sequence number of the audio stream. The value indicates the position of the audio stream in all audio streams.', example='1'),
              lang?: string(name='Lang', description='The language. For more information, see [FFmpeg language definition](https://www.ffmpeg.org/ffmpeg-all.html#Metadata).', example='und'),
              numFrames?: string(name='NumFrames', description='The total number of frames.', example='100'),
              sampleFmt?: string(name='SampleFmt', description='The sampling format.', example='fltp'),
              samplerate?: string(name='Samplerate', description='The sampling rate.', example='44100'),
              startTime?: string(name='StartTime', description='The start time.', example='0.000000'),
              timebase?: string(name='Timebase', description='The time base.', example='1/44100'),
            }
          ](name='AudioStream')
          }(name='AudioStreamList', description='The list of audio streams.'),
          subtitleStreamList?: {
            subtitleStream?: [ 
            {
              index?: string(name='Index', description='The sequence number of the subtitle stream. The value indicates the position of the subtitle stream in all subtitle streams.', example='1'),
              lang?: string(name='Lang', description='The language. For more information, see [FFmpeg language definition](https://www.ffmpeg.org/ffmpeg-all.html#Metadata).', example='und'),
            }
          ](name='SubtitleStream')
          }(name='SubtitleStreamList', description='The list of subtitle streams.'),
          videoStreamList?: {
            videoStream?: [ 
            {
              avgFPS?: string(name='AvgFPS', description='The average frame rate.', example='15.0'),
              bitrate?: string(name='Bitrate', description='The bitrate.', example='512.701'),
              codecLongName?: string(name='CodecLongName', description='The full name of the encoding format.', example='H.264/AVC/MPEG-4 AVC/MPEG-4 part 10'),
              codecName?: string(name='CodecName', description='The short name of the encoding format. Valid values: H264, mov, aac, avc, and mpeg.', example='H264'),
              codecTag?: string(name='CodecTag', description='The tag of the encoding format.', example='0x31637661'),
              codecTagString?: string(name='CodecTagString', description='The tag string of the encoding format.', example='avc1'),
              codecTimeBase?: string(name='CodecTimeBase', description='The codec time base.', example='1/30'),
              dar?: string(name='Dar', description='The display aspect ratio (DAR) of the video stream.', example='16:9'),
              duration?: string(name='Duration', description='The duration.', example='79.200000'),
              fps?: string(name='Fps', description='The frame rate.', example='15.0'),
              hasBFrames?: string(name='HasBFrames', description='Indicates whether the video stream contains bidirectional frames (B-frames). A value of **1** indicates that the video stream contains B-frames. A value of **2** indicates that the video stream does not contain B-frames.', example='2'),
              height?: string(name='Height', description='The latter number in the video resolution. The number indicates the video height.', example='360'),
              index?: string(name='Index', description='The sequence number of the video stream. The value indicates the position of the video stream in all video streams.', example='5'),
              lang?: string(name='Lang', description='The language. For more information, see [FFmpeg documentation](https://www.ffmpeg.org/ffmpeg-all.html#Metadata).', example='und'),
              level?: string(name='Level', description='The codec level.', example='31'),
              networkCost?: {
                avgBitrate?: string(name='AvgBitrate', description='The average bitrate of the video stream.', example='2659.326'),
                costBandwidth?: string(name='CostBandwidth', description='The maximum bandwidth that was consumed.', example='100'),
                preloadTime?: string(name='PreloadTime', description='The amount of preload time.', example='0.01'),
              }(name='NetworkCost', description='The network bandwidth consumption.'),
              numFrames?: string(name='NumFrames', description='The total number of frames.', example='12'),
              pixFmt?: string(name='PixFmt', description='The pixel format of the video stream.', example='yuv420p'),
              profile?: string(name='Profile', description='The codec profile.', example='High'),
              rotate?: string(name='Rotate', description='The rotation angle of the video.', example='90'),
              sar?: string(name='Sar', description='The sample aspect ratio (SAR).', example='1:1'),
              startTime?: string(name='StartTime', description='The start time.', example='0.046029'),
              timebase?: string(name='Timebase', description='The time base.', example='1/15360'),
              width?: string(name='Width', description='The former number in the video resolution. The number indicates the video width and cannot be negative.', example='640'),
            }
          ](name='VideoStream')
          }(name='VideoStreamList', description='The list of video streams.'),
        }(name='Streams', description='The stream information.'),
      }(name='MediaInfo', description='The media information.'),
      playList?: {
        play?: [ 
        {
          activityName?: string(name='ActivityName', description='The name of the workflow activity.', example='test name'),
          bitrate?: string(name='Bitrate', description='The bitrate of the media file.', example='25.067'),
          duration?: string(name='Duration', description='The duration.', example='7.965000'),
          encryption?: string(name='Encryption', description='Indicates whether the media file is encrypted. Valid values:

*   **0**: The media file is not encrypted.
*   **1**: The media file is encrypted.', example='1'),
          file?: {
            state?: string(name='State', description='The status of the media file. Valid values:

*   **Normal**: The file is normal.
*   **Deleted**: The file is deleted.', example='Normal'),
            URL?: string(name='URL', description='The OSS URL of the playback file.', example='http://example-bucket-****.oss-cn-hangzhou.aliyuncs.com//example-****.mp4l-test/in/1.mp4'),
          }(name='File', description='The playback file.'),
          format?: string(name='Format', description='The encoding format of the media file. Valid values: mov, mp4, m4a, 3gp, 3g2, and mj2.', example='mov'),
          fps?: string(name='Fps', description='The frame rate.', example='25.0'),
          height?: string(name='Height', description='The height of the media file.', example='10'),
          mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the workflow that generates the playback file.', example='6cc3aa66d1cb4bb2adf14e726c0a****'),
          mediaWorkflowName?: string(name='MediaWorkflowName', description='The name of the workflow that generates the playback file.', example='example-mediaworkflow-****'),
          size?: string(name='Size', description='The size.', example='100'),
          width?: string(name='Width', description='The width.', example='11'),
        }
      ](name='Play')
      }(name='PlayList', description='The playlist.'),
      publishState?: string(name='PublishState', description='The publishing status of the media file. Valid values:

- **Initiated**: The media file is in the initial state.
- **UnPublish**: The media file has not been published, and the playback permission on the OSS object is Private.
- **Published**: The media file has been published, and the playback permission on the OSS object is Default.
- **Deleted**: The file is deleted.', example='Published'),
      runIdList?: {
        runId?: [ string ](name='RunId')
      }(name='RunIdList', description='The IDs of the media workflow execution instances.'),
      size?: string(name='Size', description='The size of the file.', example='5872904'),
      snapshotList?: {
        snapshot?: [ 
        {
          activityName?: string(name='ActivityName', description='The name of the workflow activity that generates the snapshot.', example='example-activity1-****'),
          count?: string(name='Count', description='The number of snapshots. This parameter is valid only when the value of the **Type** parameter is **Sequence**.', example='3'),
          file?: {
            state?: string(name='State', description='The status of the file. Valid values:

- **Normal**: The file is normal.
- **Deleted**: The file is deleted.', example='Normal'),
            URL?: string(name='URL', description='The OSS URL of the snapshot.', example='http://example1-bucket1-****.oss-cn-hangzhou.aliyuncs.com//example111-****.png'),
          }(name='File', description='The snapshot.'),
          mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the workflow that generates the snapshot.', example='6cc3aa66d1cb4bb2adf14e726c0a****'),
          mediaWorkflowName?: string(name='MediaWorkflowName', description='The name of the workflow that generates the snapshot.', example='example-workflow-****'),
          type?: string(name='Type', description='The type of the snapshot. Valid values:

- **Single**: a single snapshot
- **Sequence**: snapshots in sequence', example='Single'),
        }
      ](name='Snapshot')
      }(name='SnapshotList', description='The list of snapshots.'),
      summaryList?: {
        summary?: [ 
        {
          activityName?: string(name='ActivityName', description='The name of the workflow activity.', example='example-activity-****'),
          file?: {
            state?: string(name='State', description='The status of the media file. Valid values:

*   **Normal**: The file is normal.
*   **Deleted**: The file is deleted.', example='Normal'),
            URL?: string(name='URL', description='The OSS URL of the input file.', example='http://example-bucket-****.o'),
          }(name='File', description='The information about the input file.'),
          mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the workflow that generates the summary.', example='93ab850b4f6f44eab54b6e91d24d****'),
          mediaWorkflowName?: string(name='MediaWorkflowName', description='The name of the workflow that generates the summary.', example='example-mediaworkflow-****'),
          type?: string(name='Type', description='The type of the summary. Valid values:

*   **Video**: video
*   **Gif**: dynamic image', example='Gif'),
        }
      ](name='Summary')
      }(name='SummaryList', description='The list of video summaries.'),
      tags?: {
        tag?: [ string ](name='Tag')
      }(name='Tags', description='The tags of the media file.'),
      title?: string(name='Title', description='The title.', example='kled.mp4'),
      width?: string(name='Width', description='The width.', example='640'),
    }
  ](name='Media')
  }(name='MediaList', description='The list of media files.'),
  nonExistFileURLs?: {
    fileURL?: [ string ](name='FileURL')
  }(name='NonExistFileURLs', description='The IDs of the media files that do not exist. This parameter is not returned if all specified media files exist.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='1C8A0AEB-4321-485B-B4CB-DA4E9E6C9B42'),
}

model QueryMediaListByURLResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: QueryMediaListByURLResponseBody(name='body'),
}

/**
  * *   You can call this operation to query up to 10 media files at a time.
  * *   Before you call this operation, you must call the [AddMedia](~~44458~~) operation to add media files.
  * *   You can call this operation to query only media files that are processed in a workflow. To obtain comprehensive information about a media file that is newly uploaded to OSS, you can call this operation after the corresponding workflow is complete. To query media files that are not processed in a workflow, you must call the [SubmitMediaInfoJob](~~29220~~) operation to submit a media information analysis job. After the job is complete, you can query the information about the media files.
  * ## QPS limit
  * You can call this API operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
 */
async function queryMediaListByURL(request: QueryMediaListByURLRequest): QueryMediaListByURLResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'QueryMediaListByURL', 'POST', '/', 'json', false, 'json', request);
}

model QueryMediaWorkflowExecutionListRequest {
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  runIds: string(name='RunIds', description='The IDs of the media workflow execution instances. To obtain the instance ID, log on to the **ApsaraVideo Media Processing (MPS) console** and choose **Workflows** > **Execution Instances** in the left-side navigation pane. Separate multiple IDs with commas (,). You can query a maximum of 10 media workflow execution instances at a time.', example='48e33690ac19445488c706924321****', position='Query'),
}

model QueryMediaWorkflowExecutionListResponseBody = {
  mediaWorkflowExecutionList?: {
    mediaWorkflowExecution?: [ 
    {
      activityList?: {
        activity?: [ 
        {
          code?: string(name='Code', description='The error code.

*   This parameter is returned only if **Fail** is returned for the State parameter.
*   This parameter is not returned if the method status is **Success**.', example='InvalidParameter.ResourceContentBad'),
          endTime?: string(name='EndTime', description='The time when the method ends.', example='2016-04-01T06:53:44Z'),
          jobId?: string(name='JobId', description='The IDs of the jobs that are generated when the methods are called, such as the job IDs for the analysis, transcode, and snapshot methods.', example='2376030d9d0849399cd20e20f4f3****'),
          MNSMessageResult?: {
            errorCode?: string(name='ErrorCode', description='The error code returned if the MNS message fails to be sent. This parameter is not returned if the MNS message is sent.', example='The Topic/Queue config is empty, not send message'),
            errorMessage?: string(name='ErrorMessage', description='The error message returned if the MNS message fails to be sent. This parameter is not returned if the MNS message is sent.', example='MessageConfigEmpty'),
            messageId?: string(name='MessageId', description='The ID of the message that indicates the MNS message is sent. This parameter is not returned if the MNS message fails to be sent.', example='4f3bc83233de4e2f81c7dade443e****'),
          }(name='MNSMessageResult', description='The message sent by Message Service (MNS) to notify the user of the job result.'),
          message?: string(name='Message', description='The error message.

*   This parameter is returned only if **Fail** is returned for the State parameter.
*   This parameter is not returned if the method status is **Success**.', example='The resource operated InputFile is bad'),
          name?: string(name='Name', description='The name of the method.

> The name of each method in a media workflow is unique.', example='Start'),
          startTime?: string(name='StartTime', description='The time when the method is called.', example='2016-04-01T06:53:44Z'),
          state?: string(name='State', description='The status of the workflow method. Valid values:

*   Running: The method is being called.
*   Success: The method is called.
*   Fail: The method failed to be called.
*   Skipped: The method is skipped.

> For example, after the analysis is complete, the transcode method is called and high-definition and standard-definition transcoding jobs are created. The system determines whether to run the jobs based on the analysis result. If the resolution of the input video is low, the high-definition transcoding job may be skipped.', example='Running'),
          type?: string(name='Type', description='The methods that are supported in the media workflow. Valid values: Start, Snapshot, Transcode, Analysis, and Report. For more information, see [Methods supported for media workflows](~~68494~~).', example='Start'),
        }
      ](name='Activity')
      }(name='ActivityList', description='The methods that are called in the media workflow.'),
      creationTime?: string(name='CreationTime', description='The time when the media workflow was created.', example='016-04-01T06:53:43Z'),
      input?: {
        inputFile?: {
          bucket?: string(name='Bucket', description='The name of the OSS bucket in which the input file is stored.', example='example-bucket-****'),
          location?: string(name='Location', description='The ID of the OSS region in which the input file resides.', example='mps-cn-shanghai'),
          object?: string(name='Object', description='The name of the OSS object that is used as the input file.', example='example-mediaWorkflow-****/example-object-****/example.mp4'),
        }(name='InputFile', description='The input file of the media workflow.'),
        userData?: string(name='UserData', description='The user-defined data.', example='example data ****'),
      }(name='Input', description='The input data of the media workflow.'),
      mediaId?: string(name='MediaId', description='The ID of the media asset. A media file contains all the information about a media workflow.', example='512046582a924698a41e0f8b0d2b****'),
      mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the media workflow.', example='93ab850b4f6f44eab54b6e91****81d4'),
      name?: string(name='Name', description='The name of the media workflow.', example='example-mediaworkflow-****'),
      runId?: string(name='RunId', description='The ID of the execution instance.', example='48e33690ac19445488c706924321****'),
      state?: string(name='State', description='The status of the media workflow. Valid values:

*   Running: The media workflow is running.
*   Completed: The media workflow is complete.

> Completed only indicates that the media workflow is complete. View the status of each method in the workflow, such as the transcode and snapshot methods, to check whether the method is called.

*   Fail: The media workflow fails.', example='Completed'),
    }
  ](name='MediaWorkflowExecution')
  }(name='MediaWorkflowExecutionList', description='The details of the media workflows.'),
  nonExistRunIds?: {
    runId?: [ string ](name='RunId')
  }(name='NonExistRunIds', description='The IDs of the execution instances that do not exist.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='D1D5C080-8E2F-5030-8AB4-13092F17631B'),
}

model QueryMediaWorkflowExecutionListResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: QueryMediaWorkflowExecutionListResponseBody(name='body'),
}

/**
  * *   You can call this operation to query a maximum of 10 media workflow execution instances at a time.
  * *   Before you call this operation, make sure that the workflow pipeline is enabled. Otherwise, the workflow may not run as expected. For example, the following exceptions may occur: the workflow node is invalid and jobs created in the workflow cannot be executed.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function queryMediaWorkflowExecutionList(request: QueryMediaWorkflowExecutionListRequest): QueryMediaWorkflowExecutionListResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'QueryMediaWorkflowExecutionList', 'POST', '/', 'json', false, 'json', request);
}

model QueryMediaWorkflowListRequest {
  mediaWorkflowIds: string(name='MediaWorkflowIds', description='The IDs of the media workflows that you want to query. To obtain the IDs of the media workflows, you can log on to the **ApsaraVideo Media Processing (MPS) console** and choose **Workflows** > **Workflow Settings** in the left-side navigation pane. You can query up to 10 media workflows at a time. Separate multiple IDs of media workflows with commas (,).', example='93ab850b4f6f44eab54b6e9181d4****,72dfa5e679ab4be9a3ed9974c736****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model QueryMediaWorkflowListResponseBody = {
  mediaWorkflowList?: {
    mediaWorkflow?: [ 
    {
      creationTime?: string(name='CreationTime', description='The time when the media workflow was created.', example='2016-04-01T05:29:38Z'),
      mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the media workflow.', example='93ab850b4f6f44eab54b6e9181d4****'),
      name?: string(name='Name', description='The name of the media workflow.', example='example-mediaworkflow-****'),
      state?: string(name='State', description='The state of the media workflow. Valid values:

*   **Inactive**: The media workflow was deactivated.
*   **Active**: The media workflow was activated.
*   **Deleted**: The media workflow was deleted.', example='Active'),
      topology?: string(name='Topology', description='The topology of the media workflow.', example='{mediaworkflow","State":"Active","Topology":"{\\"Activities\\":{\\"Act-Start\\":{\\"Parameters\\":{\\"PipelineId\\":\\"130266f58161436a80bf07cb12c8****\\",\\"InputFile\\":\\"{\\\\\\"Bucket\\\\\\": \\\\\\"example-bucket-****\\\\\\",\\\\\\"Location\\\\\\": \\\\\\"cn-shanghai\\\\\\"}\\"},\\"Type\\":\\"Start\\"},\\"Act-Report\\":{\\"Parameters\\":{},\\"Type\\":\\"Report\\"},\\"Act-Transcode-M3U8\\":{\\"Parameters\\":{\\"Outputs\\":\\"[{\\\\\\"Object\\\\\\":\\\\\\"transcode/{ObjectPrefix}{FileName}\\\\\\",\\\\\\"TemplateId\\\\\\": \\\\\\"957d1719ee85ed6527b90cf62726****\\\\\\"}]\\",\\"OutputBucket\\":\\"example-bucket-****\\",\\"OutputLocation\\":\\"cn-shanghai\\"},\\"Type\\":\\"Transcode\\"}},\\"Dependencies\\":{\\"Act-Start\\":[\\"Act-Transcode-M3U8\\"],\\"Act-Report\\":[],\\"Act-Transcode-M3U8\\":[\\"Act-Report\\"]}}","MediaWorkflowId":"93ab850b4f6f44eab54b6e91d24d****"}]},"RequestId":"16CD0CDD-457E-420D-9755-8385075A1234"}'),
      triggerMode?: string(name='TriggerMode', description='The trigger mode of the media workflow. Valid values:

*   **OssAutoTrigger**: The media workflow is automatically triggered.
*   **NotInAuto**: The media workflow is not automatically triggered.', example='OssAutoTrigger'),
    }
  ](name='MediaWorkflow')
  }(name='MediaWorkflowList', description='The media workflows.'),
  nonExistMediaWorkflowIds?: {
    mediaWorkflowId?: [ string ](name='MediaWorkflowId')
  }(name='NonExistMediaWorkflowIds', description='The workflows that do not exist.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='16CD0CDD-457E-420D-1234-8385075A618B'),
}

model QueryMediaWorkflowListResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: QueryMediaWorkflowListResponseBody(name='body'),
}

/**
  * You can call this operation to query up to 10 media workflows at a time.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function queryMediaWorkflowList(request: QueryMediaWorkflowListRequest): QueryMediaWorkflowListResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'QueryMediaWorkflowList', 'POST', '/', 'json', false, 'json', request);
}

model QueryPipelineListRequest {
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  pipelineIds: string(name='PipelineIds', description='The IDs of the MPS queues that you want to query. To view the IDs, you can log on to the **MPS console** and choose **Global Settings** > **Pipelines** in the left-side navigation pane. You can query up to 10 MPS queues at a time. Separate multiple IDs of MPS queues with commas (,).', example='d1ce4d3efcb549419193f50f1fcd****,72dfa5e679ab4be9a3ed9974c736****', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model QueryPipelineListResponseBody = {
  nonExistPids?: {
    string?: [ string ](name='String')
  }(name='NonExistPids', description='The IDs of MPS queues that do not exist.'),
  pipelineList?: {
    pipeline?: [ 
    {
      extendConfig?: {
        isBoostNew?: boolean(name='IsBoostNew'),
        maxMultiSpeed?: int32(name='MaxMultiSpeed'),
        multiSpeedDowngradePolicy?: string(name='MultiSpeedDowngradePolicy'),
      }(name='ExtendConfig'),
      id?: string(name='Id', description='The ID of the MPS queue.', example='d1ce4d3efcb549419193f50f1fcd****'),
      name?: string(name='Name', description='The name of the MPS queue.', example='example-pipeline-****'),
      notifyConfig?: {
        mqTag?: string(name='MqTag', description='The tag string.', example='mts-test'),
        mqTopic?: string(name='MqTopic', description='The queue of messages that are received.', example='example1,example2'),
        queueName?: string(name='QueueName', description='The name of the queue that is created in MNS.', example='example-queue-****'),
        topic?: string(name='Topic', description='The name of the topic that is created in MNS.', example='example-topic-****'),
      }(name='NotifyConfig', description='The Message Service (MNS) configuration.'),
      quotaAllocate?: long(name='QuotaAllocate', description='The quota that is allocated to the MPS queue.', example='10'),
      role?: string(name='Role', description='The role that is assigned to the current RAM user.', example='AliyunMTSDefaultRole'),
      speed?: string(name='Speed', description='The type of the MPS queue. Default value: **Standard**. Valid values:

*   **Boost**: MPS queue with transcoding speed boosted
*   **Standard**: standard MPS queue
*   **NarrowBandHDV2**: MPS queue that supports Narrowband HD 2.0
*   **AIVideoCover**: MPS queue for intelligent snapshot capture
*   **AIVideoFPShot**: MPS queue for media fingerprinting
*   **AIVideoCensor**: MPS queue for automated review
*   **AIVideoMCU**: MPS queue for smart tagging
*   **AIVideoSummary**: MPS queue for video synopsis
*   **AIVideoPorn**: MPS queue for pornography detection in videos
*   **AIAudioKWS**: MPS queue for keyword recognition in audio
*   **AIAudioASR**: MPS queue for speech-to-text conversion', example='Standard'),
      speedLevel?: long(name='SpeedLevel', description='The level of the MPS queue.', example='2'),
      state?: string(name='State', description='The state of the MPS queue. Valid values:

*   **Active**: The MPS queue is active.
*   **Paused**: The MPS queue is paused.', example='Paused'),
    }
  ](name='Pipeline')
  }(name='PipelineList', description='The MPS queues.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='1C538EAA-ACAF-5AD8-B091-A72C63007149'),
}

model QueryPipelineListResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: QueryPipelineListResponseBody(name='body'),
}

/**
  * *   You can call this operation to query up to 10 MPS queues at a time.
  * *   If `"Code": "InvalidIdentity.ServiceDisabled","Message": "The request identity was not allowed operated.","Recommend"` is returned after you call this operation, check whether the RAM user that you use is assigned the AliyunMTSDefaultRole role to obtain the permissions on MPS and whether your Alibaba Cloud account has overdue payments.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function queryPipelineList(request: QueryPipelineListRequest): QueryPipelineListResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'QueryPipelineList', 'POST', '/', 'json', false, 'json', request);
}

model QuerySmarttagJobRequest {
  jobId: string(name='JobId', example='39f8e0bc005e4f309379701645f4****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  params?: string(name='Params', example='{"labelResultType":"auto"}', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model QuerySmarttagJobResponseBody = {
  jobStatus?: string(name='JobStatus', example='Success'),
  requestId?: string(name='RequestId', example='7B117AF5-2A16-412C-B127-FA6175ED1AD0'),
  results?: {
    result?: [ 
    {
      data?: string(name='Data', example='{"title":"example-title-****"}'),
      type?: string(name='Type', example='Meta'),
    }
  ](name='Result')
  }(name='Results'),
  userData?: string(name='UserData', example='example UserData ****'),
}

model QuerySmarttagJobResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: QuerySmarttagJobResponseBody(name='body'),
}

async function querySmarttagJob(request: QuerySmarttagJobRequest): QuerySmarttagJobResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'QuerySmarttagJob', 'POST', '/', 'json', false, 'json', request);
}

model QuerySmarttagTemplateListRequest {
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  templateId?: string(name='TemplateId', description='The ID of the template. You can obtain the template ID from the response of the [AddSmarttagTemplate](~~187759~~) operation. If you set this parameter to a specific value, the information about the corresponding template is returned. If you do not specify this parameter, the operation returns the information about all the templates that are created by the current RAM user.', example='05de22f255284c7a8d2aab535dde****', position='Query'),
}

model QuerySmarttagTemplateListResponseBody = {
  requestId?: string(name='RequestId', description='The ID of the request.', example='5210DBB0-E327-4D45-ADBC-0B83C8796E26'),
  templates?: {
    template?: [ 
    {
      analyseTypes?: string(name='AnalyseTypes', description='The analysis types that are used in the template. One or more values are returned. Valid values:

*   **ocr**: text recognition
*   **asr**: speech recognition
*   **classification**: video classification
*   **shows**: program recognition
*   **face**: facial recognition
*   **role**: figure recognition
*   **object**: object recognition
*   **tvstation**: logo recognition
*   **action**: action recognition
*   **emotion**: facial expression recognition
*   **landmark**: landmark recognition
*   **scene**: scene recognition
*   **movieip**: movie intellectual property recognition
*   **subtitle**: subtitle extraction', example='ocr,asr,classification,shows,face,role,object,tvstation,action,emotion,landmark,scene'),
      faceCategoryIds?: string(name='FaceCategoryIds', description='The IDs of the system facial image libraries that are used in the template. One or more values are returned. Valid values:

*   celebrity: the facial image library of celebrities
*   politician: the facial image library of politicians
*   sensitive: the facial image library of sensitive figures', example='politician,sensitive,celebrity'),
      faceCustomParamsConfig?: string(name='FaceCustomParamsConfig', description='The configurations of face-related algorithms. The value of this parameter is a JSON string and consists of the thresholds set for face detection and facial recognition. Valid values:

*   **faceDetThreshold**: The default threshold for face detection is 0.999. The threshold takes effect only for the faces that are strange to the system.
*   **faceRegThreshold**: The default threshold for facial recognition is 0.9.', example='{ "faceDetThreshold":0.999, "faceRegThreshold":0.9 }'),
      industry?: string(name='Industry', description='The industry to which the template applies. Default value: **common**. Valid values:

*   **microVideo**: short video industry
*   **common**: general industries', example='common'),
      isDefault?: boolean(name='IsDefault', description='Indicates whether the template is the default template. Valid values:

*   **true**: The template is the default template.
*   **false**: The template is not the default template.', example='false'),
      keywordConfig?: string(name='KeywordConfig', description='The configuration of keyword tags. The type field specifies the category of a keyword tag. You can specify one or more values and separate the values with commas (,). Valid values:

*   name
*   location
*   organization
*   other

> Keyword tags of all the categories are returned in one of the following scenarios: The KeywordConfig parameter is not specified or the Keyword field is invalid because it is not a JSON string, or the KeywordConfig parameter does not contain the type field or the type field is invalid.', example='{ "type": "name,location,organization,other" }'),
      knowledgeConfig?: string(name='KnowledgeConfig', description='The fields to be identified as knowledge graph information when tags are returned in Smart tagging V2.0 and Smart tagging V2.0-custom modes. For more information, see [Knowledge graph fields in smart tagging jobs](~~356383~~). If this parameter is not specified or the specified value is NULL or invalid because it is not a JSON string, the following fields are returned:

*   movie-related fields:

    *   name: the name of the intellectual property that is featured in the movie
    *   alias: the alias of the intellectual property that is featured in the movie
    *   chnl: the category of the movie
    *   genre: the genre of the movie
    *   country: the country or region in which the movie was produced
    *   language: the language of the movie
    *   releaseYear: the year when the movie was released

*   music-related fields:

    *   songName: the name of the song
    *   artistName: the name of the singer
    *   artistArea: the region to which the singer belongs, such as China, Japan, Korea, Europe, and America, or others.
    *   albumName: the name of the album

*   person-related fields:

    *   name: the name of the person
    *   gender: the gender of the person
    *   citizenship: the nationality of the person
    *   occupation: the occupation of the person
    *   classification: the type into which the person is classified
    *   nationality: the ethnic group of the person
    *   birthPlace: the place where the person was born
    *   birthDate: the date when the person was born

*   landmark-related fields:

    *   name: the display name of the landmark
    *   nameEn: the English name of the landmark
    *   Description: the description of the parameter
    *   address: the address of the landmark

*   item-related fields:

    *   brandName: the brand of the item
    *   finegrainName: the fine-grained description of the item', example='{ "movie":"name,alias,chnl,genre", "music":"songName,artistName", "person":"name,gender" }'),
      labelType?: string(name='LabelType', description='The type of the tagging. Default value: **auto**. Valid values:

*   **auto**: machine tagging
*   **hmi**: tagging by human and machine', example='hmi'),
      labelVersion?: string(name='LabelVersion', description='The version of the smart tagging feature. Default value: 1.0. Valid values:

*   1.0: Smart tagging V1.0
*   2.0: Smart tagging V2.0 (CPV tagging)
*   2.0-custom: Smart tagging V2.0-custom (CPV tagging by using custom models)', example='1.0'),
      landmarkGroupIds?: string(name='LandmarkGroupIds', description='The IDs of the landmark libraries that are used in the template.', example='common'),
      objectGroupIds?: string(name='ObjectGroupIds', description='The IDs of the object libraries that are used in the template.', example='general,item,weapon,animal'),
      scene?: string(name='Scene', description='The scenario in which the template is used. Valid values:

*   **search**: search scenarios
*   **recommend**: recommendation scenarios', example='search'),
      templateId?: string(name='TemplateId', description='The ID of the template.', example='05de22f255284c7a8d2aab535dde****'),
      templateName?: string(name='TemplateName', description='The name of the template.', example='example-****'),
    }
  ](name='Template')
  }(name='Templates', description='The templates.'),
}

model QuerySmarttagTemplateListResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: QuerySmarttagTemplateListResponseBody(name='body'),
}

/**
  * If you call this operation to query the information about a smart tagging template, you must specify the template ID. Otherwise, the operation returns the information about all the templates that are created by the current RAM user.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function querySmarttagTemplateList(request: QuerySmarttagTemplateListRequest): QuerySmarttagTemplateListResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'QuerySmarttagTemplateList', 'POST', '/', 'json', false, 'json', request);
}

model QuerySnapshotJobListRequest {
  endOfJobCreatedTimeRange?: string(name='EndOfJobCreatedTimeRange', description='The snapshot configuration.', example='2014-01-12T12:00:00Z', position='Query'),
  maximumPageSize?: long(name='MaximumPageSize', description='The ID of the MPS queue to which the snapshot jobs that you want to query are submitted. To obtain the ID, you can log on to the **MPS console** and choose **Global Settings** > **Pipelines** in the left-side navigation pane.', example='30', minimum=1, maximum=100, position='Query'),
  nextPageToken?: string(name='NextPageToken', description='The end of the time range within which the creation time of snapshot jobs to be queried is.

*   Specify the time in the ISO 8601 standard in the
*   YYYY-MM-DDThh:mm:ssZ format. The time must be in UTC.', example='cc6cbef8e8d5481ca536f5d2a466****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  pipelineId?: string(name='PipelineId', description='The start time for taking snapshots. Unit: milliseconds.', example='b11c171cced04565b1f38f1ecc39****', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  snapshotJobIds?: string(name='SnapshotJobIds', description='The beginning of the time range within which the creation time of snapshot jobs to be queried is.

*   Specify the time in the ISO 8601 standard in the
*   YYYY-MM-DDThh:mm:ssZ format. The time must be in UTC.', example='72dfa5e679ab4be9a3ed9974c736****', position='Query'),
  startOfJobCreatedTimeRange?: string(name='StartOfJobCreatedTimeRange', description='The time when the job was created.', example='2014-01-10T12:00:00Z', position='Query'),
  state?: string(name='State', description='The information about the snapshot jobs.', example='Snapshoting', position='Query'),
}

model QuerySnapshotJobListResponseBody = {
  nextPageToken?: string(name='NextPageToken', description='The OSS object that is used as the input file.', example='b11c171cced04565b1f38f1ecc39****'),
  nonExistSnapshotJobIds?: {
    string?: [ string ](name='String')
  }(name='NonExistSnapshotJobIds', description='The OSS object that is generated as the output file of the tiling job.'),
  requestId?: string(name='RequestId', description='The ID of the snapshot job.', example='34BCAB31-2833-43A7-9FBD-B34302AB23EQ'),
  snapshotJobList?: {
    snapshotJob?: [ 
    {
      code?: string(name='Code', description='You can call this operation to query up to 10 snapshot jobs at a time.


## Limits on QPS

You can call this operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).', example='InvalidParameter'),
      count?: string(name='Count', description='The ID of the request.', example='1'),
      creationTime?: string(name='CreationTime', description='The stride of a single image.', example='2021-06-30T12:34:29Z'),
      id?: string(name='Id', description='The OSS output file of the tiling job.', example='cc6cbef8e8d5481ca536f5d2a466****'),
      input?: {
        bucket?: string(name='Bucket', description='The ID of the snapshot job.', example='example'),
        location?: string(name='Location', description='The ID of the MPS queue to which the snapshot job was submitted.', example='example-location'),
        object?: string(name='Object', description='The error code returned when the job fails. This parameter is not returned if the job is successfully processed.', example='example.flv'),
        roleArn?: string(name='RoleArn', description='The custom data.', example='acs:ram::1:role/testrole'),
      }(name='Input', description='The number of snapshots that are contained in the tiled image.'),
      MNSMessageResult?: {
        errorCode?: string(name='ErrorCode', description='The number of snapshots that were taken.', example='InvalidParameter'),
        errorMessage?: string(name='ErrorMessage', description='The OSS bucket that stores the input file.', example='The resource operated InputFile is bad'),
        messageId?: string(name='MessageId', description='The ID of the region in which the input OSS bucket is located.', example='799454621135656C7F815F198A76****'),
      }(name='MNSMessageResult', description='The OSS object that is used as the input file.'),
      message?: string(name='Message', description='The ARN of the specified RAM role. Format: acs:ram::$accountID:role/$roleName.', example='The resource operated InputFile is bad'),
      pipelineId?: string(name='PipelineId', description='The start time for taking snapshots. Unit: milliseconds.', example='b11c171cced04565b1f38f1ecc39****'),
      snapshotConfig?: {
        frameType?: string(name='FrameType', description='The ID of the region in which the output OSS bucket is located.', example='intra'),
        height?: string(name='Height', description='The number of snapshots to take. If the Num parameter is set in the request, snapshots are taken at intervals.', example='8'),
        interval?: string(name='Interval', description='The OSS object that is generated as the output file of the snapshot job.', example='10'),
        num?: string(name='Num', description='The status of the snapshot job. 

- **Submitted**: The job was submitted.
- **Snapshoting**: The job is being processed.
- **Success**: The job was successfully processed.
- **Fail**: The job failed.', example='10'),
        outputFile?: {
          bucket?: string(name='Bucket', description='The OSS bucket that stores the output file.', example='example'),
          location?: string(name='Location', description='The ID of the region in which the output OSS bucket is located.', example='example-location'),
          object?: string(name='Object', description='The OSS object that is generated as the output file of the tiling job.', example='example.png'),
          roleArn?: string(name='RoleArn', description='The ARN of the specified RAM role. Format: acs:ram::$accountID:role/$roleName.', example='acs:ram::1:role/testrole'),
        }(name='OutputFile', description='The OSS output file of the tiling job.'),
        tileOut?: {
          cellHeight?: string(name='CellHeight', description='The interval for taking snapshots.

*   If this Interval parameter is specified in the request, snapshots are taken at intervals. The value must be greater than 0.
*   Unit: seconds.
*   Default value: **10**.', example='8'),
          cellSelStep?: string(name='CellSelStep', description='The number of rows that the tiled image can contain. Default value: **10**.', example='3'),
          cellWidth?: string(name='CellWidth', description='The type of the snapshot. Valid values:

*   **normal**: normal frames.
*   **intra**: I-frames.
*   Default value: **intra**.', example='8'),
          color?: string(name='Color', description='Indicates whether the single images are retained. Default value: **true**.', example='black'),
          columns?: string(name='Columns', description='The height of the output snapshot.', example='10'),
          isKeepCellPic?: string(name='IsKeepCellPic', description='The Object Storage Service (OSS) output file of the snapshot job.', example='false'),
          lines?: string(name='Lines', description='The Alibaba Cloud Resource Name (ARN) of the specified RAM role. Format: acs:ram::$accountID:role/$roleName.', example='10'),
          margin?: string(name='Margin', description='The width of the output snapshot.', example='0'),
          padding?: string(name='Padding', description='The number of columns that the tiled image can contain. Default value: **10**.', example='0'),
        }(name='TileOut', description='The margin width of the tiled image.

*   Default value: **0**.
*   Unit: pixel.'),
        tileOutputFile?: {
          bucket?: string(name='Bucket', description='The error code returned when the job fails. This parameter is not returned if the job is successfully processed.', example='example'),
          location?: string(name='Location', description='The error message returned when the job fails. This parameter is not returned if the job is successfully processed.', example='example-location'),
          object?: string(name='Object', description='The ID of the message. This parameter is not returned if the job fails.', example='example.png'),
          roleArn?: string(name='RoleArn', description='The message sent by MNS to notify the user of the job result.', example='acs:ram::1:role/testrole'),
        }(name='TileOutputFile', description='The error message returned when the job fails. This parameter is not returned if the job is successfully processed.'),
        time?: string(name='Time', description='The width of a single image. The default value is the width of the output snapshot.', example='4'),
        timeArray?: {
          timePointList?: [ long ](name='TimePointList')
        }(name='TimeArray'),
        width?: string(name='Width', description='The OSS bucket that stores the output file.', example='8'),
      }(name='SnapshotConfig', description='The height of a single image. The default value is the height of the output snapshot.'),
      state?: string(name='State', description='The information about the job input.', example='Snapshoting'),
      tileCount?: string(name='TileCount', description='The snapshot job IDs that do not exist. This parameter is not returned if all specified snapshot jobs are found.', example='7'),
      userData?: string(name='UserData', description='The token that is used to retrieve the next page of the query results. The value is a 32-bit UUID. If the returned query results cannot be displayed within one page, this parameter is returned. The value of this parameter is updated for each query.', example='testid-001'),
    }
  ](name='SnapshotJob')
  }(name='SnapshotJobList', description='The distance between images.

*   Default value: **0**.
*   Unit: pixel.'),
}

model QuerySnapshotJobListResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: QuerySnapshotJobListResponseBody(name='body'),
}

/**
  * The status of the snapshot jobs that you want to query.
  * *   **Submitted**: The job was submitted.
  * *   **Snapshoting**: The job is being processed.
  * *   **Success**: The job was successfully processed.
  * *   **Fail**: The job failed.
  *
 */
async function querySnapshotJobList(request: QuerySnapshotJobListRequest): QuerySnapshotJobListResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'QuerySnapshotJobList', 'POST', '/', 'json', false, 'json', request);
}

model QueryTemplateListRequest {
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  templateIds: string(name='TemplateIds', description='The response parameters.', example='16f01ad6175e4230ac42bb5182cd****,88c6ca184c0e424d5w5b665e2a12****', position='Query'),
}

model QueryTemplateListResponseBody = {
  nonExistTids?: {
    string?: [ string ](name='String')
  }(name='NonExistTids', description='The IDs of the templates that do not exist. This parameter is not returned if all specified transcoding templates are found.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='BC860F04-778A-472F-AB39-E1BF329C1EA8'),
  templateList?: {
    template?: [ 
    {
      audio?: {
        bitrate?: string(name='Bitrate', description='The audio bitrate of the output file.

*   Valid values: **\\[8,1000]**.
*   Unit: Kbit/s.
*   Default value: **128**.', example='500'),
        channels?: string(name='Channels', description='The number of sound channels. Default value: **2**.', example='2'),
        codec?: string(name='Codec', description='The audio codec format. Default value: **aac**. Valid values:

*   **aac**
*   **mp3**
*   **vorbis**
*   **flac**', example='aac'),
        profile?: string(name='Profile', description='The codec profile of the audio. Valid values when the value of Codec is aac:

*   **aac_low**
*   **aac_he**
*   **aac_he_v2**
*   **aac_ld**
*   **aac_eld**', example='aac_low'),
        qscale?: string(name='Qscale', description='The strength of the independent denoising algorithm.', example='1'),
        remove?: string(name='Remove', description='Indicates whether the audio stream is deleted. Valid values:

*   **true**
*   **false**
*   Default value: **false**.', example='false'),
        samplerate?: string(name='Samplerate', description='The sampling rate.

*   Unit: Hz
*   Default value: **44100**.', example='44100'),
        volume?: {
          integratedLoudnessTarget?: string(name='IntegratedLoudnessTarget', description='The output volume.

*   This parameter takes effect only if the value of Method is dynamic.
*   Unit: dB.
*   Valid values: \\[-70,-5].
*   Default value: -6.', example='-6'),
          level?: string(name='Level', description='The increased volume relative to the volume of the input audio.

*   This parameter takes effect only if the value of Method is linear.
*   Unit: dB.
*   Valid values: less than or equal to 20.
*   Default value: -20.', example='-20'),
          loudnessRangeTarget?: string(name='LoudnessRangeTarget', description='The range of the volume relative to the output volume.

*   This parameter takes effect only if the value of Method is dynamic.
*   Unit: dB.
*   Valid values: \\[1,20].
*   Default value: 8.', example='8'),
          method?: string(name='Method', description='The method that is used to adjust the volume. Valid values:

*   **auto**
*   **dynamic**
*   **linear**
*   Default value: dynamic.', example='auto'),
          peakLevel?: string(name='PeakLevel', description='The volume adjustment coefficient.

This parameter takes effect only if the value of Method is adaptive.

Valid values: \\[0,1].

Default value: 0.9.', example='0.9'),
          truePeak?: string(name='TruePeak', description='The peak volume.

*   This parameter takes effect only if the value of Method is dynamic.
*   Unit: dB.
*   Valid values: \\[-9,0].
*   Default value: -1.', example='-1'),
        }(name='Volume', description='The volume control configurations.'),
      }(name='Audio', description='The audio codec configurations.'),
      container?: {
        format?: string(name='Format', description='The format of the container. Valid values: flv, mp4, ts, m3u8, gif, mp3, ogg, and flac.', example='flv'),
      }(name='Container', description='The container format configurations.'),
      creationTime?: string(name='CreationTime', description='The time when the template was created.', example='2021-03-04T06:44:43Z'),
      id?: string(name='Id', description='The transcoding template ID.', example='16f01ad6175e4230ac42bb5182cd****'),
      muxConfig?: {
        gif?: {
          ditherMode?: string(name='DitherMode', description='The color dithering algorithm of the palette. Valid values: sierra and bayer.', example='sierra'),
          finalDelay?: string(name='FinalDelay', description='The duration for which the final frame is paused.', example='0'),
          isCustomPalette?: string(name='IsCustomPalette', description='Indicates whether a custom palette is used.', example='false'),
          loop?: string(name='Loop', description='The loop count.', example='0'),
        }(name='Gif', description='The transmuxing configurations for GIF.'),
        segment?: {
          duration?: string(name='Duration', description='The length of the segment. Unit: seconds.', example='10'),
        }(name='Segment', description='The segment configurations.'),
        webp?: {
          loop?: string(name='Loop', description='The loop count.', example='0'),
        }(name='Webp', description='The transmuxing configurations for WebP.'),
      }(name='MuxConfig', description='The transmuxing configurations.'),
      name?: string(name='Name', description='The name of the template.', example='MPS-example'),
      state?: string(name='State', description='The status of the template. Valid values:

*   **Normal**
*   **Deleted**', example='Normal'),
      transConfig?: {
        adjDarMethod?: string(name='AdjDarMethod', description='The method of resolution adjustment. Default value: **none**. Valid values:

*   rescale
*   crop
*   none', example='none'),
        isCheckAudioBitrate?: string(name='IsCheckAudioBitrate', description='Indicates whether the audio bitrate is checked. If the bitrate of the output audio is higher than that of the input audio, the input bitrate is retained and the specified audio bitrate does not take effect. This parameter has a lower priority than IsCheckAudioBitrateFail. Valid values:

*   **true**

*   **false**

*   Default value:

    *   If this parameter is not specified and the codec of the output audio is different from that of the input audio, the default value is false.
    *   If this parameter is not specified and the codec of the output audio is the same as that of the input audio, the default value is true.', example='false'),
        isCheckAudioBitrateFail?: string(name='IsCheckAudioBitrateFail', description='Indicates whether audio bitrate check errors are allowed. This parameter has a greater priority than IsCheckAudioBitrate. Valid values:

*   **true**: If the audio bitrate check fails, the input file is not transcoded.
*   **false**: The audio bitrate is not checked.
*   Default value: **false**.', example='false'),
        isCheckReso?: string(name='IsCheckReso', description='Indicates whether the resolution is checked. If the output resolution is higher than the input resolution based on the width or height, the input resolution is retained. Valid values:

*   **true**
*   **false**
*   Default value: **false**.', example='false'),
        isCheckResoFail?: string(name='IsCheckResoFail', description='Indicates whether the resolution is checked. If the output resolution is higher than the input resolution based on the width or height, a transcoding failure is returned. Valid values:

*   **true**
*   **false**
*   Default value: **false**.', example='false'),
        isCheckVideoBitrate?: string(name='IsCheckVideoBitrate', description='Indicates whether the video bitrate is checked. If the bitrate of the output video is higher than that of the input video, the input bitrate is retained. Valid values:

*   **true**
*   **false**
*   Default value: **false**.', example='false'),
        isCheckVideoBitrateFail?: string(name='IsCheckVideoBitrateFail', description='Indicates whether video bitrate check errors are allowed. This parameter has a higher priority than IsCheckVideoBitrate. Valid values:

*   **true**: If the video bitrate check fails, the input file is not transcoded.
*   **false**: The video bitrate is not checked.
*   Default value: **false**.', example='false'),
        transMode?: string(name='TransMode', description='The transcoding mode. Default value: **onepass**. Valid values:

*   **onepass**
*   **twopass**
*   **CBR**', example='onepass'),
      }(name='TransConfig', description='The general transcoding configurations.'),
      video?: {
        bitrate?: string(name='Bitrate', description='The average bitrate of the video. Unit: Kbit/s.', example='200'),
        bitrateBnd?: {
          max?: string(name='Max', description='The upper limit of the total bitrate. Unit: Kbit/s.', example='100'),
          min?: string(name='Min', description='The lower limit of the total bitrate. Unit: Kbit/s.', example='500'),
        }(name='BitrateBnd', description='The average bitrate range of the video.'),
        bufsize?: string(name='Bufsize', description='The buffer size.

*   Unit: KB.
*   Default value: **6000**.', example='6000'),
        codec?: string(name='Codec', description='The codec. Default value: **H.264**.', example='H.264'),
        crf?: string(name='Crf', description='The constant rate factor.

*   Default value when the encoding format is H.264: **23**. Default value when the encoding format is H.265: **26**.
*   If this parameter is set, the value of Bitrate becomes invalid.', example='15'),
        crop?: string(name='Crop', description='The method of video cropping. Valid values:

*   **border**: automatically detects and removes black bars.
*   **Value in the width:height:left:top format**: crops the video based on custom settings. Format: width:height:left:top. Example: 1280:800:0:140.', example='border'),
        degrain?: string(name='Degrain', description='The level of video quality control.', example='10'),
        fps?: string(name='Fps', description='The frame rate.

*   The value is 60 if the frame rate of the input file exceeds 60.
*   Default value: **the frame rate of the input file**.', example='25'),
        gop?: string(name='Gop', description='The maximum number of frames between two keyframes. Default value: **250**.', example='10'),
        hdr2sdr?: string(name='Hdr2sdr', description='Indicates whether the HDR2SDR conversion feature is enabled. If this feature is enabled, high dynamic range (HDR) videos are transcoded to standard dynamic range (SDR) videos.', example='true'),
        height?: string(name='Height', description='The height of the video.

*   Unit: pixel.
*   Default value: **the height of the input video**.', example='800'),
        longShortMode?: string(name='LongShortMode', description='Indicates whether the auto-rotate screen feature is enabled.

*   If this feature is enabled, the width of the output video corresponds to the long side of the input video, which is the height of the input video in portrait mode. The height of the output video corresponds to the short side of the input video, which is the width of the input video in portrait mode. Valid values:
*   **true**
*   **false**
*   Default value: **false**.', example='false'),
        maxFps?: string(name='MaxFps', description='The maximum frame rate.', example='60'),
        maxrate?: string(name='Maxrate', description='The maximum bitrate of the video. Unit: Kbit/s.', example='500'),
        narrowBand?: {
          abrmax?: float(name='Abrmax', description='The upper limit of the dynamic bitrate. If this parameter is set, the average bitrate is in the range of (0, 1000000].', example='3000'),
          maxAbrRatio?: float(name='MaxAbrRatio', description='The maximum ratio of the upper limit of dynamic bitrate. If this parameter is set, the value of Abrmax does not exceed x times of the source video bitrate. Valid values: (0,1.0].', example='1.0'),
          version?: string(name='Version', description='The Narrowband HD version. Only 1.0 may be returned.', example='1.0'),
        }(name='NarrowBand', description='The Narrowband HD settings.'),
        pad?: string(name='Pad', description='The black bars that are added to the video. Format: width:height:left:top. Example: 1280:800:0:140.', example='1280:800:0:140'),
        pixFmt?: string(name='PixFmt', description='The pixel format of the video. Valid values: standard pixel formats such as yuv420p and yuvj420p.', example='yuv420p'),
        preset?: string(name='Preset', description='The preset video algorithm. Valid values: veryfast, fast, medium, slow, and slower. Default value: **medium**.', example='medium'),
        profile?: string(name='Profile', description='The encoding profile. Valid values:

*   **baseline**: applicable to mobile devices.
*   **main**: applicable to standard-definition devices.
*   **high**: applicable to high-definition devices.
*   Default value: **high**.', example='high'),
        qscale?: string(name='Qscale', description='The strength of the independent denoising algorithm.', example='1'),
        remove?: string(name='Remove', description='Indicates whether the video stream is deleted. Valid values:

*   **true**
*   **false**
*   Default value: **false**.', example='false'),
        resoPriority?: string(name='ResoPriority', description='The policy of resolution adjustment. Valid values: cropFirst, widthFirst, and heightFirst.', example='heightFirst'),
        scanMode?: string(name='ScanMode', description='The scan mode. Valid values:

*   **interlaced**
*   **progressive**', example='interlaced'),
        width?: string(name='Width', description='The width of the video.

*   Unit: pixel.
*   Default value: **the width of the input video**.', example='256'),
      }(name='Video', description='The video codec configurations.'),
    }
  ](name='Template')
  }(name='TemplateList', description='The transcoding templates.'),
}

model QueryTemplateListResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: QueryTemplateListResponseBody(name='body'),
}

/**
  * The IDs of the transcoding templates that you want to query. You can query up to 10 transcoding templates at a time. Separate multiple IDs of custom transcoding templates with commas (,).
  *
 */
async function queryTemplateList(request: QueryTemplateListRequest): QueryTemplateListResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'QueryTemplateList', 'POST', '/', 'json', false, 'json', request);
}

model QueryTraceAbJobRequest {
  jobId?: string(name='JobId', example='31fa3c9ca8134fb4b0b0f7878301****', position='Query'),
  mediaId?: string(name='MediaId', example='3e6149d5a8c944c09b1a8d2dc3e4****', position='Query'),
}

model QueryTraceAbJobResponseBody = {
  data?: [ 
    {
      callback?: string(name='Callback', example='http://callbacktest.com/callback'),
      gmtCreate?: long(name='GmtCreate', example='1627357322'),
      gmtModified?: long(name='GmtModified', example='1627357325'),
      input?: string(name='Input', example='{"Bucket":"ivison-test","Location":"oss-cn-shanghai","Object":"test.mp4"}'),
      jobId?: string(name='JobId', example='bfb786c639894f4d80648792021e****'),
      level?: long(name='Level', example='2'),
      mediaId?: string(name='MediaId', example='437bd2b516ffda105d07b12a9a82****'),
      output?: string(name='Output', example='{"Bucket":"ivison-test","Location":"oss-cn-shanghai","Object":"out.mp4"}'),
      result?: string(name='Result', example='{"Code":"success","Message":"ok"}'),
      status?: string(name='Status', example='success'),
      userData?: string(name='UserData', example='123'),
      userId?: long(name='UserId', example='13466****'),
    }
  ](name='Data'),
  message?: string(name='Message', example='ok'),
  requestId?: string(name='RequestId', description='Id of the request', example='338CA33A-AE83-5DF4-B6F2-C6D3ED8143F5'),
  statusCode?: long(name='StatusCode', example='200'),
}

model QueryTraceAbJobResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: QueryTraceAbJobResponseBody(name='body'),
}

async function queryTraceAbJob(request: QueryTraceAbJobRequest): QueryTraceAbJobResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'QueryTraceAbJob', 'POST', '/', 'json', false, 'json', request);
}

model QueryTraceExtractJobRequest {
  jobId: string(name='JobId', example='31fa3c9ca8134fb4b0b0f7878301****', position='Query'),
}

model QueryTraceExtractJobResponseBody = {
  data?: {
    trace?: string(name='Trace', example='example water mark'),
  }(name='Data'),
  message?: string(name='Message', example='ok'),
  requestId?: string(name='RequestId', description='Id of the request', example='7D9F3008-9316-5817-BFA3-6180D752039D'),
  statusCode?: long(name='StatusCode', example='200'),
}

model QueryTraceExtractJobResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: QueryTraceExtractJobResponseBody(name='body'),
}

async function queryTraceExtractJob(request: QueryTraceExtractJobRequest): QueryTraceExtractJobResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'QueryTraceExtractJob', 'POST', '/', 'json', false, 'json', request);
}

model QueryTraceM3u8JobRequest {
  createTimeEnd?: long(name='CreateTimeEnd', example='1527441303', position='Query'),
  createTimeStart?: long(name='CreateTimeStart', example='1527441300', position='Query'),
  jobId?: string(name='JobId', example='88c6ca184c0e47098a5b665e2a12****', position='Query'),
  pageNumber?: long(name='PageNumber', example='1', position='Query'),
  pageSize?: long(name='PageSize', example='10', position='Query'),
}

model QueryTraceM3u8JobResponseBody = {
  data?: [ 
    {
      gmtCreate?: long(name='GmtCreate', example='1627357322'),
      gmtModified?: long(name='GmtModified', example='1627357327'),
      jobId?: string(name='JobId', example='88c6ca184c0e47098a5b665e2a12****'),
      mediaId?: string(name='MediaId', example='437bd2b516ffda105d07b12a9a82****'),
      output?: string(name='Output', example='{"Bucket":"ivison-test","Location":"oss-cn-shanghai","Object":"out.mp4"}'),
      status?: string(name='Status', example='success'),
      trace?: string(name='Trace', example='平头哥半导体(上海)'),
      userData?: string(name='UserData', example='123'),
      userId?: long(name='UserId', example='13466****'),
    }
  ](name='Data'),
  message?: string(name='Message', example='ok'),
  requestId?: string(name='RequestId', description='Id of the request', example='5CA6E020-4102-4FFF-AA56-5ED7ECD8****'),
  statusCode?: long(name='StatusCode', example='200'),
}

model QueryTraceM3u8JobResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: QueryTraceM3u8JobResponseBody(name='body'),
}

async function queryTraceM3u8Job(request: QueryTraceM3u8JobRequest): QueryTraceM3u8JobResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'QueryTraceM3u8Job', 'POST', '/', 'json', false, 'json', request);
}

model QueryWaterMarkTemplateListRequest {
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  waterMarkTemplateIds: string(name='WaterMarkTemplateIds', description='The IDs of the watermark templates that you want to query. To obtain the IDs of the watermark templates, you can log on to the **ApsaraVideo Media Processing (MPS) console** and choose **Global Settings** > **Watermark Templates** in the left-side navigation pane. You can query up to 10 watermark templates at a time. Separate multiple IDs of watermark templates with commas (,).', example='3780bd69b2b74540bc7b1096f564****', position='Query'),
}

model QueryWaterMarkTemplateListResponseBody = {
  nonExistWids?: {
    string?: [ string ](name='String')
  }(name='NonExistWids', description='The IDs of the templates that do not exist.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='17079AF5-6276-51A9-B755-D26594C93F3C'),
  waterMarkTemplateList?: {
    waterMarkTemplate?: [ 
    {
      dx?: string(name='Dx', description='The horizontal offset. Unit: pixel.', example='100'),
      dy?: string(name='Dy', description='The vertical offset. Unit: pixel.', example='100'),
      height?: string(name='Height', description='The height of the watermark image. Unit: pixel.', example='8'),
      id?: string(name='Id', description='The ID of the watermark template.', example='3780bd69b2b74540bc7b1096f564****'),
      name?: string(name='Name', description='The name of the watermark template.', example='example-watermark'),
      ratioRefer?: {
        dx?: string(name='Dx', description='The horizontal offset of the watermark relative to the output video image. Default value: **0**. The default value indicates no offset. The value can be an integer or a decimal.

*   **Integer**: the vertical offset. This indicates the absolute position. Unit: pixel.
*   **Decimal**: the ratio of the horizontal offset to the width of the output video. The ratio varies based on the size of the video. Four decimal places are supported, such as 0.9999. More decimal places are discarded.', example='0.51'),
        dy?: string(name='Dy', description='The vertical offset of the watermark relative to the output video image. Default value: **0**. The default value indicates no offset. The value can be an integer or a decimal.

*   **Integer**: the vertical offset. This indicates the absolute position. Unit: pixel.
*   **Decimal**: the ratio of the vertical offset to the height of the output video. The ratio varies based on the size of the video. Four decimal places are supported, such as 0.9999. More decimal places are discarded.', example='0.4'),
        height?: string(name='Height', description='The height of the watermark image in the output video. The value can be an integer or a decimal.

*   **Integer**: the height of the watermark image. This indicates the absolute position. Unit: pixel.
*   **Decimal**: the ratio of the height of the watermark image to the height of the output video. The ratio varies based on the size of the video. Four decimal places are supported, such as 0.9999. More decimal places are discarded.', example='0.33'),
        width?: string(name='Width', description='The width of the watermark image in the output video. The value can be an integer or a decimal.

*   **Integer**: the width of the watermark image. This indicates the absolute position. Unit: pixel.
*   **Decimal**: the ratio of the width of the watermark image to the width of the output video. The ratio varies based on the size of the video. Four decimal places are supported, such as 0.9999. More decimal places are discarded.', example='0.36'),
      }(name='RatioRefer', description='The values of the Height, Width, Dx, and Dy parameters relative to the reference edges. If the values of the Height, Width, Dx, and Dy parameters are decimals between 0 and 1, the values are calculated by referring to the following edges in sequence:

*   **Width**: the width edge.
*   **Height**: the height edge.
*   **Long**: the long edge.
*   **Short**: the short edge.'),
      referPos?: string(name='ReferPos', description='The position of the watermark. Valid values:

*   **TopRight**: the upper-right corner.
*   **TopLeft**: the upper-left corner.
*   **BottomRight**: the lower-right corner.
*   **BottomLeft**: the lower-left corner.', example='TopRight'),
      state?: string(name='State', description='The status of the watermark template. Valid values: Valid values:

*   **Normal**: The watermark template is normal.
*   **Deleted**: The watermark template is deleted.', example='Normal'),
      timeline?: {
        duration?: string(name='Duration', description='The display duration of the watermark. Default value: **ToEND**. The default value indicates that the watermark is displayed until the video ends.', example='10'),
        start?: string(name='Start', description='The beginning of the time range during which the watermark is displayed.

*   Unit: seconds.
*   Default value: **0**.', example='0'),
      }(name='Timeline', description='The timeline of the watermark.'),
      type?: string(name='Type', description='The type of the watermark. Valid values:

*   Image: an image watermark.
*   Text: a text watermark.

> Only watermarks of the **Image** type are supported.', example='Image'),
      width?: string(name='Width', description='The width of the watermark image. Unit: pixel.', example='8'),
    }
  ](name='WaterMarkTemplate')
  }(name='WaterMarkTemplateList', description='The details of the watermark templates.'),
}

model QueryWaterMarkTemplateListResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: QueryWaterMarkTemplateListResponseBody(name='body'),
}

/**
  * You can call this operation to query up to 10 watermark templates at a time.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function queryWaterMarkTemplateList(request: QueryWaterMarkTemplateListRequest): QueryWaterMarkTemplateListResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'QueryWaterMarkTemplateList', 'POST', '/', 'json', false, 'json', request);
}

model RegisterCustomFaceRequest {
  categoryId: string(name='CategoryId', description='The ID of the figure library in which you want to register a custom face. The ID is used to uniquely identify a figure library. You can specify the ID of a custom figure library. Make sure that the ID is unique and keep the ID for future API operation calls. If you set this parameter to the ID of a system figure library, the custom face is registered in the system figure library. The ID can be up to 120 characters in length and is not case-sensitive.', example='CategoryId001-****', position='Query'),
  imageUrl: string(name='ImageUrl', description='The URL of the facial image that you want to register for the specified figure. The image must contain only one face.', example='http://example-****.jpeg', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  personId: string(name='PersonId', description='The ID of the figure for which you want to register a custom face. The ID is used to uniquely identify a figure. You can specify a figure ID. Make sure that the ID is unique and keep the ID for future API operation calls. The ID can be up to 120 characters in length and is not case-sensitive. The value returned is of the String type.', example='PersonId001-****', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model RegisterCustomFaceResponseBody = {
  faceId?: string(name='FaceId', description='The ID of the face.', example='c6cc71cb44a9491093818faf9d60****'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='91AEA76D-25B5-50DF-9126-AA6BB10FDAF4'),
}

model RegisterCustomFaceResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: RegisterCustomFaceResponseBody(name='body'),
}

/**
  * *   You can call this operation to register only one custom face at a time.
  * *   A maximum of 10 images can be registered for a custom face.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function registerCustomFace(request: RegisterCustomFaceRequest): RegisterCustomFaceResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'RegisterCustomFace', 'POST', '/', 'json', false, 'json', request);
}

model RegisterCustomViewRequest {
  algorithm: string(name='Algorithm', example='landmark', position='Query'),
  customEntityId: string(name='CustomEntityId', example='2', position='Query'),
  customGroupId: string(name='CustomGroupId', example='1', position='Query'),
  imageUrl: string(name='ImageUrl', example='http://127.66.**.**/image.jpeg', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model RegisterCustomViewResponseBody = {
  customViewId?: string(name='CustomViewId', example='1'),
  requestId?: string(name='RequestId', example='580e8ce3-3b80-44c5-9f3f-36ac3cc5bdd5'),
}

model RegisterCustomViewResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: RegisterCustomViewResponseBody(name='body'),
}

async function registerCustomView(request: RegisterCustomViewRequest): RegisterCustomViewResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'RegisterCustomView', 'POST', '/', 'json', false, 'json', request);
}

model SearchMediaWorkflowRequest {
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  pageNumber?: long(name='PageNumber', description='The number of the page to return. Default value: **1**.', example='1', position='Query'),
  pageSize?: long(name='PageSize', description='The number of entries to return on each page.

*   A maximum of **100** entries can be returned on each page.
*   Default value: **10**.', example='10', minimum=1, maximum=100, position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  stateList?: string(name='StateList', description='The status of the media workflows that you want to query. You can specify multiple states. Separate multiple states with commas (,). Default value: **Inactive,Active,Deleted**. Valid values:

*   **Inactive**: Deactivated media workflows are queried.
*   **Active**: Activated media workflows are queried.
*   **Deleted**: Deleted media workflows are queried.', example='Inactive,Active,Deleted', position='Query'),
}

model SearchMediaWorkflowResponseBody = {
  mediaWorkflowList?: {
    mediaWorkflow?: [ 
    {
      creationTime?: string(name='CreationTime', description='The time when the media workflow was created.', example='2016-04-01T05:38:41Z'),
      mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the media workflow.', example='88c6ca184c0e4578645b665e2a12****'),
      name?: string(name='Name', description='The name of the media workflow.', example='example-mediaworkflow-****'),
      state?: string(name='State', description='The status of the media workflow. Valid values:

*   **Inactive**: The media workflow is deactivated.
*   **Active**: The media workflow is activated.
*   **Deleted**: The media workflow is deleted.', example='Active'),
      topology?: string(name='Topology', description='The topology of the media workflow.', example='{"MediaWorkflowList":{"MediaWorkflow":[{"CreationTime":"2016-04-01T05:29:38Z","Name":"example-mediaworkflow-****","State":"Active","Topology":"{\\"Activities\\":{\\"Act-Start\\":{\\"Parameters\\":{\\"PipelineId\\":\\"130266f58161436a80bf07cb12c8****\\",\\"InputFile\\":\\"{\\\\\\"Bucket\\\\\\": \\\\\\"example-bucket-****\\\\\\",\\\\\\"Location\\\\\\": \\\\\\"example-location\\\\\\"}\\"},\\"Type\\":\\"Start\\"},\\"Act-Report\\":{\\"Parameters\\":{},\\"Type\\":\\"Report\\"},\\"Act-Transcode-M3U8\\":{\\"Parameters\\":{\\"Outputs\\":\\"[{\\\\\\"Object\\\\\\":\\\\\\"transcode/{ObjectPrefix}{FileName}\\\\\\",\\\\\\"TemplateId\\\\\\": \\\\\\"957d1719ee85ed6527b90cf62726****\\\\\\"}]\\",\\"OutputBucket\\":\\"example-bucket-****\\",\\"OutputLocation\\":\\"example-location\\"},\\"Type\\":\\"Transcode\\"}},\\"Dependencies\\":{\\"Act-Start\\":[\\"Act-Transcode-M3U8\\"],\\"Act-Report\\":[],\\"Act-Transcode-M3U8\\":[\\"Act-Report\\"]}}","MediaWorkflowId":"93ab850b4f6f44eab54b6e91d24d****"}]},"RequestId":"16CD0CDD-457E-420D-9755-8385075A1234"}'),
      triggerMode?: string(name='TriggerMode', description='The trigger mode of the media workflow. Valid values:

*   **OssAutoTrigger**: The media workflow is automatically triggered.
*   **NotInAuto**: The media workflow is not automatically triggered.', example='OssAutoTrigger'),
    }
  ](name='MediaWorkflow')
  }(name='MediaWorkflowList', description='The details of the media workflows.'),
  pageNumber?: long(name='PageNumber', description='The page number of the returned page.', example='1'),
  pageSize?: long(name='PageSize', description='The number of entries returned on each page.', example='10'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='16CD0CDD-457E-420D-9755-8385075A1234'),
  totalCount?: long(name='TotalCount', description='The total number of entries returned.', example='1'),
}

model SearchMediaWorkflowResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: SearchMediaWorkflowResponseBody(name='body'),
}

/**
  * You can call this operation to query media workflows in the specified state. If you do not specify the state, all media workflows are queried by default.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function searchMediaWorkflow(request: SearchMediaWorkflowRequest): SearchMediaWorkflowResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'SearchMediaWorkflow', 'POST', '/', 'json', false, 'json', request);
}

model SearchPipelineRequest {
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  pageNumber?: long(name='PageNumber', description='The number of the page to return. Default value: **1**.', example='1', position='Query'),
  pageSize?: long(name='PageSize', description='The number of entries to return on each page.

*   A maximum of **100** entries can be returned on each page.
*   Default value: **10**.', example='10', minimum=1, maximum=100, position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  state?: string(name='State', description='The status of the MPS queues that you want to query. If you leave this parameter empty, all MPS queues are queried.

*   **All**: queries all MPS queues.
*   **Active**: queries the MPS queues that are active.
*   **Paused**: queues the MPS queues that are paused.
*   Default value: **All**.', example='Paused', position='Query'),
}

model SearchPipelineResponseBody = {
  pageNumber?: long(name='PageNumber', description='The page number of the returned page.', example='1'),
  pageSize?: long(name='PageSize', description='The number of entries returned per page.', example='10'),
  pipelineList?: {
    pipeline?: [ 
    {
      creationTime?: string(name='CreationTime'),
      id?: string(name='Id', description='The ID of the MPS queue.', example='d1ce4d3efcb549419193f50f1fcd****'),
      name?: string(name='Name', description='The name of the MPS queue.', example='example-pipeline-****'),
      notifyConfig?: {
        mqTag?: string(name='MqTag', description='The tags.', example='mts-test'),
        mqTopic?: string(name='MqTopic', description='The queue of messages that are received.', example='example1,example2'),
        queueName?: string(name='QueueName', description='The name of the queue that is created in MNS.', example='example-queue-****'),
        topic?: string(name='Topic', description='The name of the topic that is created in MNS.', example='example-topic-****'),
      }(name='NotifyConfig', description='The Message Service (MNS) configuration.'),
      quotaAllocate?: long(name='QuotaAllocate', description='The quota that is allocated to the MPS queue.', example='10'),
      role?: string(name='Role', description='The role that is assigned to the current RAM user.', example='AliyunMTSDefaultRole'),
      speed?: string(name='Speed', description='The type of the MPS queue. Default value: **Standard**. Valid values:

*   **Boost**: MPS queue with transcoding speed boosted
*   **Standard**: standard MPS queue
*   **NarrowBandHDV2**: MPS queue that supports Narrowband HD 2.0
*   **AIVideoCover**: MPS queue for intelligent snapshot capture
*   **AIVideoFPShot**: MPS queue for media fingerprinting
*   **AIVideoCensor**: MPS queue for automated review
*   **AIVideoMCU**: MPS queue for smart tagging
*   **AIVideoSummary**: MPS queue for video synopsis
*   **AIVideoPorn**: MPS queue for pornography detection in videos
*   **AIAudioKWS**: MPS queue for keyword recognition in audio
*   **AIAudioASR**: MPS queue for speech-to-text conversion', example='Standard'),
      speedLevel?: long(name='SpeedLevel', description='The level of the MPS queue.', example='1'),
      state?: string(name='State', description='The state of the MPS queue. Valid values:

*   **Active**: The MPS queue is active.
*   **Paused**: The MPS queue is paused.', example='Paused'),
    }
  ](name='Pipeline')
  }(name='PipelineList', description='The MPS queues.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='338CA33A-AE83-5DF4-B6F2-C6D3ED8143F5'),
  totalCount?: long(name='TotalCount', description='The total number of entries returned.', example='1'),
}

model SearchPipelineResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: SearchPipelineResponseBody(name='body'),
}

/**
  * You can call this operation to query MPS queues in the specified state. If you do not specify the state, all MPS queues are queried by default.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function searchPipeline(request: SearchPipelineRequest): SearchPipelineResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'SearchPipeline', 'POST', '/', 'json', false, 'json', request);
}

model SearchTemplateRequest {
  namePrefix?: string(name='NamePrefix', description='The name prefix based on which you want to search for templates.', example='S00000001', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  pageNumber?: long(name='PageNumber', description='The number of the page to return. Default value: **1**.', example='1', position='Query'),
  pageSize?: long(name='PageSize', description='The size of each page set during the result paging query.

- Upper limit: 100.
- Default value: 10.', example='10', minimum=1, maximum=100, position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  state?: string(name='State', description='The status of the custom transcoding templates that you want to query.

*   **All**: All custom transcoding templates are queried.
*   **Normal**: Normal custom transcoding templates are queried.
*   **Deleted**: Deleted custom transcoding templates are queried.
*   Default value: **All**.', example='Normal', position='Query'),
}

model SearchTemplateResponseBody = {
  pageNumber?: long(name='PageNumber', description='The number of the page to return.', example='1'),
  pageSize?: long(name='PageSize', description='The page number of the returned page.', example='10'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='BC860F04-778A-472F-AB39-E1BF329C****'),
  templateList?: {
    template?: [ 
    {
      audio?: {
        bitrate?: string(name='Bitrate', description='The audio bitrate of the output file.

*   Unit: Kbit/s.
*   Default value: **128**.', example='500'),
        channels?: string(name='Channels', description='The number of sound channels. Default value: **2**.', example='2'),
        codec?: string(name='Codec', description='The audio codec format. Default value: **aac**. Valid values:

*   **aac**
*   **mp3**
*   **vorbis**
*   **flac**', example='aac'),
        profile?: string(name='Profile', description='The codec profile of the audio. Valid values when the value of Codec is aac:

*   **aac_low**
*   **aac_he**
*   **aac_he_v2**
*   **aac_ld**
*   **aac_eld**', example='aac_low'),
        qscale?: string(name='Qscale', description='The strength of the independent denoising algorithm. Valid values: **\\[1,9]**.', example='1'),
        remove?: string(name='Remove', description='Indicates whether the audio stream is deleted. Valid values:

*   **true**
*   **false**
*   Default value: **false**.', example='false'),
        samplerate?: string(name='Samplerate', description='The sampling rate.

*   Unit: Hz
*   Default value: **44100**.', example='44100'),
      }(name='Audio', description='The audio codec configurations.'),
      container?: {
        format?: string(name='Format', description='The format of the container. Valid values:

*   **flv**
*   **mp4**
*   **ts**
*   **m3u8**
*   **gif**
*   **mp3**
*   **ogg**
*   **flac**', example='mp4'),
      }(name='Container', description='The container format configurations.'),
      creationTime?: string(name='CreationTime', description='The time when the template was created.', example='2021-03-04T06:44:43Z'),
      id?: string(name='Id', description='The transcoding template ID.', example='16f01ad6175e4230ac42bb5182cd****'),
      muxConfig?: {
        gif?: {
          ditherMode?: string(name='DitherMode', description='The color dithering algorithm of the palette. Valid values: sierra and bayer.', example='sierra'),
          finalDelay?: string(name='FinalDelay', description='The duration for which the final frame is paused. Unit: centisecond.', example='0'),
          isCustomPalette?: string(name='IsCustomPalette', description='Indicates whether a custom palette is used.', example='false'),
          loop?: string(name='Loop', description='The loop count.', example='0'),
        }(name='Gif', description='The transmuxing configurations for GIF.'),
        segment?: {
          duration?: string(name='Duration', description='The length of the segment. Unit: seconds.', example='10'),
        }(name='Segment', description='The segment configurations.'),
      }(name='MuxConfig', description='The transmuxing configurations.'),
      name?: string(name='Name', description='The name of the template.', example='MPS-example'),
      state?: string(name='State', description='The status of the template. Valid values:

*   **Normal**
*   **Deleted**', example='Normal'),
      transConfig?: {
        adjDarMethod?: string(name='AdjDarMethod', description='The method of resolution adjustment. Default value: **none**. Valid values:

*   rescale
*   crop
*   none', example='none'),
        isCheckAudioBitrate?: string(name='IsCheckAudioBitrate', description='Indicates whether the audio bitrate is checked. If the bitrate of the output audio is higher than that of the input audio, the input bitrate is retained and the specified audio bitrate does not take effect. This parameter has a lower priority than IsCheckAudioBitrateFail. Valid values:

*   **true**

*   **false**

*   Default value:

    *   If this parameter is empty and the codec of the output audio is different from that of the input audio, the default value is false.
    *   If this parameter is empty and the codec of the output audio is the same as that of the input audio, the default value is true.', example='false'),
        isCheckAudioBitrateFail?: string(name='IsCheckAudioBitrateFail', description='Indicates whether audio bitrate check errors are allowed. This parameter has a greater priority than IsCheckAudioBitrate. Valid values:

*   **true**: If the audio bitrate check fails, the input file is not transcoded.
*   **false**: The audio bitrate is not checked.
*   Default value: **false**.', example='false'),
        isCheckReso?: string(name='IsCheckReso', description='Indicates whether the resolution is checked. If the output resolution is higher than the input resolution based on the width or height, the input resolution is retained. Valid values:

*   **true**
*   **false**
*   Default value: **false**.', example='false'),
        isCheckResoFail?: string(name='IsCheckResoFail', description='Indicates whether the resolution is checked. If the output resolution is higher than the input resolution based on the width or height, a transcoding failure is returned. Valid values:

*   **true**
*   **false**
*   Default value: **false**.', example='false'),
        isCheckVideoBitrate?: string(name='IsCheckVideoBitrate', description='Indicates whether the video bitrate is checked. If the bitrate of the output video is higher than that of the input video, the input bitrate is retained. Valid values:

*   **true**
*   **false**
*   Default value: **false**.', example='false'),
        isCheckVideoBitrateFail?: string(name='IsCheckVideoBitrateFail', description='Indicates whether video bitrate check errors are allowed. This parameter has a higher priority than IsCheckVideoBitrate. Valid values:

*   **true**: If the video bitrate check fails, the input file is not transcoded.
*   **false**: The video bitrate is not checked.
*   Default value: **false**.', example='false'),
        transMode?: string(name='TransMode', description='The transcoding mode. Default value: **onepass**. Valid values:

*   **onepass**
*   **twopass**
*   **CBR**', example='onepass'),
      }(name='TransConfig', description='The general transcoding configurations.'),
      video?: {
        bitrate?: string(name='Bitrate', description='The average bitrate of the video. Unit: Kbit/s.', example='200'),
        bitrateBnd?: {
          max?: string(name='Max', description='The upper limit of the total bitrate. Unit: Kbit/s.', example='500'),
          min?: string(name='Min', description='The lower limit of the total bitrate. Unit: Kbit/s.', example='100'),
        }(name='BitrateBnd', description='The average bitrate range of the video.'),
        bufsize?: string(name='Bufsize', description='The buffer size.

*   Unit: KB.
*   Default value: **6000**.', example='6000'),
        codec?: string(name='Codec', description='The codec.

*   Valid values: H.264 and H.265.
*   Default value: **H.264**.', example='H.264'),
        crf?: string(name='Crf', description='The constant rate factor.

*   Default value when the value of Codec is H.264: **23**, default value when the value of Codec is H.265: **26**.
*   If this parameter is set, the value of Bitrate becomes invalid.', example='15'),
        crop?: string(name='Crop', description='The method of video cropping. Valid values:

*   **border**: automatically detects and removes black bars.
*   **Value in the width:height:left:top format**: crops the video image based on the custom settings. Format: width:height:left:top. Example: 1280:800:0:140.', example='border'),
        degrain?: string(name='Degrain', description='The level of video quality control.', example='10'),
        fps?: string(name='Fps', description='The frame rate of the video.

*   The value is 60 if the frame rate of the input video exceeds 60.
*   Default value: **the frame rate of the input video**.', example='25'),
        gop?: string(name='Gop', description='The maximum number of frames between two keyframes. Default value: **250**.', example='10'),
        hdr2sdr?: string(name='Hdr2sdr', description='Indicates whether the HDR2SDR conversion feature is enabled. If this feature is enabled, high dynamic range (HDR) videos are transcoded to standard dynamic range (SDR) videos.', example='true'),
        height?: string(name='Height', description='The height of the video.

*   Unit: pixel.
*   Default value: **the height of the input video**.', example='800'),
        longShortMode?: string(name='LongShortMode', description='Indicates whether the auto-rotate screen feature is enabled.

*   If this feature is enabled, the width of the output video corresponds to the long side of the input video, which is the height of the input video in portrait mode. The height of the output video corresponds to the short side of the input video, which is the width of the input video in portrait mode. Valid values:
*   **true**
*   **false**
*   Default value: **false**.', example='false'),
        maxFps?: string(name='MaxFps', description='The maximum frame rate.', example='60'),
        maxrate?: string(name='Maxrate', description='The maximum bitrate of the video. Unit: Kbit/s.', example='500'),
        narrowBand?: {
          abrmax?: float(name='Abrmax', description='The upper limit of the dynamic bitrate. If this parameter is set, the average bitrate is in the range of (0, 1000000].', example='3000'),
          maxAbrRatio?: float(name='MaxAbrRatio', description='The maximum ratio of the upper limit of dynamic bitrate. If this parameter is set, the value of Abrmax does not exceed x times of the source video bitrate. Valid values: (0,1.0].', example='1.0'),
          version?: string(name='Version', description='The Narrowband HD version. Only 1.0 may be returned.', example='1.0'),
        }(name='NarrowBand', description='The Narrowband HD settings.'),
        pad?: string(name='Pad', description='The black bars that are added to the video.

*   Format: width:height:left:top.
*   Example: 1280:800:0:140.', example='1280:800:0:140'),
        pixFmt?: string(name='PixFmt', description='The pixel format of the video. Valid values: standard pixel formats such as yuv420p and yuvj420p.', example='yuv420p'),
        preset?: string(name='Preset', description='The preset video algorithm. Default value: **medium**. Valid values:

*   **veryfast**
*   **fast**
*   **medium**
*   **slow**
*   **slower**', example='medium'),
        profile?: string(name='Profile', description='The codec profile. Valid values:

*   **baseline**: applicable to mobile devices.
*   **main**: applicable to standard-definition devices.
*   **high**: applicable to high-definition devices.
*   Default value: **high**.', example='high'),
        qscale?: string(name='Qscale', description='The strength of the independent denoising algorithm.', example='1'),
        remove?: string(name='Remove', description='Indicates whether the video stream is deleted. Valid values:

*   **true**
*   **false**
*   Default value: **false**.', example='false'),
        resoPriority?: string(name='ResoPriority', description='The policy of resolution adjustment.', example='heightFirst'),
        scanMode?: string(name='ScanMode', description='The scan mode. Valid values:

*   **interlaced**
*   **progressive**', example='interlaced'),
        width?: string(name='Width', description='The width of the video.

*   Valid values: **\\[128,4096]**.
*   Unit: pixel.
*   Default value: **the width of the input video**.', example='256'),
      }(name='Video', description='The video codec configurations.'),
    }
  ](name='Template')
  }(name='TemplateList', description='The transcoding templates.'),
  totalCount?: long(name='TotalCount', description='The total number of search results.', example='1'),
}

model SearchTemplateResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: SearchTemplateResponseBody(name='body'),
}

/**
  * You can call this operation up to 100 times per second. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function searchTemplate(request: SearchTemplateRequest): SearchTemplateResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'SearchTemplate', 'POST', '/', 'json', false, 'json', request);
}

model SearchWaterMarkTemplateRequest {
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  pageNumber?: long(name='PageNumber', description='The page number. Default value: **1**.', example='1', position='Query'),
  pageSize?: long(name='PageSize', description='The number of entries per page.

*   A maximum of **100** entries can be returned on each page.
*   Default value: **10**.', example='10', minimum=1, maximum=100, position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  state?: string(name='State', description='The state of the watermark templates that you want to query. Valid values:

*   **All (default)**
*   **Normal**
*   **Deleted**', example='Normal', position='Query'),
}

model SearchWaterMarkTemplateResponseBody = {
  pageNumber?: long(name='PageNumber', description='The width of the watermark image in the output video. The value can be an integer or a decimal.

*   **Integer**: the width of the watermark image. This indicates the absolute position. Unit: pixel.
*   **Decimal**: the ratio of the width of the watermark image to the width of the output video. The ratio varies based on the size of the video. Four decimal places are supported, such as 0.9999. More decimal places are discarded.', example='1'),
  pageSize?: long(name='PageSize', description='The values of the Height, Width, Dx, and Dy parameters relative to the reference edges. If the values of the Height, Width, Dx, and Dy parameters are decimals between 0 and 1, the values are calculated by referring to the following edges in sequence:

*   **Width**: the width edge.
*   **Height**: the height edge.
*   **Long**: the long edge.
*   **Short**: the short edge.', example='10'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='FC029D04-8F47-57FF-A759-23383C15617D'),
  totalCount?: long(name='TotalCount', description='The type of the watermark. Valid values:

*   Image: an image watermark.
*   Text: a text watermark.

>  Only watermarks of the **Image** types are supported.', example='1'),
  waterMarkTemplateList?: {
    waterMarkTemplate?: [ 
    {
      dx?: string(name='Dx', description='The name of the watermark template.', example='100'),
      dy?: string(name='Dy', description='The values of the Height, Width, Dx, and Dy parameters relative to the reference edges. If the values of the Height, Width, Dx, and Dy parameters are decimals between 0 and 1, the values are calculated by referring to the following edges in sequence:

*   **Width**: the width edge.
*   **Height**: the height edge.
*   **Long**: the long edge.
*   **Short**: the short edge.', example='100'),
      height?: string(name='Height', description='The ID of the watermark template.', example='8'),
      id?: string(name='Id', description='The vertical offset. Unit: pixel.', example='88c6ca184c0e4578645b665e2a12****'),
      name?: string(name='Name', description='The width of the watermark image in the output video. The value can be an integer or a decimal.

*   **Integer**: the width of the watermark image. This indicates the absolute position. Unit: pixel.
*   **Decimal**: the ratio of the width of the watermark image to the width of the output video. The ratio varies based on the size of the video. Four decimal places are supported, such as 0.9999. More decimal places are discarded.', example='example-watermark'),
      ratioRefer?: {
        dx?: string(name='Dx', description='The horizontal offset. Unit: pixel.', example='0.51'),
        dy?: string(name='Dy', description='The timeline of the watermark.', example='0.2'),
        height?: string(name='Height', description='The height of the watermark image. Unit: pixel.', example='0.33'),
        width?: string(name='Width', description='The width of the watermark image. Unit: pixel.', example='0.36'),
      }(name='RatioRefer', description='The status of the watermark template. Valid values: Valid values:

*   **Normal**: The watermark template is normal.
*   **Deleted**: The watermark template is deleted.'),
      referPos?: string(name='ReferPos', description='The beginning of the time range during which the watermark is displayed.

*   Unit: seconds.
*   Default value: **0**.', example='TopRight'),
      state?: string(name='State', description='The display duration of the watermark. Default value: **ToEND**. The default value indicates that the watermark is displayed until the video ends.', example='Normal'),
      timeline?: {
        duration?: string(name='Duration', description='The horizontal offset of the watermark relative to the output video image. Default value: **0**. The default value indicates no offset.

The value can be an integer or a decimal.

*   **Integer**: the vertical offset. This indicates the absolute position. Unit: pixel.
*   **Decimal**: the ratio of the horizontal offset to the width of the output video. The ratio varies based on the size of the video. Four decimal places are supported, such as 0.9999. More decimal places are discarded.', example='ToEND'),
        start?: string(name='Start', description='The total number of returned entries.', example='0'),
      }(name='Timeline', description='The timeline of the watermark.'),
      type?: string(name='Type', description='The position of the watermark. Valid values:

*   TopRight: the upper-right corner.
*   TopLeft: the upper-left corner.
*   BottomRight: the lower-right corner.
*   BottomLeft: the lower-left corner.', example='Image'),
      width?: string(name='Width', description='The vertical offset. Unit: pixel.', example='8'),
    }
  ](name='WaterMarkTemplate')
  }(name='WaterMarkTemplateList', description='The height of the watermark image in the output video. The value can be an integer or a decimal.

*   **Integer**: the height of the watermark image. This indicates the absolute position. Unit: pixel.
*   **Decimal**: the ratio of the height of the watermark image to the height of the output video. The ratio varies based on the size of the video. Four decimal places are supported, such as 0.9999. More decimal places are discarded.'),
}

model SearchWaterMarkTemplateResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: SearchWaterMarkTemplateResponseBody(name='body'),
}

/**
  * The total number of returned entries.
  *
 */
async function searchWaterMarkTemplate(request: SearchWaterMarkTemplateRequest): SearchWaterMarkTemplateResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'SearchWaterMarkTemplate', 'POST', '/', 'json', false, 'json', request);
}

model SubmitAnalysisJobRequest {
  analysisConfig?: string(name='AnalysisConfig', description='The job configurations. Set this parameter as required. For more information, see the "AnalysisConfig" section of the [Parameter details](~~29253~~) topic.', example='{"QualityControl":{"RateQuality":25,"MethodStreaming":"network"}}', position='Query'),
  input: string(name='Input', description='The input information about the preset template analysis job to be submitted. The value must be a JSON object. You must log on to the Object Storage Service (OSS) console to grant the read permissions on the specified OSS bucket to MPS. For more information, see the "Input" section of the [Parameter details](~~29253~~) topic.

> The OSS bucket must reside in the same region as your MPS service.', example='{"Bucket":"example-bucket","Location":"oss-cn-hangzhou","Object":"example.flv"}', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  pipelineId: string(name='PipelineId', description='The ID of the MPS queue to which the job is submitted. To view the ID of the MPS queue, log on to the **MPS console** and choose **Global Settings** > **Pipelines** in the left-side navigation pane. If you want to enable asynchronous notifications, make sure that the MPS queue is bound to a Message Service (MNS) topic.', example='bb558c1cc25b45309aab5be44d19****', position='Query'),
  priority?: string(name='Priority', description='The priority of the job in the MPS queue to which the job is submitted.

*   Valid values: **1 to 10**. A value of 10 indicates the highest priority.
*   Default value: **6**.', example='10', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  userData?: string(name='UserData', description='The custom data. The custom data can contain letters, digits, and hyphens (-), and can be up to 1,024 bytes in length. The custom data cannot start with a special character.', example='testid-001', position='Query'),
}

model SubmitAnalysisJobResponseBody = {
  analysisJob?: {
    analysisConfig?: {
      propertiesControl?: {
        crop?: {
          height?: string(name='Height', description='The height of the video after the margins were cropped out.

> This parameter is invalid if the **Mode** parameter is set to Auto or None.', example='8'),
          left?: string(name='Left', description='The left margin that was cropped out.

> This parameter is invalid if the **Mode** parameter is set to Auto or None.', example='8'),
          mode?: string(name='Mode', description='The cropping mode. Valid values:

*   **Auto**: Cropping was automatically run. This is the default value.
*   **Force**: Cropping was forced to run.
*   **None**: Cropping was forced not to run.', example='Auto'),
          top?: string(name='Top', description='The top margin that was cropped out.

> This parameter is invalid if the **Mode** parameter is set to Auto or None.', example='8'),
          width?: string(name='Width', description='The width of the video after the margins were cropped out.

> This parameter is invalid if the **Mode** parameter is set to Auto or None.', example='8'),
        }(name='Crop', description='The cropping configurations of video images.'),
        deinterlace?: string(name='Deinterlace', description='Indicates whether deinterlacing was forced to run. Valid values:

*   **Auto**: Deinterlacing was automatically run.
*   **Force**: Deinterlacing was forced to run.
*   **None**: Deinterlacing was forced not to run.', example='Force'),
      }(name='PropertiesControl', description='The control on the attributes of the job output.'),
      qualityControl?: {
        methodStreaming?: string(name='MethodStreaming', description='The playback mode. Valid values:

*   **network**: online playback
*   **local**: playback on local devices
*   Default value: **network**.', example='network'),
        rateQuality?: string(name='RateQuality', description='The quality level of the output file.', example='50'),
      }(name='QualityControl', description='The quality control on the job output.'),
    }(name='AnalysisConfig', description='The job configurations.'),
    code?: string(name='Code', description='The error code returned if the job failed. This parameter is not returned if the job was successful.', example='InvalidParameter.ResourceNotFound'),
    creationTime?: string(name='CreationTime', description='The time when the job was created.', example='2014-01-10T12:00:00Z'),
    id?: string(name='Id', description='The ID of the template analysis job.', example='57f6aa3f84824309bcba67231b40****'),
    inputFile?: {
      bucket?: string(name='Bucket', description='The name of the OSS bucket.', example='example-bucket'),
      location?: string(name='Location', description='The ID of the OSS region.', example='oss-cn-hangzhou'),
      object?: string(name='Object', description='The name of the OSS object that is used as the input file.', example='example.flv'),
    }(name='InputFile', description='The information about the job input.'),
    MNSMessageResult?: {
      errorCode?: string(name='ErrorCode', description='The error code returned if the job failed. This parameter is not returned if the job was successful.', example='InvalidParameter.ResourceNotFound'),
      errorMessage?: string(name='ErrorMessage', description='The error message returned if the job failed. This parameter is not returned if the job was successful.', example='The resource operated \\"PipelineId\\" cannot be found'),
      messageId?: string(name='MessageId', description='The ID of the message returned if the job was successful. This parameter is not returned if the job failed.', example='3ca84a39a9024f19853b21be9cf9****'),
    }(name='MNSMessageResult', description='The message sent by MNS to notify users of the job result.'),
    message?: string(name='Message', description='The error message returned if the job failed.', example='The resource operated \\"PipelineId\\" cannot be found'),
    percent?: long(name='Percent', description='The transcoding progress.', example='100'),
    pipelineId?: string(name='PipelineId', description='The ID of the MPS queue to which the analysis job was submitted.', example='bb558c1cc25b45309aab5be44d19****'),
    priority?: string(name='Priority', description='The priority of the job in the MPS queue to which the job was submitted.

*   Valid values: **1 to 10**. A value of 10 indicates the highest priority.
*   Default value: **10**.', example='10'),
    state?: string(name='State', description='The status of the job. Valid values:

*   **Submitted**: The job is submitted.
*   **Analyzing**: The job is being run.
*   **Success**: The job is successful.
*   **Fail**: The job fails.', example='Success'),
    templateList?: {
      template?: [ 
      {
        audio?: {
          bitrate?: string(name='Bitrate', description='The audio bitrate of the output file.

*   Unit: Kbit/s.
*   Default value: **128**.', example='8'),
          channels?: string(name='Channels', description='The number of sound channels. Default value: **2**.', example='1'),
          codec?: string(name='Codec', description='The audio codec format. Default value: **acc**.', example='mp3'),
          profile?: string(name='Profile', description='The codec profile of the audio. Valid values if the **Codec** parameter is set to **aac**: aac_low, aac_he, aac_he_v2, aac_ld, and aac_eld.', example='aac_low'),
          qscale?: string(name='Qscale', description='The level of quality control on the audio.', example='10'),
          samplerate?: string(name='Samplerate', description='The sampling rate.

*   Unit: Hz.
*   Default value: **44100**.', example='32000'),
        }(name='Audio', description='The audio codec configurations.'),
        container?: {
          format?: string(name='Format', description='The container format.', example='flv'),
        }(name='Container', description='The container format configurations.'),
        id?: string(name='Id', description='The ID of the transcoding template.', example='S00000000-00****'),
        muxConfig?: {
          gif?: {
            finalDelay?: string(name='FinalDelay', description='The interval between two consecutive loops for the GIF format. Unit: 0.01s. For example, a value of 500 indicates 5 seconds.', example='0'),
            loop?: string(name='Loop', description='The number of loops for the GIF or WebP format. Default value: 0.', example='0'),
          }(name='Gif', description='The transmuxing configurations for the GIF format.'),
          segment?: {
            duration?: string(name='Duration', description='The length of the segment. Unit: seconds.', example='60'),
          }(name='Segment', description='The segment configurations.'),
        }(name='MuxConfig', description='The transmuxing configurations.'),
        name?: string(name='Name', description='The name of the template.', example='FLV-UD'),
        state?: string(name='State', description='The status of the template.

*   **Normal**: The template is normal.
*   **Deleted**: The template is deleted.', example='Normal'),
        transConfig?: {
          transMode?: string(name='TransMode', description='The transcoding mode. Valid values: onepass, twopass, and CBR. Default value: **onepass**.', example='onepass'),
        }(name='TransConfig', description='The general transcoding configurations.'),
        video?: {
          bitrate?: string(name='Bitrate', description='The average bitrate of the video. Unit: Kbit/s.', example='10'),
          bitrateBnd?: {
            max?: string(name='Max', description='The upper limit of the total bitrate. Unit: Kbit/s.', example='20'),
            min?: string(name='Min', description='The lower limit of the total bitrate. Unit: Kbit/s.', example='10'),
          }(name='BitrateBnd', description='The average bitrate range of the video.'),
          bufsize?: string(name='Bufsize', description='The size of the buffer.

*   Unit: KB.
*   Default value: **6000**.', example='5000'),
          codec?: string(name='Codec', description='The video codec. Default value: **H.264**.', example='H.264'),
          crf?: string(name='Crf', description='The constant rate factor.

*   Default value if the Codec parameter is set to H.264: **23**. Default value if the Codec parameter is set to H.265: **26**.
*   If this parameter is returned, the setting of the Bitrate parameter is invalid.', example='27'),
          degrain?: string(name='Degrain', description='The strength of the independent noise reduction algorithm.', example='5'),
          fps?: string(name='Fps', description='The frame rate.

*   The value is 60 if the frame rate of the input video exceeds 60.
*   Default value: the frame rate of the input video.', example='60'),
          gop?: string(name='Gop', description='The maximum number of frames between two keyframes. Default value: **250**.', example='1'),
          height?: string(name='Height', description='The height of the video.

*   Unit: pixel.
*   Default value: the height of the input video.', example='1880'),
          maxrate?: string(name='Maxrate', description='The maximum bitrate of the video. Unit: Kbit/s.', example='10'),
          pixFmt?: string(name='PixFmt', description='The pixel format for video color encoding. Valid values: standard pixel formats such as yuv420p and yuvj420p.', example='yuvj420p'),
          preset?: string(name='Preset', description='The preset video algorithm. Valid values: veryfast, fast, medium, slow, and slower. Default value: **medium**.', example='medium'),
          profile?: string(name='Profile', description='The codec profile. Valid values:

*   **baseline**: applicable to mobile devices.
*   **main**: applicable to standard-definition devices.
*   **high**: applicable to high-definition devices.
*   Default value: **high**.', example='baseline'),
          qscale?: string(name='Qscale', description='The level of quality control on the video.', example='15'),
          scanMode?: string(name='ScanMode', description='The scan mode. Valid values:

*   **interlaced**
*   **progressive**', example='progressive'),
          width?: string(name='Width', description='The width of the video.

*   Unit: pixel.
*   Default value: the width of the input video.', example='1990'),
        }(name='Video', description='The video codec configurations.'),
      }
    ](name='Template')
    }(name='TemplateList', description='The matched preset templates.'),
    userData?: string(name='UserData', description='The custom data.', example='testid-001'),
  }(name='AnalysisJob', description='The information about the preset template analysis job that was submitted.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='B52658D4-07AB-43CD-82B0-210958A65E23'),
}

model SubmitAnalysisJobResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: SubmitAnalysisJobResponseBody(name='body'),
}

/**
  * *   After you call the SubmitAnalysisJob operation to submit a preset template analysis job, ApsaraVideo Media Processing (MPS) intelligently analyzes the input file of the job and recommends a suitable preset template. You can call the [QueryAnalysisJobList](~~29224~~) operation to query the analysis result or enable asynchronous notifications to receive the analysis result.
  * *   The analysis result is retained only for two weeks after it is generated. The analysis result is deleted after two weeks. If you use the recommended preset template in a transcoding job after two weeks, the job fails, and the `AnalysisResultNotFound` error code is returned.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function submitAnalysisJob(request: SubmitAnalysisJobRequest): SubmitAnalysisJobResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'SubmitAnalysisJob', 'POST', '/', 'json', false, 'json', request);
}

model SubmitCopyrightExtractJobRequest {
  callBack?: string(name='CallBack', example='http://example.com/callback', position='Query'),
  input?: string(name='Input', example='{"Bucket":"example","Location":"oss-cn-shanghai","Object":"example.mp4"}', position='Query'),
  params?: string(name='Params', example='{"algoType":"v1"}', position='Query'),
  url?: string(name='Url', example='http://www.example.com/video/test.mp4', position='Query'),
  userData?: string(name='UserData', example='123', position='Query'),
}

model SubmitCopyrightExtractJobResponseBody = {
  data?: {
    jobId?: string(name='JobId', example='ebbfe90c63b54ed4b61acb2f6c44****'),
  }(name='Data'),
  message?: string(name='Message', example='ok'),
  requestId?: string(name='RequestId', description='Id of the request', example='05F8B913-E9F3-4A6F-9922-48CADA0FFAAD'),
  statusCode?: long(name='StatusCode', example='200'),
}

model SubmitCopyrightExtractJobResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: SubmitCopyrightExtractJobResponseBody(name='body'),
}

async function submitCopyrightExtractJob(request: SubmitCopyrightExtractJobRequest): SubmitCopyrightExtractJobResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'SubmitCopyrightExtractJob', 'POST', '/', 'json', false, 'json', request);
}

model SubmitCopyrightJobRequest {
  callBack?: string(name='CallBack', example='http://example.com/callback', position='Query'),
  description?: string(name='Description', position='Query'),
  input?: string(name='Input', example='{"Bucket":"example-bucket","Location":"oss-cn-shanghai","Object":"example.mp4"}', position='Query'),
  level?: long(name='Level', example='2', position='Query'),
  message: string(name='Message', position='Query'),
  output: string(name='Output', example='{"Bucket":"example-bucket","Location":"oss-cn-shanghai","Object":"example_result.mp4"}', position='Query'),
  params?: string(name='Params', example='{"algoType":"v2"}', position='Query'),
  startTime?: string(name='StartTime', example='0', position='Query'),
  totalTime?: string(name='TotalTime', example='10', position='Query'),
  url?: string(name='Url', example='http://www.example.com/video/test.mp4', position='Query'),
  userData?: string(name='UserData', example='123', position='Query'),
}

model SubmitCopyrightJobResponseBody = {
  data?: {
    jobId?: string(name='JobId', example='bfb786c639894f4d80648792021e****'),
  }(name='Data'),
  message?: string(name='Message', example='ok'),
  requestId?: string(name='RequestId', description='Id of the request', example='FA258E67-09B8-4EAA-8F33-BA567834A2C3'),
  statusCode?: long(name='StatusCode', example='200'),
}

model SubmitCopyrightJobResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: SubmitCopyrightJobResponseBody(name='body'),
}

async function submitCopyrightJob(request: SubmitCopyrightJobRequest): SubmitCopyrightJobResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'SubmitCopyrightJob', 'POST', '/', 'json', false, 'json', request);
}

model SubmitFpDBDeleteJobRequest {
  delType?: string(name='DelType', description='The operation type. Valid values:

*   **Purge**: clears the media fingerprint library. The content in the library is deleted, but the library is not deleted.
*   **Delete**: deletes the media fingerprint library. Both the library and its content are deleted.
*   Default value: **Purge**.', example='Purge', position='Query'),
  fpDBId: string(name='FpDBId', description='The ID of the media fingerprint library. You can obtain the library ID from the response parameters of the [CreateFpShotDB](~~170149~~) operation.', example='88c6ca184c0e47098a5b665e2a12****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  pipelineId?: string(name='PipelineId', description='The ID of the ApsaraVideo Media Processing (MPS) queue. This ID can be used to associate the job with a notification method. To view the MPS queue ID, log on to the **MPS console** and choose **Global Settings** > **Pipelines** in the left-side navigation pane.', example='fb712a6890464059b1b2ea7c8647****', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  userData?: string(name='UserData', description='The custom data. The value can contain letters and digits and can be up to 128 bytes in length.', example='example data', position='Query'),
}

model SubmitFpDBDeleteJobResponseBody = {
  jobId?: string(name='JobId', description='The ID of the job. We recommend that you keep this ID for subsequent operation calls.', example='d98459323c024947a104f6a50cbf****'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='4247B23C-26DE-529F-8D9F-FD6811AE979B'),
}

model SubmitFpDBDeleteJobResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: SubmitFpDBDeleteJobResponseBody(name='body'),
}

/**
  * You can call this operation to clear or delete the specified media fingerprint library based on the library ID. If you clear a media fingerprint library, the content in the library is deleted, but the library is not deleted. If you delete a media fingerprint library, both the library and the content in the library are deleted. If you do not specify the operation type, the system clears the media fingerprint library by default.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function submitFpDBDeleteJob(request: SubmitFpDBDeleteJobRequest): SubmitFpDBDeleteJobResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'SubmitFpDBDeleteJob', 'POST', '/', 'json', false, 'json', request);
}

model SubmitFpFileDeleteJobRequest {
  fileIds?: string(name='FileIds', description='The IDs of the media files that you want to delete. Separate multiple file IDs with commas (,). You can delete up to 200 media files at a time. You can obtain media file IDs from the response parameters of the [ListFpShotFiles](~~209266~~) operation.', example='41e6536e4f2250e2e9bf26cdea19****', position='Query'),
  fpDBId: string(name='FpDBId', description='The ID of the media fingerprint library. You can obtain the library ID from the response parameters of the [CreateFpShotDB](~~170149~~) operation.', example='88c6ca184c0e432bbf5b665e2a15****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  pipelineId?: string(name='PipelineId', description='The ID of the ApsaraVideo Media Processing (MPS) queue to which the job is submitted. The MPS queue is bound with a notification method. To view the MPS queue ID, log on to the **MPS console** and choose **Global Settings** > **MPS queue and Callback** in the left-side navigation pane.', example='ed450ea0bfbd41e29f80a401fb4d****', position='Query'),
  primaryKeys?: string(name='PrimaryKeys', description='The primary keys of the files to be deleted. Separate multiple primary keys with commas (,). You can delete up to 200 primary keys at a time. You can obtain the primary keys of media files from the response parameters of the [ListFpShotFiles](~~209266~~) operation.

>  This parameter is available only in the China (Beijing), China (Hangzhou), and China (Shanghai) regions.', example='24e0fba7188fae707e146esa54****', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  userData?: string(name='UserData', description='The custom data. The value can contain letters and digits and can be up to 128 bytes in length.', example='example data', position='Query'),
}

model SubmitFpFileDeleteJobResponseBody = {
  jobId?: string(name='JobId', description='The job ID.', example='39f8e0bc005e4f309379701645f4****'),
  requestId?: string(name='RequestId', description='The request ID.', example='D127C68E-F1A1-4CE5-A874-8FF724881A12'),
}

model SubmitFpFileDeleteJobResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: SubmitFpFileDeleteJobResponseBody(name='body'),
}

/**
  * ## [](#)Limits
  * *   You can call this operation to delete up to 200 media files from a media fingerprint library at a time.
  * *   This operation is available in the following regions: China (Beijing), China (Hangzhou), China (Shanghai), and Singapore.
  * ## [](#qps-)QPS limits
  * You can call this operation up to 10 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limits](~~342832~~).
  *
 */
async function submitFpFileDeleteJob(request: SubmitFpFileDeleteJobRequest): SubmitFpFileDeleteJobResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'SubmitFpFileDeleteJob', 'POST', '/', 'json', false, 'json', request);
}

model SubmitFpShotJobRequest {
  fpShotConfig: string(name='FpShotConfig', description='The configurations of the media fingerprint analysis job. The value is a JSON object. For more information, see the "FpShotConfig" section of the [Parameter details](~~93568~~) topic.', example='{
      "PrimaryKey": "12345****",
      "SaveType": "save",
      "FpDBId": "417f2ada5999daf****"
}', position='Query'),
  input: string(name='Input', description='The OSS URL of the job input. The value is a JSON object. You can query the OSS URL in the OSS or MPS console.

> The OSS bucket must reside in the same region as your MPS service.', example='{"Bucket":"example-bucket-****","Location":"oss-cn-shanghai","Object":"example-****.flv"}', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  pipelineId?: string(name='PipelineId', description='The ID of the MPS queue. This ID can be used to associate the job with a notification method. To view the ID of the MPS queue, perform the following steps: Log on to the **MPS console**. In the left-side navigation pane, choose **Global Settings** > **Pipelines**.', example='88c6ca184c0e47098a5b665e2a12****', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  userData?: string(name='UserData', description='The custom data. The value can be up to 128 bytes in length and cannot start with a special character.', example='testid-****', position='Query'),
}

model SubmitFpShotJobResponseBody = {
  jobId?: string(name='JobId', description='The ID of the media fingerprint analysis job. We recommend that you keep this ID for subsequent operation calls.', example='2a0697e35a7342859f733a9190c4****'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='25818875-5F78-4A13-BEF6-D7393642CA58'),
}

model SubmitFpShotJobResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: SubmitFpShotJobResponseBody(name='body'),
}

/**
  * *   You can call this operation to submit a video, audio, image, or text fingerprint analysis job.
  * *   This operation asynchronously submits a job. The query results may not have been generated when the response is returned. After the results are generated, an asynchronous message is returned.
  * *   You can submit a text fingerprint analysis job only in the China (Shanghai) region.
  * *   The input file of the job must be in one of the following formats:
  *     *   Image formats: JPEG, PNG, and BMP.
  *     *   Video formats: MP4, AVI, MKV, MPG, TS, MOV, FLV, MXF.
  *     *   Video encoding formats: MPEG2, MPEG4, H264, HEVC, and WMV.
  * ### QPS limit
  * You can call this operation up to 150 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function submitFpShotJob(request: SubmitFpShotJobRequest): SubmitFpShotJobResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'SubmitFpShotJob', 'POST', '/', 'json', false, 'json', request);
}

model SubmitIProductionJobRequest {
  functionName: string(name='FunctionName', example='ImageCartoonize', position='Query'),
  input?: string(name='Input', example='oss://example-****.oss-cn-shanghai.aliyuncs.com/example.mp4', position='Query'),
  jobParams?: string(name='JobParams', example='{"Model":"gif"}', position='Query'),
  modelId?: string(name='ModelId', example='null', position='Query'),
  notifyUrl?: string(name='NotifyUrl', example='mns://125340688170****.mns.cn-beijing.aliyuncs.com/queues/example-pipeline', position='Query'),
  output?: string(name='Output', example='oss://example-****.oss-cn-shanghai.aliyuncs.com/iproduction/{source}-{timestamp}-{sequenceId}.srt', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  pipelineId?: string(name='PipelineId', example='39f8e0bc005e4f309379701645f4****', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  scheduleParams?: string(name='ScheduleParams', example='null', position='Query'),
  userData?: string(name='UserData', example='null', position='Query'),
}

model SubmitIProductionJobResponseBody = {
  jobId?: string(name='JobId', example='39f8e0bc005e4f309379701645f4****'),
  requestId?: string(name='RequestId', example='5210DBB0-E327-4D45-ADBC-0B83C8796E26'),
  result?: string(name='Result', example='{ "Code":"Success", "Details":[], "FunctionName":"ImageCartoonize", "JobId":"39f8e0bc005e4f309379701645f4****", "Message":"success", "State":"Success", "Type":"IProduction" }'),
}

model SubmitIProductionJobResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: SubmitIProductionJobResponseBody(name='body'),
}

/**
  * *   Jobs that are submitted by calling this operation run in an asynchronous manner. After a job is added to the ApsaraVideo Media Processing (MPS) queue, the job is scheduled to run. You can call the [QueryIProductionJob](~~170217~~) operation or configure a callback to query the job result.
  * *   Capabilities provided by the intelligent production feature vary based on the region. Before you call this operation to submit an intelligent production job, check whether the job is supported in the region in which your service is activated. For more information, see [Regions and endpoints](~~43248~~).
  * ### [](#qps)QPS limit
  * You can call this API operation up to 100 times per second per account. Requests that exceed this limit are dropped, and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limits](~~342832~~).
  *
 */
async function submitIProductionJob(request: SubmitIProductionJobRequest): SubmitIProductionJobResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'SubmitIProductionJob', 'POST', '/', 'json', false, 'json', request);
}

model SubmitImageCopyrightRequest {
  message: string(name='Message', position='Query'),
  output?: string(name='Output', example='{"Bucket":"abc-test","Location":"oss-cn-shanghai","Object":"out.jpeg"}', position='Query'),
  params?: string(name='Params', example='{"width":2999, "height":2999, "afa": 3, "type":1, "version":0}', position='Query'),
}

model SubmitImageCopyrightResponseBody = {
  data?: {
    jobId?: string(name='JobId', example='bfb786c639894f4d80648792021e****'),
  }(name='Data'),
  message?: string(name='Message', example='ok'),
  requestId?: string(name='RequestId', description='Id of the request', example='D1D5C080-8E2F-5030-8AB4-13092F17631B'),
  statusCode?: long(name='StatusCode', example='200'),
}

model SubmitImageCopyrightResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: SubmitImageCopyrightResponseBody(name='body'),
}

async function submitImageCopyright(request: SubmitImageCopyrightRequest): SubmitImageCopyrightResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'SubmitImageCopyright', 'POST', '/', 'json', false, 'json', request);
}

model SubmitJobsRequest {
  input: string(name='Input', description='The information about the input file. For more information, see the "Input" section of the [Parameter details](~~29253~~) topic.

> 

*   The path of an Object Storage Service (OSS) object must be URL-encoded in UTF-8 before you use the path in MPS.

*   The OSS bucket must reside in the same region as your MPS service.', example='a/b/c/test-cn.srt', position='Query'),
  outputBucket: string(name='OutputBucket', description='The name of the OSS bucket that stores the output file.

*   For more information about the term bucket, see [Terms](~~31827~~).', example='exampleBucket', position='Query'),
  outputLocation?: string(name='OutputLocation', description='The region in which the OSS bucket that stores the output file resides.

*   The OSS bucket must reside in the same region as MPS.
*   For more information about the term bucket, see [Terms](~~31827~~).', example='oss-cn-hangzhou', position='Query'),
  outputs: string(name='Outputs', description='The job output configurations. For more information, see the "Output" section of the [Parameter details](~~29253~~) topic.

*   Specify the value in a JSON array of Output objects. You can specify up to 30 Output objects.', example='[{"OutputObject":"exampleOutput.mp4","TemplateId":"6181666213ab41b9bc21da8ff5ff****","WaterMarks":[{"InputFile":{"Bucket":"exampleBucket","Location":"oss-cn-hangzhou","Object":"image_01.png"},"WaterMarkTemplateId":"9b772ce2740d4d55876d8b542d47****"}],"UserData":"testid-001"}]', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  pipelineId: string(name='PipelineId', description='The ID of the MPS queue. For more information, see [Terms](~~31827~~).

*   To obtain the ID of an MPS queue, you can log on to the [MPS console](https://mps.console.aliyun.com/overview) and choose **Global Settings** > **MPS Queue and Callback** in the left-side navigation pane.
*   If you want to receive asynchronous message notifications, associate an MNS queue or topic with the MPS queue. For more information, see [Receive notifications](https://www.alibabacloud.com/help/zh/apsaravideo-for-media-processing/latest/receive-message-notifications).', example='dd3dae411e704030b921e52698e5****', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model SubmitJobsResponseBody = {
  jobResultList?: {
    jobResult?: [ 
    {
      code?: string(name='Code', description='The error code returned if the job failed to be created. This parameter is not returned if the job was created.', example='InvalidParameter.NullValue'),
      job?: {
        code?: string(name='Code', description='The error code returned if the job failed. This parameter is not returned if the job was successful.', example='InternalError'),
        creationTime?: string(name='CreationTime', description='The time when the job was created.', example='2014-01-10T12:00:00Z'),
        finishTime?: string(name='FinishTime', description='The time when the job was complete.', example='2014-01-10T12:20:00Z'),
        input?: {
          bucket?: string(name='Bucket', description='The name of the OSS bucket in which the job input is stored.', example='example-bucket-****'),
          location?: string(name='Location', description='The ID of the OSS region in which the job input is stored.', example='oss-cn-hangzhou'),
          object?: string(name='Object', description='The name of the OSS object that is used as the job input.', example='example.flv'),
        }(name='Input', description='The information about the job input.'),
        jobId?: string(name='JobId', description='The job ID.', example='31fa3c9ca8134f9cec2b4b0b0f78****'),
        MNSMessageResult?: {
          errorCode?: string(name='ErrorCode', description='The error code returned if the job failed. This parameter is not returned if the job was successful.', example='InvalidParameter.ResourceNotFound'),
          errorMessage?: string(name='ErrorMessage', description='The error message returned if the job failed. This parameter is not returned if the job was successful.', example='The resource operated "%s" cannot be found.'),
          messageId?: string(name='MessageId', description='The ID of the error message returned if the job failed. This parameter is not returned if the job was successful.', example='123'),
        }(name='MNSMessageResult', description='The message sent by MNS to notify users of the job result.'),
        message?: string(name='Message', description='The error message returned if the job failed. This parameter is not returned if the job was successful.', example='The operation has failed due to some unknown error, exception or failure.'),
        output?: {
          amixList?: {
            amix?: [ 
            {
              amixURL?: string(name='AmixURL', description='The URL of the audio track that is mixed as the background music.

*   The URL can be an OSS URL or the string `input`.
*   A value of input indicates that two audio tracks are mixed in a video.', example='https://outpu***.oss-cn-shanghai.aliyuncs.com/mp4-to-mp3%5E1571025263578816%40.mp3'),
              duration?: string(name='Duration', description='The duration of the mixed audio track. The value is in the number or time format.', example='20'),
              map?: string(name='Map', description='The audio track that is mixed. Format: 0:a:{audio_index}. Example: 0:a:0.', example='0:a:0'),
              mixDurMode?: string(name='MixDurMode', description='The mode to specify the mixing duration. Valid values: **first** and **long**.

*   **first**: The length of the output media equals the length of the input media.
*   **long**: The length of the output media equals the length of the output media or the length of the input media, whichever is longer.
*   Default value: **long**.', example='long'),
              start?: string(name='Start', description='The start time. The value is in the number or time format. Examples: 1:25:36.240 and 32000.23.', example='0'),
            }
          ](name='Amix')
          }(name='AmixList', description='The audio tracks that are mixed.'),
          audio?: {
            bitrate?: string(name='Bitrate', description='The audio bitrate of the output file.

*   Unit: Kbit/s.
*   Default value: **128**.', example='128'),
            channels?: string(name='Channels', description='The number of sound channels.

*   If the value of Codec is mp3, the value of this parameter can only be **1** or **2**.
*   If the value of Codec is aac, the value of this parameter can only be **1**, **2**, **4**, **5**, **6**, or **8**.
*   Default value: **2**.', example='6'),
            codec?: string(name='Codec', description='The audio codec.

*   Valid values: **aac**, **mp3**, **vorbis**, and **flac**.
*   Default value: **aac**.', example='aac'),
            profile?: string(name='Profile', description='The codec profile of the audio.

>  Valid values if the value of **Codec** is **aac**: **aac_low**, **aac_he**, **aac_he_v2**, **aac_ld**, and **aac_eld**.', example='aac_low'),
            qscale?: string(name='Qscale', description='The level of quality control on the audio.', example='15'),
            samplerate?: string(name='Samplerate', description='The sampling rate.

*   Valid values: **22050**, **32000**, **44100**, **48000**, and **96000**.
*   Unit: Hz.
*   Default value: **44100**.

>  If the video container format is FLV and the audio codec is MP3, the value of this parameter cannot be 32000, 48000, or 96000. If the audio codec is MP3, the value of this parameter cannot be 96000.', example='32000'),
            volume?: {
              level?: string(name='Level', description='The volume adjustment range.

*   Unit: decibel.
*   Default value: **-20**.', example='-20'),
              method?: string(name='Method', description='The method that is used to adjust the volume. Valid values:

*   **auto**
*   **dynamic**
*   **linear**', example='auto'),
            }(name='Volume', description='The volume configurations.'),
          }(name='Audio', description='The audio configurations.

>  If this parameter is specified in the request, the corresponding configurations in the specified transcoding template are overwritten.'),
          audioStreamMap?: string(name='AudioStreamMap', description='The sequence number of the audio stream.

*   Format: 0:a:{Sequence number}. Example: 0:a:0.
*   The sequence number is the index of the audio stream in the list and starts from 0.
*   If no sequence number is specified, the default audio stream is used.', example='0:a:0'),
          clip?: {
            timeSpan?: {
              duration?: string(name='Duration', description='The duration of the clip.

*   Format: `hh:mm:ss[.SSS]` or `sssss[.SSS]`.
*   Valid values: `[00:00:00.000,23:59:59.999]` or `[0.000,86399.999]`.', example='01:00:59.999'),
              seek?: string(name='Seek', description='The start time.

*   Format: `hh:mm:ss[.SSS]` or `sssss[.SSS]`.
*   Valid values: `[00:00:00.000,23:59:59.999]` or `[0.000,86399.999]`.', example='01:59:59.999'),
            }(name='TimeSpan', description='The time span of the clip.'),
          }(name='Clip', description='The information about the clip.'),
          container?: {
            format?: string(name='Format', description='The container format.

*   Default value: **mp4**.
*   Video formats include FLV, MP4, HLS (M3U8 + TS), and MPEG-DASH (MPD + fMP4).
*   Audio formats include MP3, MP4, Ogg, FLAC, and M4A.
*   Image formats include GIF and WebP.
*   If the container format is GIF, the video codec must be GIF.
*   If the container format is WebP, the video codec must be WebP.
*   If the container format is FLV, the video codec cannot be H.265.', example='flv'),
          }(name='Container', description='The container format configurations.'),
          deWatermark?: string(name='DeWatermark', description='The configurations of watermark blurring. The value is a JSON object. For more information, see the **DeWatermark** section of the [Parameter details](~~29253~~) topic.', example='{"0": [{"l": 10,"t": 10,"w": 10,"h": 10},{"l": 100,"t": 0.1,"w": 10,"h": 10}],"128000": [],"250000": [{"l": 0.2,"t": 0.1,"w": 0.01,"h": 0.05}]}'),
          digiWaterMark?: {
            alpha?: string(name='Alpha', description='The transparency of the text or image.

*   Value values: **(0,1]**.
*   Default value: **1.0**.', example='1.0'),
            inputFile?: {
              bucket?: string(name='Bucket', description='The name of the OSS bucket in which the input file is stored.', example='example-bucket-****'),
              location?: string(name='Location', description='The ID of the OSS region in which the input file is stored.', example='oss-cn-hangzhou'),
              object?: string(name='Object', description='The name of the OSS object that is used as the input file.', example='example-intput.flv'),
            }(name='InputFile', description='The details of the input file.'),
            type?: string(name='Type', description='The type of the watermark. If this parameter is specified in the request, the corresponding parameter in the watermark template is overwritten. Valid values:

*   **Image** (default)
*   **Text**', example='Image'),
          }(name='DigiWaterMark', description='The digital watermarks.'),
          encryption?: {
            id?: string(name='Id', description='The encryption ID.', example='31fa3c9ca8134f9cec2b4b0b0f78****'),
            key?: string(name='Key', description='The key that is used to encrypt the video.', example='encryptionkey128'),
            keyType?: string(name='KeyType', description='The key encryption method. Keys cannot be transmitted to MPS in plaintext. Keys must be encrypted by using Base64 or Key Management Service (KMS). For example, if the key is encryptionkey128, you can encrypt the key by using the following method: Base64("encryptionkey128") or KMS(Base64("encryptionkey128").', example='Base64'),
            keyUri?: string(name='KeyUri', description='The URL that is used to request the key. The URL is Base64-encoded.', example='https://1161758785*****.cn-shanghai.fc.aliyuncs.com/2016-08-15/proxy/HLS-decyptServer/decyptServer/'),
            skipCnt?: string(name='SkipCnt', description='The number of unencrypted frames at the beginning of the video. Leaving these frames unencrypted enables video playback to quickly start.', example='3'),
            type?: string(name='Type', description='The encryption type. Only **hls-aes-128** may be returned.', example='hls-aes-128'),
          }(name='Encryption', description='The encryption configurations. Only outputs in the M3U8 format are supported.'),
          m3U8NonStandardSupport?: {
            ts?: {
              md5Support?: boolean(name='Md5Support', description='Indicates whether the output of the MD5 value of the TS file is supported in the M3U8 video. Valid values:

*   **true**
*   **false**', example='true'),
              sizeSupport?: boolean(name='SizeSupport', description='Indicates whether the size of the TS file is generated in the output M3U8 video. Valid values:

*   **true**
*   **false**', example='true'),
            }(name='TS', description='The non-standard support configurations for TS files. The value is a JSON object. For more information, see the **TS** section of the [Parameter details](~~29253~~) topic.'),
          }(name='M3U8NonStandardSupport', description='The non-standard support configuration for M3U8. The value is a JSON object. For more information, see the **M3U8NonStandardSupport** section of the [Parameter details](~~29253~~) topic.'),
          mergeConfigUrl?: string(name='MergeConfigUrl', description='The URL of the merging configuration file. Only one of **MergeList** and **MergeConfigUrl** takes effect.

*   The configuration file specified by MergeConfigUrl can contain up to 50 clips.
*   MergeConfigUrl indicates the URL of the configuration file for merging clips.
*   Make sure that the configuration file is stored as an object in OSS and that MPS can access the OSS object. For information about the file content, see the details about merging parameters.
*   Example of the content of the merging configuration file: `{"MergeList":[{"MergeURL":"http://exampleBucket****.oss-cn-hangzhou.aliyuncs.com/video_01.mp4"}]}`.', example='`{"MergeList":[{"MergeURL":"http://exampleBucket****.oss-cn-hangzhou.aliyuncs.com/video_01.mp4"}]}'),
          mergeList?: {
            merge?: [ 
            {
              duration?: string(name='Duration', description='The duration of the clip.

*   Format: `hh:mm:ss[.SSS]` or `sssss[.SSS]`.
*   Examples: 01:59:59.999 and 32000.23.', example='00000.20'),
              mergeURL?: string(name='MergeURL', description='The OSS URL of the clip.

*   Example: `http://example-bucket-****.oss-cn-hangzhou.aliyuncs.com/example-object-****.flv`.
*   The OSS URL of the object must be URL-encoded by using the UTF-8 standard.', example='http://example-bucket-****.oss-cn-hangzhou.aliyuncs.com/example-object-****.flv'),
              roleArn?: string(name='RoleArn', description='The Alibaba Cloud Resource Name (ARN) of the Resource Access Management (RAM) role used for delegated authorization.', example='acs:ram::<your uid>:role/<your role name>'),
              start?: string(name='Start', description='The start point in time of the clip.

*   Format: `hh:mm:ss[.SSS]` or `sssss[.SSS]`.
*   Examples: 01:59:59.999 and 32000.23.', example='00000.50'),
            }
          ](name='Merge')
          }(name='MergeList', description='The configurations for merging clips.'),
          muxConfig?: {
            gif?: {
              ditherMode?: string(name='DitherMode', description='The color dithering algorithm of the palette. Valid values: **sierra** and **bayer**.', example='bayer'),
              finalDelay?: string(name='FinalDelay', description='The duration for which the final frame is paused. Unit: centisecond.', example='0'),
              isCustomPalette?: string(name='IsCustomPalette', description='Indicates whether a custom palette is used. Valid values:

*   **true**
*   **false**', example='false'),
              loop?: string(name='Loop', description='The loop count.', example='0'),
            }(name='Gif', description='The transmuxing configurations for GIF.'),
            segment?: {
              duration?: string(name='Duration', description='The length of the segment. The value is an integer. Unit: seconds.

*   Valid values: **\\[1,10]**.
*   Default value: **10**.', example='20'),
            }(name='Segment', description='The segment configuration. The value is a JSON object.'),
            webp?: {
              loop?: string(name='Loop', description='The loop count.', example='0'),
            }(name='Webp', description='The transmuxing configurations for WebP.'),
          }(name='MuxConfig', description='The transmuxing configurations. If this parameter is specified in the request, the corresponding configurations in the specified transcoding template are overwritten.'),
          openingList?: {
            opening?: [ 
            {
              height?: string(name='Height', description='The height of the opening part.

*   Valid values: values in the range of **(0,4096)**, **-1**, and **full**.
*   Default value: **-1**.
*   A value of **-1** indicates that the height of the source of the opening part is retained.
*   A value of **full** indicates that the height of the main part is used for the opening part.', example='1080'),
              start?: string(name='Start', description='The amount of time after which the opening part is played. The value starts from 0.

*   Unit: seconds.
*   Default value: **0**.', example='1'),
              width?: string(name='Width', description='The width of the opening part.

*   Valid values: values in the range of **(0,4096)**, **-1**, and **full**.
*   Default value: **-1**.
*   A value of **-1** indicates that the width of the source of the opening part is retained.
*   A value of **full** indicates that the width of the main part is used for the opening part.', example='1920'),
              openUrl?: string(name='openUrl', description='The OSS URL of the opening part.', example='http://example-bucket-****.oss-cn-hangzhou.aliyuncs.com/opening_01.flv'),
            }
          ](name='Opening')
          }(name='OpeningList', description='The opening parts. The value is a JSON object.'),
          outSubtitleList?: {
            outSubtitle?: [ 
            {
              map?: string(name='Map', description='The video track. Format: `0:{Stream}:{Stream sequence number}`, which is `0:v:{video_index}`. The value of Stream is v, which indicates a video stream. The sequence number is the index of the video stream in the list and starts from 0.', example='0:v:0'),
              message?: string(name='Message', description='The error message returned if the job failed to be created. This parameter is not returned if the job was created.', example='The specified parameter “%s” cannot be null.'),
              outSubtitleFile?: {
                bucket?: string(name='Bucket', description='The name of the OSS bucket in which the input file is stored.', example='example-bucket-****'),
                location?: string(name='Location', description='The ID of the OSS region in which the output file is stored.', example='oss-cn-hangzhou'),
                object?: string(name='Object', description='The name of the OSS object that is used as the output file.', example='example-output.flv'),
                roleArn?: string(name='RoleArn', description='The ARN of the RAM role used for delegated authorization.', example='acs:ram::<your uid>:role/<your role name>'),
              }(name='OutSubtitleFile', description='The details of the output file.'),
              success?: boolean(name='Success', description='Indicates whether the job was created. Valid values:

*   **true**
*   **false**', example='true'),
            }
          ](name='OutSubtitle')
          }(name='OutSubtitleList', description='The output subtitles.'),
          outputFile?: {
            bucket?: string(name='Bucket', description='The name of the OSS bucket in which the input file is stored.', example='example-bucket-****'),
            location?: string(name='Location', description='The ID of the OSS region in which the output file is stored.', example='oss-cn-hangzhou'),
            object?: string(name='Object', description='The name of the OSS object that is used as the output file.', example='example-output.flv'),
            roleArn?: string(name='RoleArn', description='The ARN of the RAM role used for delegated authorization.', example='acs:ram::<your uid>:role/<your role name>'),
          }(name='OutputFile', description='The details of the output file.'),
          priority?: string(name='Priority', description='The priority of the job in the MPS queue to which the job is added.

*   A value of **10** indicates the highest priority.
*   Default value: **6**.', example='5'),
          properties?: {
            bitrate?: string(name='Bitrate', description='The bitrate of the video.', example='1000'),
            duration?: string(name='Duration', description='The duration of the video.', example='55'),
            fileFormat?: string(name='FileFormat', description='The format of the video.', example='QuickTime / MOV'),
            fileSize?: string(name='FileSize', description='The size of the file.', example='3509895'),
            format?: {
              bitrate?: string(name='Bitrate', description='The total bitrate.', example='1000'),
              duration?: string(name='Duration', description='The total duration.', example='55'),
              formatLongName?: string(name='FormatLongName', description='The full name of the container format.', example='QuickTime / MOV'),
              formatName?: string(name='FormatName', description='The short name of the container format. Valid values: mov, mp4, m4a, 3gp, 3g2, and mj2.', example='mov'),
              numPrograms?: string(name='NumPrograms', description='The total number of program streams.', example='0'),
              numStreams?: string(name='NumStreams', description='The total number of media streams.', example='2'),
              size?: string(name='Size', description='The size of the file.', example='3509895'),
              startTime?: string(name='StartTime', description='The start time.', example='0.000000'),
            }(name='Format', description='The format information.'),
            fps?: string(name='Fps', description='The frame rate of the video. The value is a number.', example='25'),
            height?: string(name='Height', description='The height of the video.', example='720'),
            streams?: {
              audioStreamList?: {
                audioStream?: [ 
                {
                  bitrate?: string(name='Bitrate', description='The bitrate of the audio stream.', example='128.806'),
                  channelLayout?: string(name='ChannelLayout', description='The output layout of the sound channels.', example='stereo'),
                  channels?: string(name='Channels', description='The number of sound channels.', example='2'),
                  codecLongName?: string(name='CodecLongName', description='The full name of the codec.', example='AAC (Advanced Audio Coding)'),
                  codecName?: string(name='CodecName', description='The short name of the codec.', example='aac'),
                  codecTag?: string(name='CodecTag', description='The tag of the codec.', example='0x6134706d'),
                  codecTagString?: string(name='CodecTagString', description='The tag string of the codec.', example='mp4a'),
                  codecTimeBase?: string(name='CodecTimeBase', description='The codec time base.', example='1/44100'),
                  duration?: string(name='Duration', description='The duration of the audio stream.', example='17.159546'),
                  index?: string(name='Index', description='The sequence number of the audio stream. The value indicates the position of the audio stream in all audio streams.', example='1'),
                  lang?: string(name='Lang', description='The language of the audio stream. For more information, see [FFmpeg documentation](https://www.ffmpeg.org/ffmpeg-all.html#Metadata) and [ISO 639](https://en.wikipedia.org/wiki/List_of_ISO\\_639-1\\_codes).', example='eng'),
                  numFrames?: string(name='NumFrames', description='The total number of frames.', example='25'),
                  sampleFmt?: string(name='SampleFmt', description='The sampling format.', example='fltp'),
                  samplerate?: string(name='Samplerate', description='The sampling rate of the audio stream.', example='44100'),
                  startTime?: string(name='StartTime', description='The start time of the audio stream.', example='0.000000'),
                  timebase?: string(name='Timebase', description='The time base of the audio stream.', example='1/44100'),
                }
              ](name='AudioStream')
              }(name='AudioStreamList', description='The audio streams.'),
              subtitleStreamList?: {
                subtitleStream?: [ 
                {
                  index?: string(name='Index', description='The sequence number of the subtitle stream. The value indicates the position of the subtitle stream in all subtitle streams.', example='1'),
                  lang?: string(name='Lang', description='The language of the subtitle stream. For more information, see [FFmpeg documentation](https://www.ffmpeg.org/ffmpeg-all.html#Metadata) and [ISO 639](https://en.wikipedia.org/wiki/List_of_ISO\\_639-1\\_codes).', example='eng'),
                }
              ](name='SubtitleStream')
              }(name='SubtitleStreamList', description='The subtitle streams.'),
              videoStreamList?: {
                videoStream?: [ 
                {
                  avgFPS?: string(name='AvgFPS', description='The average frame rate of the video stream.', example='23.976025'),
                  bitrate?: string(name='Bitrate', description='The bitrate of the video stream.', example='1496.46'),
                  codecLongName?: string(name='CodecLongName', description='The full name of the codec.', example='H.264 / AVC / MPEG-4 AVC / MPEG-4 part 10'),
                  codecName?: string(name='CodecName', description='The short name of the codec.', example='h264'),
                  codecTag?: string(name='CodecTag', description='The tag of the codec.', example='0x31637661'),
                  codecTagString?: string(name='CodecTagString', description='The tag string of the codec.', example='avc1'),
                  codecTimeBase?: string(name='CodecTimeBase', description='The codec time base.', example='1001/48000'),
                  dar?: string(name='Dar', description='The display aspect ratio (DAR) of the video stream.', example='16:9'),
                  duration?: string(name='Duration', description='The duration of the video stream.', example='17.225542'),
                  fps?: string(name='Fps', description='The frame rate of the video stream.', example='23.976025'),
                  hasBFrames?: string(name='HasBFrames', description='Indicates whether the video stream contains B-frames.', example='2'),
                  height?: string(name='Height', description='The height of the video stream in pixels.', example='720'),
                  index?: string(name='Index', description='The sequence number of the video stream. The value indicates the position of the video stream in all video streams.', example='0'),
                  lang?: string(name='Lang', description='The language of the video stream. For more information, see [FFmpeg documentation](https://www.ffmpeg.org/ffmpeg-all.html#Metadata) and [ISO 639](https://en.wikipedia.org/wiki/List_of_ISO\\_639-1\\_codes).', example='eng'),
                  level?: string(name='Level', description='The codec level.', example='51'),
                  networkCost?: {
                    avgBitrate?: string(name='AvgBitrate', description='The average bitrate of the video stream.', example='100'),
                    costBandwidth?: string(name='CostBandwidth', description='The maximum bandwidth that was consumed.', example='10'),
                    preloadTime?: string(name='PreloadTime', description='The amount of time consumed to preload the video stream.', example='8'),
                  }(name='NetworkCost', description='The network bandwidth that was consumed.'),
                  numFrames?: string(name='NumFrames', description='The total number of frames.', example='25'),
                  pixFmt?: string(name='PixFmt', description='The pixel format of the video stream.', example='yuv420p'),
                  profile?: string(name='Profile', description='The codec profile.', example='high'),
                  sar?: string(name='Sar', description='The sample aspect ratio (SAR) of the video stream.', example='1:1'),
                  startTime?: string(name='StartTime', description='The start time of the video stream.', example='0.000000'),
                  timebase?: string(name='Timebase', description='The time base of the video stream.', example='1/24000'),
                  width?: string(name='Width', description='The width of the video stream in pixels.', example='1280'),
                }
              ](name='VideoStream')
              }(name='VideoStreamList', description='The video streams.'),
            }(name='Streams', description='The stream information.'),
            width?: string(name='Width', description='The width of the video.', example='1280'),
          }(name='Properties', description='The media properties.'),
          rotate?: string(name='Rotate', description='The rotation angle of the video, in the clockwise direction.', example='180'),
          subtitleConfig?: {
            extSubtitleList?: {
              extSubtitle?: [ 
              {
                charEnc?: string(name='CharEnc', description='The character set used by the external subtitle.

*   Valid values: **UTF-8**, **GBK**, **BIG5**, and **auto**.
*   Default value: **auto**.

>  If this parameter is set to **auto**, the detected character set may not be the actual character set. We recommend that you set this parameter to another value.', example='UTF-8'),
                fontName?: string(name='FontName', description='The font of the hardcoded subtitles converted from external subtitles. Default value: **SimSun**. For more information, see [Fonts](~~59950~~).', example='"WenQuanYi Zen Hei", "Yuanti SC Regular", "SimSun"'),
                input?: {
                  bucket?: string(name='Bucket', description='The name of the OSS bucket in which the input file is stored.', example='example-bucket-****'),
                  location?: string(name='Location', description='The ID of the OSS region in which the input file is stored.', example='oss-cn-hangzhou'),
                  object?: string(name='Object', description='The name of the OSS object that is used as the input file.', example='example-output.flv'),
                }(name='Input', description='The OSS object that is used as the external subtitle. The value is a JSON object. Files in the **SRT** or **ASS** format are supported.'),
              }
            ](name='ExtSubtitle')
            }(name='ExtSubtitleList', description='The external subtitles. The value is a JSON array that contains up to **four** objects.'),
            subtitleList?: {
              subtitle?: [ 
              {
                map?: string(name='Map', description='The audio track. Format: `0:{Stream}:{Stream sequence number}`, which is `0:a:{audio_index}`. The value of Stream is a, which indicates an audio stream. The sequence number is the index of the audio stream in the list and starts from 0.', example='0:a:0'),
              }
            ](name='Subtitle')
            }(name='SubtitleList', description='The subtitles.'),
          }(name='SubtitleConfig', description='The subtitle configurations.'),
          superReso?: {
            isHalfSample?: string(name='IsHalfSample', description='Indicates whether parameters related to the sampling rate are obtained. Valid values:

*   **true**
*   **false**', example='true'),
          }(name='SuperReso', description='The configurations for using the resolution of the source video.'),
          tailSlateList?: {
            tailSlate?: [ 
            {
              bgColor?: string(name='BgColor', description='The color of the bars that are added to the ending part if the size of the ending part is smaller than that of the main part. Default value: **White**. For more information, see [Background colors](https://docs-aliyun.cn-hangzhou.oss.aliyun-inc.com/assets/attach/29253/cn_zh/1502784952344/color.txt?spm=a2c4g.11186623.2.63.241240f77qp3Yy\\&file=color.txt).', example='White'),
              blendDuration?: string(name='BlendDuration', description='The duration of the transition between the main part and the ending part. A fade transition is used: The last frame of the main part fades out, and the first frame of the ending part fades in. Unit: seconds. Default value: **0**.', example='2'),
              height?: string(name='Height', description='The height of the ending part.

*   Valid values: values in the range of **(0,4096)**, **-1**, and **full**.
*   A value of **-1** indicates that the height of the source of the ending part is retained. A value of **full** indicates that the height of the main part is used for the ending part.
*   Default value: -1.', example='1080'),
              isMergeAudio?: boolean(name='IsMergeAudio', description='Indicates whether the audio content of the ending part is merged. Valid values:

*   **true**
*   **false**', example='false'),
              start?: string(name='Start', description='The start time.', example='1'),
              tailUrl?: string(name='TailUrl', description='The OSS URL of the ending part.', example='http://example-bucket-****.oss-cn-hangzhou.aliyuncs.com/opening_01.flv'),
              width?: string(name='Width', description='The width of the ending part.

*   Valid values: values in the range of **(0,4096)**, **-1**, and **full**.
*   A value of **-1** indicates that the width of the source of the ending part is retained. A value of **full** indicates that the width of the main part is used for the ending part.
*   Default value: -1.', example='1920'),
            }
          ](name='TailSlate')
          }(name='TailSlateList', description='The ending parts. The value is a JSON object.'),
          templateId?: string(name='TemplateId', description='The ID of the transcoding template.', example='S00000000-000010'),
          transConfig?: {
            adjDarMethod?: string(name='AdjDarMethod', description='The method of resolution adjustment. Default value: **none**. Valid values:

*   rescale: The video image is resized.
*   crop: The video image is cropped.
*   pad: The video image is scaled out to fill the view.
*   none: The resolution is not adjusted.', example='crop'),
            isCheckAudioBitrate?: string(name='IsCheckAudioBitrate', description='Indicates whether the audio bitrate is checked. If the bitrate of the output audio is higher than that of the input audio, the input bitrate is retained and the specified audio bitrate does not take effect. This parameter has a lower priority than IsCheckAudioBitrateFail. Valid values:

*   **true**

*   **false**

*   Default value:

    *   If this parameter is empty and the codec of the output audio is different from the codec of the input audio, the default value is false.
    *   If this parameter is empty and the codec of the output audio is the same as the codec of the input audio, the default value is true.', example='false'),
            isCheckAudioBitrateFail?: string(name='IsCheckAudioBitrateFail', description='Indicates whether the audio bitrate is checked. This parameter has a higher priority than **IsCheckAudioBitrate**. If the bitrate of the output audio is higher than that of the input audio, a transcoding failure is returned without transcoding the audio. Valid values:

*   **false**
*   **true**', example='true'),
            isCheckReso?: string(name='IsCheckReso', description='Indicates whether the resolution is checked. If the output resolution is higher than the input resolution based on the width or height, the input resolution is retained. Valid values:

*   **true**
*   **false**
*   Default value: **false**.', example='false'),
            isCheckResoFail?: string(name='IsCheckResoFail', description='Indicates whether the resolution is checked. This parameter has a higher priority than IsCheckReso. If the output resolution is higher than the input resolution based on the width or height, a transcoding failure is returned without transcoding the video. Valid values:

*   **true**
*   **false**
*   Default value: **false**.', example='false'),
            isCheckVideoBitrate?: string(name='IsCheckVideoBitrate', description='Indicates whether the video bitrate is checked. If the bitrate of the output video is higher than that of the input video, the input bitrate is retained. Valid values:

*   **true**
*   **false**
*   Default value: **false**.', example='false'),
            isCheckVideoBitrateFail?: string(name='IsCheckVideoBitrateFail', description='Indicates whether the video bitrate is checked. If the bitrate of the output video is higher than that of the input video, a transcoding failure is returned without transcoding the video. This parameter has a higher priority than**IsCheckVideoBitrate**. Valid values:

*   **true**
*   **false**
*   Default value: **false**.', example='true'),
            transMode?: string(name='TransMode', description='The transcoding mode. Valid values:

*   **onepass**: transcoding based on one-pass algorithms, which has higher accuracy.
*   **twopass**: transcoding based on two-pass algorithms, which has lower accuracy.
*   **CBR**: transcoding based on a fixed bitrate.', example='onepass'),
          }(name='TransConfig', description='The general transcoding configurations.

>  If this parameter is specified in the request, the corresponding parameter in the specified transcoding template are overwritten.'),
          userData?: string(name='UserData', description='The custom data.', example='example data'),
          video?: {
            bitrate?: string(name='Bitrate', description='The bitrate of the output video. Unit: Kbit/s.', example='10'),
            bitrateBnd?: {
              max?: string(name='Max', description='The upper limit of the total bitrate. Unit: Kbit/s.', example='20'),
              min?: string(name='Min', description='The lower limit of the total bitrate. Unit: Kbit/s.', example='10'),
            }(name='BitrateBnd', description='The average bitrate range of the video.'),
            bufsize?: string(name='Bufsize', description='The size of the buffer.

*   Unit: KB.
*   Default value: **6000**.', example='1000'),
            codec?: string(name='Codec', description='The video codec.

*   Valid values: **H.264**, **H.265**, **GIF**, and **WEBP**.
*   Default value: **H.264**.', example='H.264'),
            crf?: string(name='Crf', description='The constant rate factor.

*   If **Crf** is returned, the value of **Bitrate** is invalid.
*   Default value if the value of Codec is H.264: **23**. Default value if the value of Codec is H.265: **26**.', example='22'),
            crop?: string(name='Crop', description='The method of video cropping. Valid values:

*   **border**: Black borders are automatically detected and removed.
*   A value in the format of width:height:left:top: The video is cropped based on the custom settings.', example='1280:800:0:140'),
            degrain?: string(name='Degrain', description='The strength of the independent noise reduction algorithm.', example='5'),
            fps?: string(name='Fps', description='The frame rate.

*   Unit: frames per second.
*   Valid values: 0 to 60. The value is 60 if the frame rate of the input file exceeds 60.
*   Default value: the frame rate of the input file.', example='60'),
            gop?: string(name='Gop', description='The maximum interval between keyframes or the maximum number of frames in a frame group. Unit: seconds.

*   Default value: 10.
*   If the maximum number of frames is returned, the value does not have a unit.', example='1'),
            height?: string(name='Height', description='The height of the video.

*   Unit: pixel.
*   Default value: the height of the input video.', example='1280'),
            maxFps?: string(name='MaxFps', description='The maximum frame rate.', example='15'),
            maxrate?: string(name='Maxrate', description='The maximum bitrate of the video. Unit: Kbit/s.', example='10'),
            pad?: string(name='Pad', description='The black borders that are added to the video.

*   The value is in the width:height:left:top format.
*   Unit: pixel.', example='1280:800:0:140'),
            pixFmt?: string(name='PixFmt', description='The pixel format of the video.

*   The default pixel format can be **yuv420p** or the pixel format of the input file.

*   Valid values: standard pixel formats such as **yuv420p** and **yuvj420p**.

    **

    **Note** If a non-standard pixel format such as yuvj420p(pc, bt470bg/bt470bg/smpte170m) is used, compatibility with the pixel format must be configured. Otherwise, the transcoding job fails.', example='yuvj420p'),
            preset?: string(name='Preset', description='The preset video algorithm. Default value: **medium**. Valid values:

*   **veryfast**
*   **fast**
*   **medium**
*   **slow**
*   **slower**', example='veryfast'),
            profile?: string(name='Profile', description='The encoding profile. This parameter is returned only for the H.264 codec. Default value: **high**. Valid values:

>  If multiple definitions are involved, we recommend that you use baseline for the lowest definition to ensure normal playback on low-definition devices, and use main or high for other definitions.

*   **baseline**: applicable to mobile devices.
*   **main**: applicable to standard-definition devices.
*   **high**: applicable to high-definition devices.', example='baseline'),
            qscale?: string(name='Qscale', description='The level of quality control on the video.', example='15'),
            resoPriority?: string(name='ResoPriority', description='The priority of the resource.', example='1'),
            scanMode?: string(name='ScanMode', description='The scan mode. Valid values:

*   If this parameter is **empty**, the scan mode of the input file is used.
*   **auto**: automatic deinterlacing.
*   **progressive**: progressive scan.
*   **interlaced**: interlaced scan.
*   **By default**, this parameter is empty.

**Best practice**: Interlaced scan consumes less bandwidth than progressive scan, but the image quality is poor. Therefore, mainstream video production uses progressive scan.

*   If **progressive scan** or **interlaced scan** is used when the scan mode of the input file is neither of them, the transcoding job fails.
*   We recommend that you use **the scan mode of the input file** or **automatic deinterlacing** to improve compatibility.', example='interlaced'),
            width?: string(name='Width', description='The width of the video.

*   Unit: pixel.
*   Default value: **the width of the input video**.', example='1080'),
          }(name='Video', description='The video configurations.

>  If this parameter is specified, **AliyunVideoCodec** in the template specified by **TemplateId** is overwritten.'),
          videoStreamMap?: string(name='VideoStreamMap', description='The sequence number of the video stream.

*   Format: 0:a:{Sequence number}. Example: 0:a:0.
*   The sequence number is the index of the video stream in the list and starts from 0.
*   If no sequence number is specified, the default video stream is used.', example='0:a:0'),
          waterMarkConfigUrl?: string(name='WaterMarkConfigUrl', description='The URL of the watermark configuration file.', example='http://example.com/configure'),
          waterMarkList?: {
            waterMark?: [ 
            {
              dx?: string(name='Dx', description='The horizontal offset of the watermark image relative to the output video. If this parameter is specified in the request, the corresponding parameter in the watermark template is overwritten. Default value: 0. The value can be an integer or a decimal number.

*   An integer indicates the pixel value of the horizontal offset.

    *   Valid values: **\\[8,4096]**.
    *   Unit: pixel.

*   A decimal number indicates the ratio of the horizontal offset to the width in the output video resolution.

    *   Valid values: (0,1).
    *   The decimal number can be accurate to four decimal places, such as 0.9999. Excess digits are automatically deleted.', example='1'),
              dy?: string(name='Dy', description='The vertical offset of the watermark image relative to the output video. If this parameter is specified in the request, the corresponding parameter in the watermark template is overwritten. The value can be an integer or a decimal number.

*   An integer indicates the pixel value of the vertical offset.

    *   Valid values: **\\[8,4096]**.
    *   Unit: pixel.

*   A decimal number indicates the ratio of the vertical offset to the height in the output video resolution.

    *   Valid values: **(0,1)**.
    *   The decimal number can be accurate to four decimal places, such as 0.9999. Excess digits are automatically deleted.', example='1'),
              height?: string(name='Height', description='The height of the watermark. If this parameter is specified in the request, the corresponding parameter in the watermark template is overwritten. The value can be an integer or a decimal number.

*   An integer indicates the pixel value of the watermark height.

    *   Valid values: **\\[8,4096]**.
    *   Unit: pixel.

*   A decimal number indicates the ratio of the watermark height to the height in the output video resolution.

    *   Valid values: **(0,1)**.
    *   The decimal number can be accurate to four decimal places, such as 0.9999. Excess digits are automatically deleted.', example='1280'),
              inputFile?: {
                bucket?: string(name='Bucket', description='The name of the OSS bucket in which the input file is stored.', example='example-bucket'),
                location?: string(name='Location', description='The ID of the OSS region in which the input file is stored.', example='oss-cn-hangzhou'),
                object?: string(name='Object', description='The name of the OSS object that is used as the input file.', example='example-logo-****.png'),
              }(name='InputFile', description='The watermark input file. PNG images and MOV files are supported.'),
              referPos?: string(name='ReferPos', description='The position of the watermark. If this parameter is specified in the request, the corresponding parameter in the watermark template is overwritten. Valid values:

*   **TopRight**
*   **TopLeft**
*   **BottomRight**
*   **BottomLeft**', example='TopRight'),
              type?: string(name='Type', description='The type of the watermark. If this parameter is specified in the request, the corresponding parameter in the watermark template is overwritten. For more information, see [Parameter details](~~29253~~). Valid values:

*   **Image**
*   **Text**', example='Image'),
              waterMarkTemplateId?: string(name='WaterMarkTemplateId', description='The ID of the watermark template.', example='88c6ca184c0e47098a5b665e2a12****'),
              width?: string(name='Width', description='The width of the watermark image. If this parameter is specified in the request, the corresponding parameter in the watermark template is overwritten. The value can be an integer or a decimal number.

*   An integer indicates the pixel value of the watermark width.

    *   Valid values: **\\[8,4096]**.
    *   Unit: pixel.

*   A decimal number indicates the ratio of the watermark width to the width in the output video resolution.

    *   Valid values: **(0,1)**.
    *   The decimal number can be accurate to four decimal places, such as 0.9999. Excess digits are automatically deleted.', example='1080'),
            }
          ](name='WaterMark')
          }(name='WaterMarkList', description='The watermarks.

>  If watermarks are truncated or fail to be generated, check whether the text watermarks that you add contain special characters. If the text watermarks contain special characters, you must escape the special characters before you add the watermarks. Alternatively, you can [submit a ticket](https://workorder-intl.console.aliyun.com/?spm=5176.12246746.top-nav.dticket.68797bbcm8H408#/ticket/add/?productId=1232) to contact Alibaba Cloud customer service for compatibility processing.'),
        }(name='Output', description='The output of the job.'),
        percent?: long(name='Percent', description='The transcoding progress.', example='100'),
        pipelineId?: string(name='PipelineId', description='The ID of the MPS queue.', example='88c6ca184c0e47098a5b665e2a126797'),
        state?: string(name='State', description='The state of the job. Valid values:

*   **Submitted**
*   **TranscodeFail**', example='Submitted'),
      }(name='Job', description='The details of the job. If the job fails to be submitted, no job ID is generated.'),
      message?: string(name='Message', description='The error message returned if the job failed to be created. This parameter is not returned if the job was created.', example='The specified parameter "%s" cannot be null.'),
      success?: boolean(name='Success', description='Indicates whether the job was successful. Valid values:

*   **true**
*   **false**', example='true'),
    }
  ](name='JobResult')
  }(name='JobResultList', description='The transcoding jobs that are generated.'),
  requestId?: string(name='RequestId', description='The request ID.', example='25818875-5F78-4A45S71F6-D73936451234'),
}

model SubmitJobsResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: SubmitJobsResponseBody(name='body'),
}

/**
  * *   SubmitJobs is an asynchronous operation. After you submit transcoding jobs, the jobs are added to an MPS queue to be scheduled and run. The transcoding jobs may not have been complete when the response is returned. After you call this operation, you can call the [QueryJobList](~~602836~~) operation to query the job results. You can also associate a Message Service (MNS) queue or topic with the MPS queue to receive notifications on the jobs. For more information, see [Receive notifications](https://www.alibabacloud.com/help/zh/apsaravideo-for-media-processing/latest/receive-message-notifications).
  * *   An input file can be up to 100 GB in size. If the size of the input file exceeds this limit, the job may fail.
  * *   If you use an **intelligent preset template** to transcode an input file, you must first call the [SubmitAnalysisJob](~~29223~~) operation to submit a preset template analysis job. After the analysis job is complete, you can call the [QueryAnalysisJobList](~~29224~~)operation to obtain the available preset templates for the input file. When you submit a transcoding job, set TemplateId to the ID of an available preset template. If you specify a preset template that is not in the available preset templates, the transcoding job fails.
  * *   If you use a **static preset template** to transcode an input file, you do not need to submit a preset template analysis job.
  * *   If you want to use multiple accounts in MPS, you can create Resource Access Management (RAM) users by using your Alibaba Cloud account. For more information, see [Create a RAM user and grant permissions to the RAM user](~~42841~~). If the Alibaba Cloud account that is used to query transcoding jobs is not the one that is used to submit the transcoding jobs, no data is returned.
  * *   For information about transcoding FAQ, see [FAQ about MPS](~~38986~~).
  * ### [](#qps)QPS limits
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped, and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limits](~~342832~~).
  *
 */
async function submitJobs(request: SubmitJobsRequest): SubmitJobsResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'SubmitJobs', 'POST', '/', 'json', false, 'json', request);
}

model SubmitMediaCensorJobRequest {
  barrages?: string(name='Barrages', description='The live comments.', example='hello world', position='Query'),
  coverImages?: string(name='CoverImages', description='The OSS URL of the image file that is used as the thumbnail. To view the OSS URL of the image file, you can log on to the **MPS console** and choose **Media Management** > **Media List** in the left-side navigation pane. You can specify up to five thumbnails in a JSON array.

*   Bucket: the name of the OSS bucket that stores the input file.

*   Location: the OSS region. The OSS region must be the same as the region in which your MPS service is activated.

*   Object: the OSS object to be moderated.

    **

    **Note**The name of the object cannot start with a forward slash (/). Otherwise, the operation fails to be called.', example='[{"Bucket":"example-bucket-****","Location":"oss-cn-shanghai","Object":"example-****.jpeg"}]', position='Query'),
  description?: string(name='Description', description='The description of the video. The value can be up to 128 bytes in size.', example='example description', position='Query'),
  externalUrl?: string(name='ExternalUrl', description='The URL of the video.', example='http://www.example.com/video-****/test-****.mp4', position='Query'),
  input?: string(name='Input', description='The Object Storage Service (OSS) URL of the media file to be moderated. To view the OSS URL of the media file, you can log on to the **MPS console** and choose **Media Management** > **Media List** in the left-side navigation pane. To moderate an image file, use the `CoverImage` parameter to specify the OSS URL of the image file. The value is a JSON object. For more information, see the "Input" section of the [Parameter details](~~29253~~) topic.

*   Bucket: the name of the OSS bucket that stores the input file.

*   Location: the OSS region. The OSS region must be the same as the region in which your MPS service is activated.

*   Object: the OSS object to be moderated.

    **

    **Note**The name of the object cannot start with a forward slash (/). Otherwise, the operation fails to be called.', example='{"Bucket":"example-bucket-****","Location":"oss-cn-shanghai","Object":"example-****.flv"}', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  pipelineId?: string(name='PipelineId', description='The ID of the MPS queue. This ID can be used to associate the job with a notification method. To view the ID of the MPS queue, you can log on to the **MPS console** and choose **Global Settings** > **Pipelines** in the left-side navigation pane. An empty string ("") indicates that the default MPS queue is used to run the job. By default, an MPS queue can process a maximum of 10 concurrent content moderation jobs. To increase the limit, [submit a ticket](https://workorder-intl.console.aliyun.com/?spm=5176.12246746.top-nav.ditem-sub.35da7bbcitpQnr#/ticket/createIndex).

> MPS queues are automatically created by the system. For more information about how to query and update MPS queues, see the [UpdatePipeline](~~188374~~) topic.', example='b22c173cced04565b1f38f1ecc39****', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  title?: string(name='Title', description='The title of the video. The value can be up to 64 bytes in size.', example='Hello World', position='Query'),
  userData?: string(name='UserData', description='The custom data. The value can be up to 128 bytes in size.', example='UserDatatestid-001-****', position='Query'),
  videoCensorConfig: string(name='VideoCensorConfig', description='The video moderation configurations and the OSS URLs of the output snapshots. To view the OSS URL of the media file, you can log on to the **MPS console** and choose **Media Management** > **Media List** in the left-side navigation pane.

The value is a JSON object.

*   OutputFile:

    *   Bucket: the name of the OSS bucket that stores the output file.
    *   Location: the OSS region. The OSS region must be the same as the region in which your MPS service is activated.
    *   Object: the OSS object to be generated. In the value, {Count} indicates the sequence number of the frame snapshot.

*   StoreVideoTimeline: specifies whether to generate the `{jobId}.video_timeline` file. The file is stored in OSS. A value of true indicates that the file is generated. A value of false indicates that the file is not generated. If you do not specify this parameter, the file is not generated by default. For more information about the format of the file, see the "VideoTimelines" parameter in the [QueryMediaCensorJobDetail](~~91779~~) topic.

*   SaveType: the output mode. A value of abnormal indicates that snapshots are generated only for illegal frames. A value of all indicates that snapshots are generated for all frames.

*   Biztype: the moderation template. If you do not specify this parameter or set the value to common, the default template is used. You can submit a ticket to create a custom moderation template. Then, set this parameter to your user ID to use the custom moderation template.

*   Scenes: the moderation scenarios. You can specify the moderation scenarios that you want to use. If you do not specify this parameter, the terrorism and porn moderation scenarios are used by default. Valid values:

    *   porn: pornographic content detection
    *   terrorism: terrorist content detection
    *   ad: ad violation detection
    *   live: undesirable scene detection
    *   logo: special logo detection
    *   audio: audio anti-spam

> If the input file contains audio tracks and the audio moderation scenario is specified, the audio tracks are moderated. If the input file does not contain audio tracks, you do not need to specify the audio moderation scenario.', example='{"Scenes" : ["porn"], "OutputFile":{"Bucket": "example-001","Location": "oss-cn-hangzhou","Object": "test/example-{Count}.jpg"},"SaveType" : "abnormal","BizType":"common"}', position='Query'),
}

model SubmitMediaCensorJobResponseBody = {
  jobId?: string(name='JobId', description='The ID of the content moderation job. We recommend that you keep this ID for subsequent operation calls.', example='88c6ca184c0e47098a5b665e2****'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='25818875-5F78-4A13-BEF6-D7393642CA58'),
}

model SubmitMediaCensorJobResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: SubmitMediaCensorJobResponseBody(name='body'),
}

/**
  * The job that you submit by calling this operation is run in asynchronous mode. The job is added to an ApsaraVideo Media Processing (MPS) queue and then scheduled, queued, and run. You can call the [QueryMediaCensorJobDetail](~~91779~~) operation or configure an asynchronous notification to obtain the job result.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function submitMediaCensorJob(request: SubmitMediaCensorJobRequest): SubmitMediaCensorJobResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'SubmitMediaCensorJob', 'POST', '/', 'json', false, 'json', request);
}

model SubmitMediaInfoJobRequest {
  async?: boolean(name='Async', description='Specifies whether to enable the asynchronous mode for the job. We recommend that you set this parameter to true. Valid values:

*   **true**: enables the asynchronous mode.
*   **false**: does not enable the asynchronous mode.', example='true', position='Query'),
  input: string(name='Input', description='The information about the input media file. The value is a JSON string. You must perform the following operations to add the OSS bucket in which the input media file is stored as a media bucket: Log on to the **MPS console**, choose **Workflows** > **Media Buckets** in the left-side navigation pane, and then click **Add Bucket**. After you add the OSS bucket as a media bucket, you must perform URL encoding for the OSS object. For example, `{"Bucket":"example-bucket","Location":"example-location","Object":"example%2Fexample.flv"}` indicates the `example-bucket.example-location.aliyuncs.com/example/example.flv` file.

> The OSS bucket must reside in the same region as your MPS service.', example='{"Bucket":"example-bucket","Location":"example-location","Object":"example%2Fexample.flv"}', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  pipelineId?: string(name='PipelineId', description='The ID of the MPS queue to which the analysis job is submitted. To view the ID of the MPS queue, log on to the **MPS console** and choose **Global Settings** > **Pipelines** in the left-side navigation pane.', example='88c6ca184c0e432bbf5b665e2a15****', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  userData?: string(name='UserData', description='The custom data. The custom data can contain letters, digits, and hyphens (-), and can be up to 1,024 bytes in length. The custom data cannot start with a special character.', example='testid-001', position='Query'),
}

model SubmitMediaInfoJobResponseBody = {
  mediaInfoJob?: {
    async?: boolean(name='Async', description='Indicates whether the job is run in asynchronous mode.', example='true'),
    code?: string(name='Code', description='The error code returned if the job fails.', example='InvalidParameter.JsonObjectFormatInvalid'),
    creationTime?: string(name='CreationTime', description='The time when the job was created.', example='2014-01-10T12:00:00Z'),
    input?: {
      bucket?: string(name='Bucket', description='The name of the OSS bucket in which the input media file is stored.', example='example-bucket'),
      location?: string(name='Location', description='The region in which the OSS bucket that stores the input media file resides.', example='example-location'),
      object?: string(name='Object', description='The name of the OSS object that is used as the input media file.', example='example.flv'),
    }(name='Input', description='The information about the input media file.'),
    jobId?: string(name='JobId', description='The ID of the job.', example='23ca1d184c0e4341e5b665e2a12****'),
    MNSMessageResult?: {
      errorCode?: string(name='ErrorCode', description='The error code that is returned if the job fails. This parameter is not returned if the job is successful.', example='The parameter \\"Input\\" does not conform to the JSON Object specification'),
      errorMessage?: string(name='ErrorMessage', description='The error message that is returned if the job fails. This parameter is not returned if the job is successful.', example='InvalidParameter.JsonObjectFormatInvalid'),
      messageId?: string(name='MessageId', description='The ID of the message that is returned if the job is successful. This parameter is not returned if the job fails.', example='16f01ad6175e4230ac42bb5182cd****'),
    }(name='MNSMessageResult', description='The message sent by Message Service (MNS) to notify users of the job result.'),
    message?: string(name='Message', description='The error message returned if the job fails.', example='The parameter ”*” does not conform to the JSON Object specification'),
    pipelineId?: string(name='PipelineId', description='The ID of the MPS queue to which the analysis job is submitted.', example='88c6ca184c0e432bbf5b665e2a15****'),
    properties?: {
      bitrate?: string(name='Bitrate', description='The bitrate. Unit: Kbit/s.', example='1630.045'),
      duration?: string(name='Duration', description='The duration of the input media file. Unit: seconds.', example='17.226000'),
      fileFormat?: string(name='FileFormat', description='The format of the input media file.', example='QuickTime/MOV'),
      fileSize?: string(name='FileSize', description='The size of the file. Unit: bytes.', example='3509895'),
      format?: {
        bitrate?: string(name='Bitrate', description='The total bitrate. Unit: Kbit/s.', example='1630.045'),
        duration?: string(name='Duration', description='The duration of the input media file. Unit: seconds.', example='17.226000'),
        formatLongName?: string(name='FormatLongName', description='The full name of the container format.', example='QuickTime/MOV'),
        formatName?: string(name='FormatName', description='The short name of the container format. For more information about the parameters, see [Parameter details](https://www.alibabacloud.com/help/zh/apsaravideo-for-media-processing/latest/parameter-details-a).', example='mov'),
        numPrograms?: string(name='NumPrograms', description='The total number of program streams.', example='0'),
        numStreams?: string(name='NumStreams', description='The total number of media streams.', example='2'),
        size?: string(name='Size', description='The size of the file. Unit: bytes.', example='3509895'),
        startTime?: string(name='StartTime', description='The start time.', example='0.000000'),
      }(name='Format', description='The format information.'),
      fps?: string(name='Fps', description='The frame rate.', example='25'),
      height?: string(name='Height', description='The height of the video. Unit: pixel.', example='1080'),
      streams?: {
        audioStreamList?: {
          audioStream?: [ 
          {
            bitrate?: string(name='Bitrate', description='The bitrate. Unit: Kbit/s.', example='128.806'),
            channelLayout?: string(name='ChannelLayout', description='The output layout of the sound channels.', example='stereo'),
            channels?: string(name='Channels', description='The number of sound channels.', example='2'),
            codecLongName?: string(name='CodecLongName', description='The full name of the encoding format.', example='AAC (Advanced Audio Coding)'),
            codecName?: string(name='CodecName', description='The short name of the encoding format. Default value: acc. Valid values:

*   **acc**
*   **mp3**
*   **mp4**
*   **ogg**
*   **flac**', example='aac'),
            codecTag?: string(name='CodecTag', description='The tag of the encoding format.', example='0x6134706d'),
            codecTagString?: string(name='CodecTagString', description='The tag string of the encoding format.', example='mp4a'),
            codecTimeBase?: string(name='CodecTimeBase', description='The codec time base.', example='1/44100'),
            duration?: string(name='Duration', description='The duration of the audio stream. Unit: seconds.', example='17.159546'),
            index?: string(name='Index', description='The sequence number of the audio stream. The value indicates the position of the audio stream in all audio streams.', example='1'),
            lang?: string(name='Lang', description='The language.', example='eng'),
            numFrames?: string(name='NumFrames', description='The total number of frames.', example='123'),
            sampleFmt?: string(name='SampleFmt', description='The sampling format.', example='fltp'),
            samplerate?: string(name='Samplerate', description='The sampling rate. Unit: Hz.', example='44100'),
            startTime?: string(name='StartTime', description='The start time of the audio stream.', example='0.000000'),
            timebase?: string(name='Timebase', description='The time base.', example='1/44100'),
          }
        ](name='AudioStream')
        }(name='AudioStreamList', description='The audio streams. A media file can contain up to four audio streams.'),
        subtitleStreamList?: {
          subtitleStream?: [ 
          {
            codecLongName?: string(name='CodecLongName', description='The full name of the encoding format.', example='ASS (Advanced SSA) subtitle'),
            codecName?: string(name='CodecName', description='The short name of the encoding format. Valid values:

*   **srt**
*   **ass**', example='ass'),
            codecTag?: string(name='CodecTag', description='The tag of the encoding format.', example='0x0000'),
            codecTagString?: string(name='CodecTagString', description='The tag string of the encoding format.', example='[0][0][0][0]'),
            codecTimeBase?: string(name='CodecTimeBase', description='The codec time base.', example='0/1'),
            duration?: string(name='Duration', description='The duration of the audio stream. Unit: seconds.', example='1370.116000'),
            index?: string(name='Index', description='The sequence number of the subtitle stream. The value indicates the position of the subtitle stream in all subtitle streams.', example='3'),
            lang?: string(name='Lang', description='The language.', example='eng'),
            startTime?: string(name='StartTime', description='The start time of the subtitle stream.', example='0.000000'),
            timebase?: string(name='Timebase', description='The time base.', example='1/1000'),
          }
        ](name='SubtitleStream')
        }(name='SubtitleStreamList', description='The subtitle streams. A media file can contain up to four subtitle streams.'),
        videoStreamList?: {
          videoStream?: [ 
          {
            avgFPS?: string(name='AvgFPS', description='The average frame rate.', example='23.976025'),
            bitrate?: string(name='Bitrate', description='The bitrate. Unit: Kbit/s.', example='1496.46'),
            codecLongName?: string(name='CodecLongName', description='The full name of the encoding format.', example='H.264/AVC/MPEG-4 AVC/MPEG-4 part 10'),
            codecName?: string(name='CodecName', description='The short name of the encoding format. Valid values:

*   **h264**
*   **h265**
*   **gif**
*   **webp**', example='h264'),
            codecTag?: string(name='CodecTag', description='The tag of the encoding format.', example='0x31637661'),
            codecTagString?: string(name='CodecTagString', description='The tag string of the encoding format.', example='avc1'),
            codecTimeBase?: string(name='CodecTimeBase', description='The codec time base.', example='1001/48000'),
            colorPrimaries?: string(name='ColorPrimaries', description='The level of color reconstruction.', example='700'),
            colorRange?: string(name='ColorRange', description='The color range.', example='700'),
            colorTransfer?: string(name='ColorTransfer', description='The color channel.', example='R255 G83 B170'),
            dar?: string(name='Dar', description='The display aspect ratio (DAR). DAR is the proportional relationship between the width and the height of a video. The value is used to determine whether the video is in portrait mode or landscape mode.', example='16:9'),
            duration?: string(name='Duration', description='The duration of the video stream. Unit: seconds.', example='17.225542'),
            fps?: string(name='Fps', description='The frame rate.', example='25'),
            hasBFrames?: string(name='HasBFrames', description='Indicates whether the video stream contains bidirectional frames (B-frames). A value of 1 indicates that the video stream contains B-frames. A value of 0 indicates that the video stream does not contain B-frames.', example='0'),
            height?: string(name='Height', description='The height of the video. Unit: pixel.', example='1080'),
            index?: string(name='Index', description='The sequence number of the video stream. The value indicates the position of the video stream in all video streams. The sequence number of the first video stream to be played can be specified in some players. Default value: 1.', example='1'),
            lang?: string(name='Lang', description='The language.', example='eng'),
            level?: string(name='Level', description='The codec level.', example='41'),
            networkCost?: {
              avgBitrate?: string(name='AvgBitrate', description='The average bitrate. Unit: Kbit/s.', example='300.34'),
              costBandwidth?: string(name='CostBandwidth', description='The maximum bandwidth that is consumed.', example='10'),
              preloadTime?: string(name='PreloadTime', description='The time consumed to preload the video.', example='8'),
            }(name='NetworkCost', description='The network bandwidth that is consumed.'),
            numFrames?: string(name='NumFrames', description='The total number of frames.', example='100'),
            pixFmt?: string(name='PixFmt', description='The pixel format.', example='yuv420p'),
            profile?: string(name='Profile', description='The codec profile.', example='High'),
            rotate?: string(name='Rotate', description='The rotation angle of the video.', example='90'),
            sar?: string(name='Sar', description='The sample aspect ratio (SAR).', example='1:1'),
            startTime?: string(name='StartTime', description='The start time of the video stream.', example='0.042000'),
            timebase?: string(name='Timebase', description='The time base.', example='1/24000'),
            width?: string(name='Width', description='The width of the video. Unit: pixel.', example='1920'),
          }
        ](name='VideoStream')
        }(name='VideoStreamList', description='The video streams. A media file can contain up to four video streams.'),
      }(name='Streams', description='The media streams that are contained in the input media file.'),
      width?: string(name='Width', description='The width of the video. Unit: pixel.', example='1920'),
    }(name='Properties', description='The properties of the input media file.'),
    state?: string(name='State', description='The status of the job. Valid values:

*   **Success**: The job is successful.
*   **Fail**: The job fails.
*   **Analyzing**: The job is being run.', example='Analyzing'),
    userData?: string(name='UserData', description='The custom data.', example='testid-001'),
  }(name='MediaInfoJob', description='The details of the media information analysis job.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='62D9BE16-B7D5-550C-A482-7A0F60E09877'),
}

model SubmitMediaInfoJobResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: SubmitMediaInfoJobResponseBody(name='body'),
}

/**
  * After you call the SubmitMediaInfoJob operation, ApsaraVideo Media Processing (MPS) analyzes the input media file and generates the analysis results. You can call the [QueryMediaInfoJobList](~~29221~~) operation to query the analysis results.
  * > We recommend that you submit a media information analysis job after you confirm that the media file is uploaded to Object Storage Service (OSS). You can configure upload callbacks to be notified of the upload status of files.
  * ### QPS limit
  * You can call this operation up to 50 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function submitMediaInfoJob(request: SubmitMediaInfoJobRequest): SubmitMediaInfoJobResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'SubmitMediaInfoJob', 'POST', '/', 'json', false, 'json', request);
}

model SubmitSmarttagJobRequest {
  content?: string(name='Content', example='example content ****', position='Query'),
  contentAddr?: string(name='ContentAddr', example='http://exampleBucket.oss-cn-shanghai.aliyuncs.com/mps-test/ai-tag.mp4', position='Query'),
  contentType?: string(name='ContentType', example='application/zip', position='Query'),
  input?: string(name='Input', example='oss://mybucket-****/example-****.mp4', position='Query'),
  notifyUrl?: string(name='NotifyUrl', example='https://example.com/endpoint/aliyun/ai?id=76401125000***', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  params?: string(name='Params', example='false', position='Query'),
  pipelineId: string(name='PipelineId', example='2', position='Query'),
  priority?: string(name='Priority', description='The priority of the job in the ApsaraVideo Media Processing (MPS) queue to which the job is added. Valid values: 0 to 9. Default value: 5.', example='5', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  templateId?: string(name='TemplateId', description='The template ID, which is used to specify the analysis algorithm of the smart tagging job. For more information about how to manage templates, see [AddSmarttagTemplate](~~602910~~), [QuerySmarttagTemplateList](~~187770~~), [UpdateSmarttagTemplate](~~187776~~), and [DeleteSmarttagTemplate](~~187775~~).', example='39f8e0bc005e4f309379701645f4****', position='Query'),
  title: string(name='Title', example='example-title-****', position='Query'),
  userData?: string(name='UserData', example='{"key":"value"}', position='Query'),
}

model SubmitSmarttagJobResponseBody = {
  jobId?: string(name='JobId', example='39f8e0bc005e4f309379701645f4****'),
  requestId?: string(name='RequestId', example='7B117AF5-2A16-412C-B127-FA6175ED1AD0'),
}

model SubmitSmarttagJobResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: SubmitSmarttagJobResponseBody(name='body'),
}

async function submitSmarttagJob(request: SubmitSmarttagJobRequest): SubmitSmarttagJobResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'SubmitSmarttagJob', 'POST', '/', 'json', false, 'json', request);
}

model SubmitSnapshotJobRequest {
  input: string(name='Input', description='The information about the job input. The value must be a JSON object. You must add the Object Storage Service (OSS) bucket that stores the OSS object to be used as the job input as a media bucket in the MPS console. To add an OSS bucket as a media bucket, you can log on to the MPS console, choose Workflows > Media Buckets in the left-side navigation pane, and then click Add Bucket. After the OSS bucket is added as a media bucket, you must perform URL encoding for the OSS object. Example: `{"Bucket":"example-bucket","Location":"example-location","Object":"example%2Ftest.flv"}`. This example indicates the `"example-bucket.example-location.aliyuncs.com/example/test.flv"` object.

> The OSS bucket must reside in the same region as your MPS service.', example='{"Bucket":"example-bucket","Location":"example-location","Object":"example%2Ftest.flv"}', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  pipelineId?: string(name='PipelineId', description='The ID of the MPS queue to which you want to submit the snapshot job. To obtain the ID, you can log on to the **MPS console** and choose **Global Settings** > **Pipelines** in the left-side navigation pane.

> Make sure that an available Message Service (MNS) topic is bound to the specified MPS queue. Otherwise, the relevant messages may fail to be sent as expected.', example='dd3dae411e704030b921e52698e5****', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  snapshotConfig: string(name='SnapshotConfig', description='The snapshot configurations. For more information, see the "AliyunSnapshotConfig" section of the [Data types](~~29253~~) topic.

> If you set the Interval parameter that is nested under SnapshotConfig, snapshots are captured at the specified intervals. The default value of the Interval parameter is 10, in seconds. If an input video is short but you specify large values for both the Num and Interval parameters, the actual number of snapshots captured may be smaller than the specified number. For example, if you set the Num parameter to 5 and the Interval parameter to 3 for a video of 10 seconds, the number of snapshots captured cannot reach 5.', example='{"OutputFile":{"Bucket":"example-001","Location":"example-location","Object":"{Count}.jpg"},"Time":"5","Num":"10","Interval":"20"}', position='Query'),
  userData?: string(name='UserData', description='The custom data. The custom data can contain letters, digits, and hyphens (-) and be up to 1,024 bytes in size. The custom data cannot start with a special character.', example='testid-001', position='Query'),
}

model SubmitSnapshotJobResponseBody = {
  requestId?: string(name='RequestId', description='The ID of the request.', example='19B6D8C5-A5DD-467A-B435-29D393C71E2D'),
  snapshotJob?: {
    code?: string(name='Code', description='The error code returned if the job fails. This parameter is not returned if the job is successful.', example='ResourceContentBad'),
    count?: string(name='Count', description='The number of snapshots that are captured.', example='1'),
    creationTime?: string(name='CreationTime', description='The time when the job was created.', example='2021-05-19T03:11:48Z'),
    id?: string(name='Id', description='The ID of the snapshot job.', example='f4e3b9ba9f3840c39d6e288056f0****'),
    input?: {
      bucket?: string(name='Bucket', description='The OSS bucket that stores the object.', example='example'),
      location?: string(name='Location', description='The region in which the OSS bucket resides.', example='example-location\\"'),
      object?: string(name='Object', description='The OSS object that is used as the input file.', example='example.flv'),
      roleArn?: string(name='RoleArn', description='The ARN of the specified RAM role. Format: acs:ram::$accountID:role/$roleName.', example='acs:ram::1:role/testrole'),
    }(name='Input', description='The information about the job input.'),
    MNSMessageResult?: {
      errorCode?: string(name='ErrorCode', description='The error code returned if the job fails. This parameter is not returned if the job is successful.', example='InvalidParameter'),
      errorMessage?: string(name='ErrorMessage', description='The error message returned if the job fails. This parameter is not returned if the job is successful.', example='The resource operated InputFile is bad'),
      messageId?: string(name='MessageId', description='The ID of the message. This parameter is not returned if the job fails.', example='799454621135656C7F815F198A76****'),
    }(name='MNSMessageResult', description='The message sent by MNS to notify the user of the job result.'),
    message?: string(name='Message', description='The error message returned if the job fails. This parameter is not returned if the job is successful.', example='The resource operated InputFile is bad'),
    pipelineId?: string(name='PipelineId', description='The ID of the MPS queue to which the snapshot job is submitted.', example='dd3dae411e704030b921e52698e5****'),
    snapshotConfig?: {
      frameType?: string(name='FrameType', description='The type of the snapshot. Default value: **Normal**. Valid values:

*   **normal**: normal frames.
*   **intra**: I-frames (keyframes).

> If the FrameType parameter is set to intra in the request, only keyframes are captured. If no keyframe is found at the specified point in time, the keyframe closest to the specified point in time is captured. Keyframes are captured faster than normal frames if the same snapshot rules are applied.', example='intra'),
      height?: string(name='Height', description='The height of the output snapshot.', example='8'),
      interval?: string(name='Interval', description='The interval for capturing snapshots.

*   If this parameter is specified in the request, snapshots are captured at intervals. The value must be greater than 0 in the request.
*   Unit: seconds.
*   Default value: **10**.', example='20'),
      num?: string(name='Num', description='The number of snapshots. If the Num parameter is set in the request, snapshots are captured at intervals.', example='10'),
      outputFile?: {
        bucket?: string(name='Bucket', description='The OSS bucket that stores the output snapshot.', example='example'),
        location?: string(name='Location', description='The OSS region in which the OSS bucket for storing the output snapshot resides.', example='example-location'),
        object?: string(name='Object', description='The OSS object that is generated as the output file of the snapshot job.', example='test.png'),
        roleArn?: string(name='RoleArn', description='The Alibaba Cloud Resource Name (ARN) of the specified RAM role. Format: acs:ram::$accountID:role/$roleName.', example='acs:ram::1:role/testrole'),
      }(name='OutputFile', description='The information about the output file of the snapshot job.'),
      tileOut?: {
        cellHeight?: string(name='CellHeight', description='The height of a single image. The default value is the height of the output snapshot.', example='100'),
        cellSelStep?: string(name='CellSelStep', description='The step for selecting a single image.', example='3'),
        cellWidth?: string(name='CellWidth', description='The width of a single image. The default value is the width of the output snapshot.', example='100'),
        color?: string(name='Color', description='The background color.

*   Default value: **black**.
*   You can set the Color parameter to a **color keyword** or **random** in the request.

> If you want to set the background color to black, you can specify the color keyword in one of the following three formats: Black, black, and #000000.', example='black'),
        columns?: string(name='Columns', description='The number of columns that the tiled image contains. Default value: **10**.', example='10'),
        isKeepCellPic?: string(name='IsKeepCellPic', description='Indicates whether the single images are retained. Valid values:

*   **true**: The single images are retained.
*   **false**: The single images are not retained.
*   Default value: **true**.', example='false'),
        lines?: string(name='Lines', description='The number of rows that the tiled image contains. Default value: **10**.', example='10'),
        margin?: string(name='Margin', description='The margin width of the tiled image.

*   Default value: **0**.
*   Unit: pixel.', example='5'),
        padding?: string(name='Padding', description='The distance between two consecutive single images in the tiled image.

*   Default value: **0**.
*   Unit: pixel.', example='0'),
      }(name='TileOut', description='The tiling configurations.'),
      tileOutputFile?: {
        bucket?: string(name='Bucket', description='The OSS bucket that stores the object.', example='example'),
        location?: string(name='Location', description='The ID of the region in which the OSS bucket that stores the object is located.', example='example-location'),
        object?: string(name='Object', description='The OSS object that is generated as the output file of the tiling job.', example='example.png'),
        roleArn?: string(name='RoleArn', description='The ARN of the specified RAM role. Format: acs:ram::$accountID:role/$roleName.', example='acs:ram::1:role/testrole'),
      }(name='TileOutputFile', description='The information about the output file of the tiling job.'),
      time?: string(name='Time', description='The start time for capturing snapshots. Unit: milliseconds.', example='5'),
      timeArray?: {
        timePointList?: [ long ](name='TimePointList')
      }(name='TimeArray'),
      width?: string(name='Width', description='The width of the output snapshot.', example='8'),
    }(name='SnapshotConfig', description='The snapshot configurations.'),
    state?: string(name='State', description='The status of the snapshot job. Valid values:

*   **Submitted**: The job is submitted.
*   **Snapshoting**: The job is being processed.
*   **Success**: The job is successful.
*   **Fail**: The job fails.', example='Snapshoting'),
    tileCount?: string(name='TileCount', description='The number of single images that are contained in the tiled image.', example='5'),
    userData?: string(name='UserData', description='The custom data.', example='testid-001'),
  }(name='SnapshotJob', description='The information about the snapshot job.'),
}

model SubmitSnapshotJobResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: SubmitSnapshotJobResponseBody(name='body'),
}

/**
  * *   Only JPG images can be generated by calling this operation.
  * *   Asynchronous mode: This operation may return a response before snapshots are captured. Snapshot jobs are queued in the background and asynchronously processed by ApsaraVideo Media Processing (MPS). If the **Interval** or **Num** parameter is set, the snapshot job is processed in asynchronous mode. For more information about FAQ about capturing snapshots, see [FAQ about taking snapshots](~~60805~~).
  * *   Notifications: When you submit a snapshot job, the **PipelineId** parameter is required. An asynchronous message is sent only after the notification feature is enabled for the MPS queue.
  * ### QPS limit
  * You can call this operation up to 50 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function submitSnapshotJob(request: SubmitSnapshotJobRequest): SubmitSnapshotJobResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'SubmitSnapshotJob', 'POST', '/', 'json', false, 'json', request);
}

model SubmitTraceAbJobRequest {
  callBack?: string(name='CallBack', example='http://example.com/callback', position='Query'),
  cipherBase64ed?: string(name='CipherBase64ed', example='Qh6OdgIMcliQSI1fReOw****', position='Query'),
  input?: string(name='Input', example='{"Bucket":"ivison-test","Location":"oss-cn-shanghai","Object":"test.mp4"}', position='Query'),
  level?: long(name='Level', example='2', position='Query'),
  output: string(name='Output', example='{"Bucket":"ivison-test","Location":"oss-cn-shanghai","Dir":"out/"}', position='Query'),
  startTime?: string(name='StartTime', example='0', position='Query'),
  totalTime?: string(name='TotalTime', example='360', position='Query'),
  url?: string(name='Url', example='http://www.example.com/video/test.mp4', position='Query'),
  userData?: string(name='UserData', example='123', position='Query'),
}

model SubmitTraceAbJobResponseBody = {
  data?: {
    jobId?: string(name='JobId', example='bfb786c639894f4d80648792021e****'),
    mediaId?: string(name='MediaId', example='437bd2b516ffda105d07b12a9a82****'),
  }(name='Data'),
  message?: string(name='Message', example='ok'),
  requestId?: string(name='RequestId', description='Id of the request', example='030E2671-806A-52AF-A93C-DA8E308603A6'),
  statusCode?: long(name='StatusCode', example='200'),
}

model SubmitTraceAbJobResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: SubmitTraceAbJobResponseBody(name='body'),
}

async function submitTraceAbJob(request: SubmitTraceAbJobRequest): SubmitTraceAbJobResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'SubmitTraceAbJob', 'POST', '/', 'json', false, 'json', request);
}

model SubmitTraceExtractJobRequest {
  callBack?: string(name='CallBack', example='http://example.com/callback', position='Query'),
  input?: string(name='Input', example='{"Bucket":"example","Location":"oss-cn-shanghai","Object":"example.mp4"}', position='Query'),
  params?: string(name='Params', example='{"m3u8Type":"v1"}', position='Query'),
  url?: string(name='Url', example='http://www.example.com/video/test.mp4', position='Query'),
  userData?: string(name='UserData', example='123', position='Query'),
}

model SubmitTraceExtractJobResponseBody = {
  data?: {
    jobId?: string(name='JobId', example='bfb786c639894f4d80648792021e****'),
  }(name='Data'),
  message?: string(name='Message', example='ok'),
  requestId?: string(name='RequestId', description='Id of the request', example='A1326BD4-30B1-4CB6-Q123-3330B877B0D4'),
  statusCode?: long(name='StatusCode', example='200'),
}

model SubmitTraceExtractJobResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: SubmitTraceExtractJobResponseBody(name='body'),
}

async function submitTraceExtractJob(request: SubmitTraceExtractJobRequest): SubmitTraceExtractJobResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'SubmitTraceExtractJob', 'POST', '/', 'json', false, 'json', request);
}

model SubmitTraceM3u8JobRequest {
  keyUri?: string(name='KeyUri', example='https://cipher.abc.com', position='Query'),
  mediaId?: string(name='MediaId', example='437bd2b516ffda105d07b12a9a82****', position='Query'),
  output?: string(name='Output', example='{"Bucket":"exampleBucket","Location":"oss-cn-shanghai","Object":"out.m3u8"}', position='Query'),
  params?: string(name='Params', example='{"m3u8Type":"v1"}', position='Query'),
  trace?: string(name='Trace', position='Query'),
}

model SubmitTraceM3u8JobResponseBody = {
  data?: {
    jobId?: string(name='JobId', example='bfb786c639894f4d80648792021e****'),
  }(name='Data'),
  message?: string(name='Message', example='ok'),
  requestId?: string(name='RequestId', description='Id of the request', example='DEB915C5-D001-5C17-AF65-FF8A65DFE432'),
}

model SubmitTraceM3u8JobResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: SubmitTraceM3u8JobResponseBody(name='body'),
}

async function submitTraceM3u8Job(request: SubmitTraceM3u8JobRequest): SubmitTraceM3u8JobResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'SubmitTraceM3u8Job', 'POST', '/', 'json', false, 'json', request);
}

model TagCustomPersonRequest {
  categoryDescription?: string(name='CategoryDescription', example='CategoryDescription001-****', position='Query'),
  categoryId?: string(name='CategoryId', example='CategoryId001-****', position='Query'),
  categoryName?: string(name='CategoryName', example='CategoryNametest-****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  personDescription?: string(name='PersonDescription', example='PersonDescriptiontest-****', position='Query'),
  personId?: string(name='PersonId', example='PersonId001-****', position='Query'),
  personName?: string(name='PersonName', example='PersonNametest-****', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model TagCustomPersonResponseBody = {
  requestId?: string(name='RequestId', example='FD8B5B8C-0C3D-5776-B3B1-EE6AD11F905A'),
}

model TagCustomPersonResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: TagCustomPersonResponseBody(name='body'),
}

/**
  * The response parameters.
  *
 */
async function tagCustomPerson(request: TagCustomPersonRequest): TagCustomPersonResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'TagCustomPerson', 'POST', '/', 'json', false, 'json', request);
}

model UnbindInputBucketRequest {
  bucket: string(name='Bucket', description='The name of the input media bucket to be unbound. To obtain the media bucket name, you can log on to the **ApsaraVideo Media Processing (MPS) console** and choose **Workflows** > **Media Buckets** in the left-side navigation pane. Alternatively, you can log on to the **Object Storage Service (OSS) console** and click **Historical Paths**.', example='example-bucket-****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  roleArn?: string(name='RoleArn', description='The Alibaba Cloud Resource Name (ARN) of the role used for delegated authorization.', example='acs:ram::174809843091****:role/exampleRole', position='Query'),
}

model UnbindInputBucketResponseBody = {
  requestId?: string(name='RequestId', description='The ID of the request.', example='4AEA0480-32F4-1656-92B3-F4D4CDE6BBB3'),
}

model UnbindInputBucketResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: UnbindInputBucketResponseBody(name='body'),
}

/**
  * # Usage notes
  * You can call this operation to unbind an input media bucket from the media library based on the name of the output media bucket.
  * # QPS limits
  * You can call this API operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function unbindInputBucket(request: UnbindInputBucketRequest): UnbindInputBucketResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'UnbindInputBucket', 'POST', '/', 'json', false, 'json', request);
}

model UnbindOutputBucketRequest {
  bucket: string(name='Bucket', description='The ID of the request.', example='example-bucket-****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model UnbindOutputBucketResponseBody = {
  requestId?: string(name='RequestId', description='The operation that you want to perform. Set the value to **UnbindOutputBucket**.', example='4AEA0480-32F4-1656-92B3-F4D4CDE6BBB3'),
}

model UnbindOutputBucketResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: UnbindOutputBucketResponseBody(name='body'),
}

/**
  * The name of the output media bucket to be unbound. To obtain the media bucket name, you can log on to the **ApsaraVideo Media Processing (MPS)** console and choose **Workflows** > **Media Buckets** in the left-side navigation pane. Alternatively, you can log on to the **Object Storage Service (OSS) console** and click **My OSS Paths**.
  *
 */
async function unbindOutputBucket(request: UnbindOutputBucketRequest): UnbindOutputBucketResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'UnbindOutputBucket', 'POST', '/', 'json', false, 'json', request);
}

model UnregisterCustomFaceRequest {
  categoryId: string(name='CategoryId', description='The ID of the figure library. The ID is used to uniquely identify a figure library. You can specify the ID of a custom figure library. Make sure that the ID is unique. If you set this parameter to the ID of a system figure library, the system figure library is used. The ID can be up to 120 characters in length and is not case-sensitive. You can call the [ListCustomPersons](~~187787~~) operation to query the figure library ID.', example='CategoryId001-****', position='Query'),
  faceId: string(name='FaceId', description='The ID of the face. The ID is used to uniquely identify a face. Make sure that the ID is unique. The ID can be up to 120 characters in length and is not case-sensitive. You can call the [ListCustomPersons](~~187787~~) operation to query the face ID. If you set this parameter to ALL, all the faces associated with the specified figure are deleted.', example='15****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  personId: string(name='PersonId', description='The ID of the figure. The ID is used to uniquely identify a custom figure. Make sure that the ID is unique. The ID can be up to 120 characters in length and is not case-sensitive. You can call the [ListCustomPersons](~~187787~~) operation to query the figure ID. If you set this parameter to ALL, all the faces in the specified figure library are deleted, and the custom figure library is deleted.', example='PersonId001-****', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model UnregisterCustomFaceResponseBody = {
  requestId?: string(name='RequestId', description='The ID of the request.', example='1A3347BF-7BCE-40A6-B33E-43C2B8A9A278'),
}

model UnregisterCustomFaceResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: UnregisterCustomFaceResponseBody(name='body'),
}

/**
  * You can call this operation to delete a specific custom face, all the custom faces that are registered in a custom figure library, or a custom figure library.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped, and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function unregisterCustomFace(request: UnregisterCustomFaceRequest): UnregisterCustomFaceResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'UnregisterCustomFace', 'POST', '/', 'json', false, 'json', request);
}

model UpdateMediaRequest {
  cateId?: long(name='CateId', description='The ID of the category to which the media file belongs. The value must be an integer.

*   If you do not specify this parameter, the value is NULL.
*   The value cannot be negative.', example='1', position='Query'),
  coverURL?: string(name='CoverURL', description='The URL of the thumbnail. This parameter is used to specify the storage location of the thumbnail. To obtain the URL, you can log on to the **MPS console** and choose **Workflows** > **Media Buckets** in the left-side navigation pane. Alternatively, you can log on to the **OSS console** and click **Buckets** in the left-side navigation pane.

*   The value can be up to 3,200 bytes in length.
*   The URL complies with RFC 2396 and is encoded in UTF-8, with reserved characters being percent-encoded. For more information, see [URL encoding](~~423796~~).', example='http://example-bucket-****.oss-cn-hangzhou.aliyuncs.com/test****.jpg', position='Query'),
  description?: string(name='Description', description='The description of the media file. Multiple character types, such as letters and digits, are supported.

*   If you do not specify this parameter, the value is NULL.
*   The value is encoded in UTF-8 and can be up to 1,024 bytes in length.', example='example description', position='Query'),
  mediaId: string(name='MediaId', description='The ID of the media file whose basic information you want to update. To obtain the ID of the media file, you can log on to the **ApsaraVideo Media Processing (MPS) console** and choose **Media Management** > **Media List** in the left-side navigation pane.', example='3e1cd21131a94525be55acf65888****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  tags?: string(name='Tags', description='The tags that you want to add to the media file.

*   You can specify up to 16 tags for a media file. Separate multiple tags with commas (,).
*   Each tag can be up to 32 bytes in length.
*   The value is encoded in UTF-8.', example='tag1,tag2', position='Query'),
  title?: string(name='Title', description='The title of the media file. Multiple character types, such as letters and digits, are supported.

*   If you do not specify this parameter, the value is NULL.
*   The value is encoded in UTF-8 and can be up to 128 bytes in length.', example='hello', position='Query'),
}

model UpdateMediaResponseBody = {
  media?: {
    bitrate?: string(name='Bitrate', description='The bitrate of the media file.', example='2659.326'),
    cateId?: long(name='CateId', description='The ID of the category to which the media file belongs.', example='1'),
    censorState?: string(name='CensorState', description='The review state of the media file. Valid values:

*   **Initiated**: The media file is uploaded but not reviewed.
*   **Pass**: The media file is uploaded and passes the review.', example='Initiated'),
    coverURL?: string(name='CoverURL', description='The URL of the thumbnail.', example='http://example-bucket-****.oss-cn-shanghai.aliyuncs.com/example-****.jpg'),
    creationTime?: string(name='CreationTime', description='The time when the media file was created.', example='2016-09-14T08:30:33Z'),
    description?: string(name='Description', description='The description of the media file.', example='example description'),
    duration?: string(name='Duration', description='The duration of the media file.', example='7.965000'),
    file?: {
      state?: string(name='State', description='The state of the input file. Valid values:

*   **Normal**: The input file is normal.
*   **Deleted**: The input file is deleted.', example='Normal'),
      URL?: string(name='URL', description='The name of the OSS bucket in which the input media file is stored.', example='http://example-bucket-****.oss-cn-hangzhou.aliyuncs.com//example-****.mp4'),
    }(name='File', description='The information about the input file.'),
    format?: string(name='Format', description='The format of the media file. Valid values: mov, mp4, m4a, 3gp, 3g2, and mj2.', example='mov'),
    fps?: string(name='Fps', description='The frame rate of the media file.', example='25.0'),
    height?: string(name='Height', description='The height of the media file.', example='1080'),
    mediaId?: string(name='MediaId', description='The ID of the media file.', example='3e1cd21131a94525be55acf65888****'),
    publishState?: string(name='PublishState', description='The publishing state of the media file. Valid values:

*   **Initiated**: The media file is in the initial state.
*   **UnPublish**: The media file has not been published, and the playback permission on the OSS object is Private.
*   **Published**: The media file has been published, and the playback permission on the OSS object is Default.
*   **Deleted**: The media file is deleted.', example='Published'),
    runIdList?: {
      runId?: [ string ](name='RunId')
    }(name='RunIdList', description='The IDs of the media workflow execution instances.'),
    size?: string(name='Size', description='The size of the media file.', example='2647692'),
    tags?: {
      tag?: [ string ](name='Tag')
    }(name='Tags', description='The information about the tags.'),
    title?: string(name='Title', description='The title of the media file.', example='hello'),
    width?: string(name='Width', description='The width of the media file.', example='1920'),
  }(name='Media', description='The information about the media file.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='6A88246F-C91F-42BD-BABE-DB0DF993F960'),
}

model UpdateMediaResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: UpdateMediaResponseBody(name='body'),
}

/**
  * The basic information that you can update by calling this operation includes the title, description, and category of a media file. This operation applies to a full update. You must set all the parameters unless you want to replace the value of a specific parameter with a NULL value.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function updateMedia(request: UpdateMediaRequest): UpdateMediaResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'UpdateMedia', 'POST', '/', 'json', false, 'json', request);
}

model UpdateMediaCategoryRequest {
  cateId?: long(name='CateId', description='The ID of the category. The value cannot be negative.', example='1', position='Query'),
  mediaId: string(name='MediaId', description='The ID of the media file whose category you want to update.

> To obtain the ID of a media file, you can call the [AddMedia](~~44458~~) operation. Alternatively, perform the following operations in the ApsaraVideo Media Processing (MPS) console: In the left-side navigation pane, choose **Media Management > Media List**. Find the required video and click **Manage** in the Actions column. The ID of the video is displayed on the Basics tab.', example='3e1cd21131a94525be55acf65888****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model UpdateMediaCategoryResponseBody = {
  requestId?: string(name='RequestId', description='The ID of the request.', example='E3931857-E3D3-4D6E-9C7B-D2C09441BD01'),
}

model UpdateMediaCategoryResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: UpdateMediaCategoryResponseBody(name='body'),
}

/**
  * You can call this operation to update only the category of a media file. For more information about how to update all the information about a media file, see [UpdateMedia](~~44464~~).
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function updateMediaCategory(request: UpdateMediaCategoryRequest): UpdateMediaCategoryResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'UpdateMediaCategory', 'POST', '/', 'json', false, 'json', request);
}

model UpdateMediaCoverRequest {
  coverURL?: string(name='CoverURL', description='The URL of the thumbnail that you want to specify for the media file. The URL complies with RFC 2396 and is encoded in UTF-8. The URL can be up to 3,200 bytes in length.

>  To obtain the thumbnail URL, you can find the image in the Object Storage Service (OSS) bucket and click the image to view details. In the View Details panel, copy the part before the question mark (?) from the URL field. You can enter only an HTTP URL.', example='http://example-bucket-****.oss-cn-hangzhou.aliyuncs.com//example-****.mp4', position='Query'),
  mediaId: string(name='MediaId', description='The ID of the media file whose thumbnail you want to update. To obtain the ID of a media file, you can call the [AddMedia](~~44458~~) operation. Alternatively, perform the following operations in the ApsaraVideo Media Processing (MPS) console: In the left-side navigation pane, choose **Media Management** > **Media List**. Find the required video and click **Manage**. The ID of the video is displayed on the Basics tab.', example='6cc3aa66d1cb4bb2adf14e726c0a****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model UpdateMediaCoverResponseBody = {
  requestId?: string(name='RequestId', description='The ID of the request.', example='0DC39B9E-13D4-40BA-AE76-CFF9BD64239D'),
}

model UpdateMediaCoverResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: UpdateMediaCoverResponseBody(name='body'),
}

/**
  * You can call this operation to update only the thumbnail of a media file. For more information about how to update all the information about a media file, see [UpdateMedia](~~44464~~).
  * ## Limits on QPS
  * You can call this operation up to 100 times per second. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
 */
async function updateMediaCover(request: UpdateMediaCoverRequest): UpdateMediaCoverResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'UpdateMediaCover', 'POST', '/', 'json', false, 'json', request);
}

model UpdateMediaPublishStateRequest {
  mediaId: string(name='MediaId', description='The ID of the media file whose publishing status you want to update. You can obtain the ID of a media file from the response of the [AddMedia](~~44458~~) operation. Alternatively, perform the following operations in the ApsaraVideo Media Processing (MPS) console: In the left-side navigation pane, choose **Media Management** > **Media List**. Find the required video and click **Manage**. The ID of the video is displayed on the Basics tab.', example='3e6149d5a8c944c09b1a8d2dc3e4****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  publish?: boolean(name='Publish', description='The publishing status. Default value: **Initialed**. Valid values:

*   **true**: published.
*   **false**: unpublished.', example='true', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
}

model UpdateMediaPublishStateResponseBody = {
  requestId?: string(name='RequestId', description='The ID of the request.', example='91B6CAB9-034C-4E4E-A40B-E7F5C81E688C'),
}

model UpdateMediaPublishStateResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: UpdateMediaPublishStateResponseBody(name='body'),
}

/**
  * The published state indicates that the access control list (ACL) of media playback resources and snapshot files is set to inherit the ACL of the bucket to which they belong. The unpublished state indicates that the ACL of media playback resources and snapshot files is set to private.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function updateMediaPublishState(request: UpdateMediaPublishStateRequest): UpdateMediaPublishStateResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'UpdateMediaPublishState', 'POST', '/', 'json', false, 'json', request);
}

model UpdateMediaWorkflowRequest {
  mediaWorkflowId: string(name='MediaWorkflowId', description='The ID of the media workflow that you want to update. To obtain the ID of the media workflow, you can log on to the **ApsaraVideo Media Processing (MPS) console** and choose **Workflows** > **Workflow Settings** in the left-side navigation pane.', example='6307eb0d3f85477882d205aa040f****', position='Query'),
  name?: string(name='Name', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  topology: string(name='Topology', description='The new topology of the media workflow. The value is a JSON object that contains the activity list and activity dependencies.

> The Object Storage Service (OSS) bucket must reside in the same region as your MPS service.', example='{
      "Activities": {
            "Act-Start": {
                  "Parameters": {
                        "PipelineId": "130266f58161436a80bf07cb12c8****",
                        "InputFile": "{\\"Bucket\\": \\"example-bucket-****\\",\\"Location\\": \\"cn-shanghai\\"}"
                  },
                  "Type": "Start"
            },
            "Act-Report": {
                  "Parameters": {},
                  "Type": "Report"
            },
            "Act-Transcode-M3U8": {
                  "Parameters": {
                        "Outputs": "[{\\"Object\\":\\"transcode/{ObjectPrefix}{FileName}\\",\\"TemplateId\\": \\"957d1719ee85ed6527b90cf62726****\\"}]",
                        "OutputBucket": "example-bucket-****",
                        "OutputLocation": "cn-shanghai"
                  },
                  "Type": "Transcode"
            }
      },
      "Dependencies": {
            "Act-Start": [
                  "Act-Transcode-M3U8"
            ],
            "Act-Report": [],
            "Act-Transcode-M3U8": [
                  "Act-Report"
            ]
      }
}', position='Query'),
  triggerMode?: string(name='TriggerMode', position='Query'),
}

model UpdateMediaWorkflowResponseBody = {
  mediaWorkflow?: {
    creationTime?: string(name='CreationTime', description='The time when the media workflow was created.', example='2016-04-01T05:29:38Z'),
    mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the media workflow that is updated.', example='6307eb0d3f85477882d205aa040f****'),
    name?: string(name='Name', description='The name of the media workflow.', example='examp-mediaworkflow-****'),
    state?: string(name='State', description='The state of the media workflow. Valid values:

*   **Inactive**: The media workflow is disabled.
*   **Active**: The media workflow is enabled.', example='Active'),
    topology?: string(name='Topology', description='The topology of the media workflow.', example='{
      "Activities": {
            "Act-Start": {
                  "Parameters": {
                        "PipelineId": "130266f58161436a80bf07cb12c8****",
                        "InputFile": "{\\"Bucket\\": \\"example-bucket-****\\",\\"Location\\": \\"cn-shanghai\\"}"
                  },
                  "Type": "Start"
            },
            "Act-Report": {
                  "Parameters": {},
                  "Type": "Report"
            },
            "Act-Transcode-M3U8": {
                  "Parameters": {
                        "Outputs": "[{\\"Object\\":\\"transcode/{ObjectPrefix}{FileName}\\",\\"TemplateId\\": \\"957d1719ee85ed6527b90cf62726****\\"}]",
                        "OutputBucket": "example-bucket-****",
                        "OutputLocation": "cn-shanghai"
                  },
                  "Type": "Transcode"
            }
      },
      "Dependencies": {
            "Act-Start": [
                  "Act-Transcode-M3U8"
            ],
            "Act-Report": [],
            "Act-Transcode-M3U8": [
                  "Act-Report"
            ]
      }
}'),
    triggerMode?: string(name='TriggerMode', description='The trigger mode of the media workflow. Valid values:

*   **OssAutoTrigger**: The media workflow is automatically triggered.
*   **NotInAuto**: The media workflow is not automatically triggered.', example='OssAutoTrigger'),
  }(name='MediaWorkflow', description='The detailed information about the media workflow.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='7D752035-97DA-54E5-88E2-E8405EEA****'),
}

model UpdateMediaWorkflowResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: UpdateMediaWorkflowResponseBody(name='body'),
}

/**
  * *   You can call this operation to update the topology of a media workflow. To update the trigger mode of a media workflow, call the [UpdateMediaWorkflowTriggerMode](~~70372~~) operation.
  * *   After you delete or deactivate a media workflow, the workflow cannot be used. In this case, the workflow is not automatically triggered when you upload a file to the bucket specified by the workflow.
  * <warning>Deleting or deactivating a workflow will not affect tasks that have already been submitted. If a workflow is deleted or deactivated after a task has been submitted, tasks that are already in the processing queue will not be canceled and will be executed normally and charged the corresponding fees.></warning>
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).>
  *
 */
async function updateMediaWorkflow(request: UpdateMediaWorkflowRequest): UpdateMediaWorkflowResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'UpdateMediaWorkflow', 'POST', '/', 'json', false, 'json', request);
}

model UpdateMediaWorkflowTriggerModeRequest {
  mediaWorkflowId: string(name='MediaWorkflowId', description='The ID of the media workflow that you want to update. To obtain the ID of the media workflow, you can log on to the **ApsaraVideo Media Processing (MPS) console** and choose **Workflows** > **Workflow Settings** in the left-side navigation pane.', example='e00732b977da427d9177a4dee646****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  triggerMode: string(name='TriggerMode', description='The trigger mode of the media workflow. Valid values:

*   **OssAutoTrigger**: automatically triggers the media workflow.
*   **NotInAuto**: does not automatically trigger the media workflow.', example='NotInAuto', position='Query'),
}

model UpdateMediaWorkflowTriggerModeResponseBody = {
  mediaWorkflow?: {
    creationTime?: string(name='CreationTime', description='The time when the media workflow was created.', example='2016-04-01T05:29:37Z'),
    mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the media workflow.', example='e00732b977da427d9177a4dee646****'),
    name?: string(name='Name', description='The name of the media workflow.', example='example-mediaworkflow-****'),
    state?: string(name='State', description='The status of the media workflow. Valid values:

*   **Inactive**: The media workflow is disabled.
*   **Active**: The media workflow is enabled.', example='Inactive'),
    topology?: string(name='Topology', description='The topology of the media workflow.', example='{mediaworkflow","State":"Active","Topology":"{\\"Activities\\":{\\"Act-Start\\":{\\"Parameters\\":{\\"PipelineId\\":\\"130266f58161436a80bf07cb12c8****\\",\\"InputFile\\":\\"{\\\\\\"Bucket\\\\\\": \\\\\\"example-bucket-****\\\\\\",\\\\\\"Location\\\\\\": \\\\\\"cn-shanghai\\\\\\"}\\"},\\"Type\\":\\"Start\\"},\\"Act-Report\\":{\\"Parameters\\":{},\\"Type\\":\\"Report\\"},\\"Act-Transcode-M3U8\\":{\\"Parameters\\":{\\"Outputs\\":\\"[{\\\\\\"Object\\\\\\":\\\\\\"transcode/{ObjectPrefix}{FileName}\\\\\\",\\\\\\"TemplateId\\\\\\": \\\\\\"957d1719ee85ed6527b90cf62726****\\\\\\"}]\\",\\"OutputBucket\\":\\"example-bucket-****\\",\\"OutputLocation\\":\\"cn-shanghai\\"},\\"Type\\":\\"Transcode\\"}},\\"Dependencies\\":{\\"Act-Start\\":[\\"Act-Transcode-M3U8\\"],\\"Act-Report\\":[],\\"Act-Transcode-M3U8\\":[\\"Act-Report\\"]}}","MediaWorkflowId":"93ab850b4f6f44eab54b6e91d24d****"}]},"RequestId":"16CD0CDD-457E-420D-9755-8385075A1234"}'),
    triggerMode?: string(name='TriggerMode', description='The trigger mode of the media workflow. Valid values:

*   **OssAutoTrigger**: The media workflow is automatically triggered.
*   **NotInAuto**: The media workflow is not automatically triggered.', example='NotInAuto'),
  }(name='MediaWorkflow', description='The information about the media workflow.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='16CD0CDD-457E-420D-9755-8385075A1234'),
}

model UpdateMediaWorkflowTriggerModeResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: UpdateMediaWorkflowTriggerModeResponseBody(name='body'),
}

/**
  * You can call this operation only to modify the trigger mode of a media workflow. To modify other information about the workflow, call the [UpdateMediaWorkflow](~~44438~~) operation.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function updateMediaWorkflowTriggerMode(request: UpdateMediaWorkflowTriggerModeRequest): UpdateMediaWorkflowTriggerModeResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'UpdateMediaWorkflowTriggerMode', 'POST', '/', 'json', false, 'json', request);
}

model UpdatePipelineRequest {
  extendConfig?: string(name='ExtendConfig', position='Query'),
  name: string(name='Name', description='The new name of the MPS queue. The value can contain letters, digits, and special characters such as hyphens (-) and can be up to 128 bytes in size. The value cannot start with a special character.', example='example-pipeline-****', position='Query'),
  notifyConfig?: string(name='NotifyConfig', description='The Message Service (MNS) configuration, such as the information about the MNS queue or topic. For more information, see the "NotifyConfig" section of the [Parameter details](~~29253~~) topic.', example='{"Topic":"example-topic-****"}', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  pipelineId: string(name='PipelineId', description='The ID of the MPS queue that you want to update. To view the MPS queue ID, log on to the **MPS console** and choose **Global Settings** > **Pipelines** in the left-side navigation pane.', example='d1ce4d3efcb549419193f50f1fcd****', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  role?: string(name='Role', description='The role that is assigned to the current RAM user. To obtain the role, you can log on to the **Resource Access Management (RAM) console** and choose **Identities** > **Roles** in the left-side navigation pane.', example='AliyunMTSDefaultRole', position='Query'),
  state: string(name='State', description='The new state of the MPS queue.

*   **Active**: The MPS queue is active. Jobs in the MPS queue can be scheduled and run by MPS.
*   **Paused**: The MPS queue is paused. Jobs in the MPS queue cannot be scheduled or run by MPS, and all jobs remain in the Submitted state. Jobs that are running will not be affected.', example='Paused', position='Query'),
}

model UpdatePipelineResponseBody = {
  pipeline?: {
    id?: string(name='Id', description='The ID of the MPS queue.', example='d1ce4d3efcb549419193f50f1fcd****'),
    name?: string(name='Name', description='The new name of the MPS queue.', example='example-pipeline-****'),
    notifyConfig?: {
      mqTag?: string(name='MqTag', description='The tags of the messages.', example='mts-test'),
      mqTopic?: string(name='MqTopic', description='The queue of messages that are received.', example='example1,example2'),
      queueName?: string(name='QueueName', description='The queue that is created in MNS.', example='example-queue-****'),
      topic?: string(name='Topic', description='The topic that is created in MNS.', example='example-topic-****'),
    }(name='NotifyConfig', description='The MNS configuration.'),
    quotaAllocate?: long(name='QuotaAllocate', description='The quota that is allocated to the MPS queue.', example='10'),
    role?: string(name='Role', description='The role that is assigned to the current RAM user.', example='AliyunMTSExampleRole'),
    speed?: string(name='Speed', description='The type of the MPS queue. Default value: **Standard**. Valid values:

*   **Boost**: MPS queue with transcoding speed boosted
*   **Standard**: standard MPS queue
*   **NarrowBandHDV2**: MPS queue that supports Narrowband HD 2.0
*   **AIVideoCover**: MPS queue for intelligent snapshot capture
*   **AIVideoFPShot**: MPS queue for media fingerprinting
*   **AIVideoCensor**: MPS queue for automated review
*   **AIVideoMCU**: MPS queue for smart tagging
*   **AIVideoSummary**: MPS queue for video synopsis
*   **AIVideoPorn**: MPS queue for pornography detection in videos
*   **AIAudioKWS**: MPS queue for keyword recognition in audio
*   **AIAudioASR**: MPS queue for speech-to-text conversion', example='Standard'),
    state?: string(name='State', description='The state of the MPS queue. Valid values:

*   **Active**: The MPS queue is active.
*   **Paused**: The MPS queue is paused.', example='Paused'),
  }(name='Pipeline', description='The details of the MPS queue.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='1FE0F96B-544D-4244-9D83-DFCFB0E5A231'),
}

model UpdatePipelineResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: UpdatePipelineResponseBody(name='body'),
}

/**
  * *   You can call this operation to modify the name, status, and notification settings of the specified MPS queue.
  * *   If a paused MPS queue is selected in a workflow or a job, such as a video review or media fingerprint job, the workflow or job fails.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function updatePipeline(request: UpdatePipelineRequest): UpdatePipelineResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'UpdatePipeline', 'POST', '/', 'json', false, 'json', request);
}

model UpdateSmarttagTemplateRequest {
  analyseTypes?: string(name='AnalyseTypes', example='ocr,asr', position='Query'),
  faceCategoryIds?: string(name='FaceCategoryIds', example='celebrity', position='Query'),
  faceCustomParamsConfig?: string(name='FaceCustomParamsConfig', example='{ "faceDetThreshold":0.999, "faceRegThreshold":0.9 }', position='Query'),
  industry?: string(name='Industry', example='common', position='Query'),
  isDefault?: boolean(name='IsDefault', example='true', position='Query'),
  keywordConfig?: string(name='KeywordConfig', example='{ "type": "name,location,organization,other" }', position='Query'),
  knowledgeConfig?: string(name='KnowledgeConfig', example='{ "movie":"name,alias,chnl,genre", "music":"songName,artistName", "person":"name,gender" }', position='Query'),
  labelType?: string(name='LabelType', example='hmi', position='Query'),
  labelVersion?: string(name='LabelVersion', example='1.0', position='Query'),
  landmarkGroupIds?: string(name='LandmarkGroupIds', example='common', position='Query'),
  objectGroupIds?: string(name='ObjectGroupIds', example='general,item,weapon,animal', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  scene?: string(name='Scene', example='search', position='Query'),
  templateId: string(name='TemplateId', example='05de22f255284c7a8d2aab535dde****', position='Query'),
  templateName?: string(name='TemplateName', example='template-example-****', position='Query'),
}

model UpdateSmarttagTemplateResponseBody = {
  requestId?: string(name='RequestId', example='5210DBB0-E327-4D45-ADBC-0B83C8796E26'),
}

model UpdateSmarttagTemplateResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: UpdateSmarttagTemplateResponseBody(name='body'),
}

async function updateSmarttagTemplate(request: UpdateSmarttagTemplateRequest): UpdateSmarttagTemplateResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'UpdateSmarttagTemplate', 'POST', '/', 'json', false, 'json', request);
}

model UpdateTemplateRequest {
  audio?: string(name='Audio', description='The transmuxing configurations. The value is a JSON object. For more information, see the [MuxConfig](~~29253~~) parameter of the "Parameter details" topic.', example='{"Codec":"aac","Samplerate":"44100","Bitrate":"500","Channels":"2"}', position='Query'),
  container?: string(name='Container', description='The configurations of the video stream. The value is a JSON object. For more information, see the [Video](~~29253~~) parameter of the "Parameter details" topic.', example='{"Format":"mp4"}', position='Query'),
  muxConfig?: string(name='MuxConfig', description='The general transcoding configurations. The value is a JSON object. For more information, see the [TransConfig](~~29253~~) parameter of the "Parameter details" topic.', example='{"Segment":{"Duration":"10"}}', position='Query'),
  name: string(name='Name', description='The container format. The value is a JSON object. Default format: **MP4**.

*   Video formats: FLV, MP4, HLS (M3U8 + TS), and MPEG-DASH (MPD + fMP4)
*   Audio formats: MP3, MP4, OGG, FLAC, and M4A
*   Images formats: GIF and WebP

For more information, see the [Container](~~29253~~) parameter of the "Parameter details" topic.', example='MPS-example', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  templateId: string(name='TemplateId', description='The name of the template. The name can be up to 128 bytes in length.', example='16f01ad6175e4230ac42bb5182cd****', position='Query'),
  transConfig?: string(name='TransConfig', description='The ID of the request.', example='{"TransMode":"onepass"}', position='Query'),
  video?: string(name='Video', description='The configurations of the audio stream. The value is a JSON object. For more information, see the [Audio](~~29253~~) parameter of the "Parameter details" topic.', example='{"Codec":"H.264","Profile":"high","Bitrate":"500","Crf":"15","Width":"256","Height":"800","Fps":"25","Gop":"10"}', position='Query'),
}

model UpdateTemplateResponseBody = {
  requestId?: string(name='RequestId', description='The type of the transcoding template.', example='5E4FB22E-B9EA-4E24-8FFC-B407EA71QW21'),
  template?: {
    audio?: {
      bitrate?: string(name='Bitrate', description='The ID of the transcoding template.', example='500'),
      channels?: string(name='Channels', description='The audio bitrate of the output file.

*   Valid values: 8 to 1000.****
*   Unit: Kbit/s.
*   Default value: **128**.', example='2'),
      codec?: string(name='Codec', description='The sampling rate.

*   Unit: Hz.
*   Default value: **44100**.', example='aac'),
      profile?: string(name='Profile', description='Indicates whether the audio stream is deleted.

*   **true**: The audio stream is deleted.
*   **false**: The audio stream is retained.
*   Default value: **false**.', example='aac_low'),
      qscale?: string(name='Qscale', description='The number of sound channels. Default value: **2**.', example='1'),
      remove?: string(name='Remove', description='The audio codec format. Default value: **aac**. Valid values:

*   **aac**
*   **mp3**
*   **vorbis**
*   **flac**', example='false'),
      samplerate?: string(name='Samplerate', description='The level of the independent denoising algorithm.', example='44100'),
      volume?: {
        integratedLoudnessTarget?: string(name='IntegratedLoudnessTarget', description='The expected volume.

*   This parameter takes effect only if the value of Method is dynamic.
*   Unit: decibel.
*   Valid values: \\[-70,-5].
*   Default value: -6.', example='-6'),
        level?: string(name='Level', description='The increased volume relative to the volume of the input audio.

*   This parameter takes effect only if the value of Method is linear.
*   Unit: decibel.
*   Valid values: less than or equal to 20.
*   Default value: -20.', example='-20'),
        loudnessRangeTarget?: string(name='LoudnessRangeTarget', description='The range of the volume relative to the expected volume.

*   This parameter takes effect only if the value of Method is dynamic.
*   Unit: decibel.
*   Valid values: \\[1,20].
*   Default value: 8.', example='8'),
        method?: string(name='Method', description='The volume adjustment method. Valid values:

*   **auto**
*   **dynamic**
*   **linear**', example='auto'),
        peakLevel?: string(name='PeakLevel', description='The volume adjustment coefficient.

This parameter takes effect only if the value of Method is adaptive.

Valid values: \\[0,1].

Default value: 0.9.', example='0.9'),
        truePeak?: string(name='TruePeak', description='The peak volume.

*   This parameter takes effect only if the value of Method is dynamic.
*   Unit: decibel.
*   Valid values: \\[-9,0].
*   Default value: -1.', example='-1'),
      }(name='Volume', description='The volume control configurations.'),
    }(name='Audio', description='The audio codec settings.'),
    container?: {
      format?: string(name='Format', description='The container format.', example='mp4'),
    }(name='Container', description='The container format.'),
    id?: string(name='Id', description='The container configurations.', example='16f01ad6175e4230ac42bb5182cd****'),
    muxConfig?: {
      gif?: {
        ditherMode?: string(name='DitherMode', description='The loop count.', example='bayer'),
        finalDelay?: string(name='FinalDelay', description='The color dithering algorithm of the palette. Valid values: sierra and bayer.', example='false'),
        isCustomPalette?: string(name='IsCustomPalette', description='The segment configurations.', example='0'),
        loop?: string(name='Loop', description='Indicates whether the custom palette is used.', example='0'),
      }(name='Gif', description='The duration for which the final frame is paused. Unit: milliseconds.'),
      segment?: {
        duration?: string(name='Duration', description='The name of the template.', example='10'),
      }(name='Segment', description='The length of the segment. Unit: seconds.'),
      webp?: {
        loop?: string(name='Loop', description='The transmuxing configurations for GIF.', example='0'),
      }(name='Webp', description='The loop count.'),
    }(name='MuxConfig', description='The transmuxing configurations for WebP.'),
    name?: string(name='Name', description='The audio codec configurations.', example='MPS-example'),
    state?: string(name='State', description='The transmuxing configurations.', example='Normal'),
    transConfig?: {
      adjDarMethod?: string(name='AdjDarMethod', description='Indicates whether the video bitrate is checked. If this parameter is set to true and the system detects that the video bitrate of the output file is greater than that of the input file, the video bitrate of the input file is retained after transcoding. Valid values:

*   **true**: The video bitrate is checked.
*   **false**: The video bitrate is not checked.
*   Default value: **false**.', example='none'),
      isCheckAudioBitrate?: string(name='IsCheckAudioBitrate', description='The transcoding mode. Default value: **onepass**. Valid values:

*   **onepass**
*   **twopass**
*   **CBR**', example='false'),
      isCheckAudioBitrateFail?: string(name='IsCheckAudioBitrateFail', description='The status of the template. Valid values:

*   **Normal**: The template is normal.
*   **Deleted**: The template is deleted.', example='false'),
      isCheckReso?: string(name='IsCheckReso', description='Indicates whether the video bitrate is checked. This parameter has a higher priority than the IsCheckVideoBitrate parameter. Valid values:

*   **true**: The video bitrate is checked
*   **false**: The video bitrate is not checked.
*   Default value: **false**.', example='false'),
      isCheckResoFail?: string(name='IsCheckResoFail', description='Indicates whether the audio bitrate is checked. This parameter has a higher priority than the IsCheckAudioBitrate parameter. Valid values:

*   **true**: The audio bitrate is checked.
*   **false**: The audio bitrate is not checked.
*   Default value: **false**.', example='false'),
      isCheckVideoBitrate?: string(name='IsCheckVideoBitrate', description='Indicates whether the resolution is checked. If this parameter is set to true and the system detects that the resolution of the output file is higher than that of the input file based on the width or height, an error that indicates a transcoding failure is returned. Valid values:

*   **true**: The resolution is checked.
*   **false**: The resolution is not checked.
*   Default value: **false**.', example='false'),
      isCheckVideoBitrateFail?: string(name='IsCheckVideoBitrateFail', description='The method of resolution adjustment. Default value: **none**. Valid values:

*   rescale: The input video is rescaled.
*   crop: The input video is cropped.
*   none: No change is made.', example='false'),
      transMode?: string(name='TransMode', description='Indicates whether the resolution is checked. If the output resolution is higher than the input resolution based on the width or height, the input resolution is retained after transcoding. Valid values:

*   **true**: The resolution is checked.
*   **false**: The resolution is not checked.
*   Default value: **false**.', example='onepass'),
    }(name='TransConfig', description='Indicates whether the audio bitrate is checked. If the bitrate of the output audio is greater than the bitrate of the input audio, the bitrate of the input audio is retained after transcoding. In this case, the specified audio bitrate does not take effect. This parameter has a lower priority than the IsCheckAudioBitrateFail parameter. Valid values:

*   **true**: The audio bitrate is checked.

*   **false**: The audio bitrate is not checked.

*   Default value:

    *   If the parameter is left empty and the codec of the output audio is different from that of the input audio, the default value is false.
    *   If the parameter is left empty and the codec of the output audio is the same as that of the input audio, the default value is true.'),
    video?: {
      bitrate?: string(name='Bitrate', description='The maximum bitrate of the video. Unit: Kbit/s.', example='200'),
      bitrateBnd?: {
        max?: string(name='Max', description='The lower limit of the total bitrate. Unit: Kbit/s.', example='500'),
        min?: string(name='Min', description='The pixel format. Valid values: standard pixel formats such as yuv420p and yuvj420p.', example='100'),
      }(name='BitrateBnd', description='The upper limit of the total bitrate. Unit: Kbit/s.'),
      bufsize?: string(name='Bufsize', description='The level of quality control on the video.', example='6000'),
      codec?: string(name='Codec', description='The height of the output video.

*   Unit: pixel.
*   Default value: the height of the input video.', example='H.264'),
      crf?: string(name='Crf', description='Indicates whether the video stream is deleted. Valid values:

*   **true**: The video stream is deleted.
*   **false**: The video stream is retained.
*   Default value: **false**.', example='15'),
      crop?: string(name='Crop', description='The average bitrate of the video. Unit: Kbit/s.', example='border'),
      degrain?: string(name='Degrain', description='The average bitrate range of the video.', example='10'),
      fps?: string(name='Fps', description='The preset video algorithm. Default value: **medium**. Valid values:

*   **veryfast**
*   **fast**
*   **medium**
*   **slow**
*   **slower**', example='25'),
      gop?: string(name='Gop', description='The width of the video.

*   Unit: pixel.
*   Default value: **the width of the input video**.', example='10'),
      hdr2sdr?: string(name='Hdr2sdr', description='Indicates whether the HDR2SDR conversion feature is enabled. If this feature is enabled, high dynamic range (HDR) videos are transcoded to standard dynamic range (SDR) videos.', example='true'),
      height?: string(name='Height', description='The level of the independent denoising algorithm.', example='800'),
      longShortMode?: string(name='LongShortMode', description='The size of the buffer.

*   Unit: KB.
*   Default value: **6000**.', example='false'),
      maxFps?: string(name='MaxFps', description='The encoding profile. Valid values:

*   **baseline**: applicable to mobile devices.
*   **main**: applicable to standard-definition devices.
*   **high**: applicable to high-definition devices.
*   Default value: **high**.', example='60'),
      maxrate?: string(name='Maxrate', description='The maximum frame rate.', example='500'),
      narrowBand?: {
        abrmax?: float(name='Abrmax', description='The upper limit of the dynamic bitrate. If this parameter is set, the average bitrate is in the range of (0, 1000000].', example='3000'),
        maxAbrRatio?: float(name='MaxAbrRatio', description='The maximum ratio of the upper limit of dynamic bitrate. If this parameter is set, the value of Abrmax does not exceed x times of the source video bitrate. Valid values: (0,1.0].', example='1.0'),
        version?: string(name='Version', description='The Narrowband HD version. Only 1.0 may be returned.', example='1.0'),
      }(name='NarrowBand', description='The Narrowband HD settings.'),
      pad?: string(name='Pad', description='The video codec. Default value: **H.264**.', example='1280:800:0:140'),
      pixFmt?: string(name='PixFmt', description='The black borders added to the video.

*   Format: width:height:left:top.
*   Example: 1280:800:0:140.', example='yuv420p'),
      preset?: string(name='Preset', description='The scan mode. Valid values:

*   **interlaced**: An interlaced scan is performed.
*   **progressive**: A progressive scan is performed.', example='medium'),
      profile?: string(name='Profile', description='The bitrate quality control factor.

*   Default value if the Codec parameter is set to H.264: **23**. Default value if the Codec parameter is set to H.265: **26**.
*   If this parameter is returned, the setting of the Bitrate parameter is invalid.', example='high'),
      qscale?: string(name='Qscale', description='The method used to crop the video.

*   **border**: automatically detects and removes borders.
*   Value in the width:height:left:top format: crops the video based on custom settings.**** Example: 1280:800:0:140.', example='1'),
      remove?: string(name='Remove', description='The maximum number of frames between two keyframes. Default value: **250**.', example='false'),
      resoPriority?: string(name='ResoPriority', description='The general transcoding configurations.', example='1'),
      scanMode?: string(name='ScanMode', description='The policy of resolution adjustment.', example='interlaced'),
      width?: string(name='Width', description='The frame rate.

*   A value of 60 is returned if the frame rate of the input video exceeds 60.
*   Default value: the frame rate of the input video.', example='256'),
    }(name='Video', description='The video codec configurations.'),
  }(name='Template', description='The type of the transcoding template.'),
}

model UpdateTemplateResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: UpdateTemplateResponseBody(name='body'),
}

/**
  * A custom transcoding template cannot be updated if it is being used by a job that has been submitted.The ID of the template. You can obtain the template ID from the response of the [AddTemplate](~~213306~~) operation.
  * ### QPS limits
  * You can call this operation up to 100 times per second. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function updateTemplate(request: UpdateTemplateRequest): UpdateTemplateResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'UpdateTemplate', 'POST', '/', 'json', false, 'json', request);
}

model UpdateWaterMarkTemplateRequest {
  config: string(name='Config', description='The updated configuration of the watermark template. The value is a JSON object. For more information, see [Parameter details](~~29253~~).', example='{"Width":"10","Height":"30","Dx":"10","Dy":"5","Type":"Image","Timeline":{"Start":"0","Duration":"10"}}', position='Query'),
  name: string(name='Name', description='The new name of the watermark template. The value can contain letters and digits and can be up to 128 bytes in size.', example='example-watermark-****', position='Query'),
  ownerAccount?: string(name='OwnerAccount', position='Query'),
  ownerId?: long(name='OwnerId', position='Query'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount', position='Query'),
  resourceOwnerId?: long(name='ResourceOwnerId', position='Query'),
  waterMarkTemplateId: string(name='WaterMarkTemplateId', description='The ID of the watermark template that you want to update. To obtain the ID of the watermark template, you can log on to the **ApsaraVideo Media Processing (MPS) console** and choose **Global Settings** > **Watermark Templates** in the left-side navigation pane.', example='3780bd69b2b74540bc7b1096f564****', position='Query'),
}

model UpdateWaterMarkTemplateResponseBody = {
  requestId?: string(name='RequestId', description='The ID of the request.', example='E558894E-40D9-57C6-B5CC-0F5CDF23614E'),
  waterMarkTemplate?: {
    dx?: string(name='Dx', description='The horizontal offset. Unit: pixel.', example='10'),
    dy?: string(name='Dy', description='The vertical offset. Unit: pixel.', example='5'),
    height?: string(name='Height', description='The height of the watermark image. Unit: pixel.', example='30'),
    id?: string(name='Id', description='The ID of the watermark template. We recommend that you keep this ID for subsequent operation calls.', example='3780bd69b2b74540bc7b1096f564****'),
    name?: string(name='Name', description='The name of the watermark template.', example='example-watermark-****'),
    ratioRefer?: {
      dx?: string(name='Dx', description='The horizontal offset of the watermark relative to the output video image. Default value: **0**. The default value indicates no offset. The value can be an integer or a decimal.

*   **Integer**: the vertical offset. This indicates the absolute position. Unit: pixel.
*   **Decimal**: the ratio of the horizontal offset to the width of the output video. The ratio varies based on the size of the video. Four decimal places are supported, such as 0.9999. More decimal places are discarded.', example='0.51'),
      dy?: string(name='Dy', description='The vertical offset of the watermark relative to the output video image. Default value: **0**. The default value indicates no offset. The value can be an integer or a decimal.

*   **Integer**: the vertical offset. This indicates the absolute position. Unit: pixel.
*   **Decimal**: the ratio of the vertical offset to the height of the output video. The ratio varies based on the size of the video. Four decimal places are supported, such as 0.9999. More decimal places are discarded.', example='0.28'),
      height?: string(name='Height', description='The height of the watermark image in the output video. The value can be an integer or a decimal.

*   **Integer**: the height of the watermark image. This indicates the absolute position. Unit: pixel.
*   **Decimal**: the ratio of the height of the watermark image to the height of the output video. The ratio varies based on the size of the video. Four decimal places are supported, such as 0.9999. More decimal places are discarded.', example='0.33'),
      width?: string(name='Width', description='The width of the watermark image in the output video. The value can be an integer or a decimal.

*   **Integer**: the width of the watermark image. This indicates the absolute position. Unit: pixel.
*   **Decimal**: the ratio of the width of the watermark image to the width of the output video. The ratio varies based on the size of the video. Four decimal places are supported, such as 0.9999. More decimal places are discarded.', example='0.36'),
    }(name='RatioRefer', description='The values of the Height, Width, Dx, and Dy parameters relative to the reference edges. If the values of the Height, Width, Dx, and Dy parameters are decimals between 0 and 1, the values are calculated by referring to the following edges in sequence:

*   **Width**: the width edge.
*   **Height**: the height edge.
*   **Long**: the long edge.
*   **Short**: the short edge.'),
    referPos?: string(name='ReferPos', description='The position of the watermark. Valid values:

*   **TopRight**: the upper-right corner.
*   **TopLeft**: the upper-left corner.
*   **BottomRight**: the lower-right corner.
*   **BottomLeft**: the lower-left corner.', example='TopRight'),
    state?: string(name='State', description='The status of the watermark template. Default value: **Normal**.', example='Normal'),
    timeline?: {
      duration?: string(name='Duration', description='The display duration of the watermark. Default value: **ToEND**. The default value indicates that the watermark is displayed until the video ends.', example='10'),
      start?: string(name='Start', description='The beginning of the time range during which the watermark is displayed.

*   Unit: seconds.
*   Default value: **0**.', example='0'),
    }(name='Timeline', description='The timeline of the watermark.'),
    type?: string(name='Type', description='The type of the watermark. Valid values:

*   Image: an image watermark.
*   Text: a text watermark.

> Only watermarks of the Image type are supported.', example='Image'),
    width?: string(name='Width', description='The width of the watermark image. Unit: pixel.', example='10'),
  }(name='WaterMarkTemplate', description='The details of the watermark template.'),
}

model UpdateWaterMarkTemplateResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: UpdateWaterMarkTemplateResponseBody(name='body'),
}

/**
  * *   You can call this operation to update the information about a watermark template based on the ID of the watermark template. For example, you can update the name and configurations of a watermark template.
  * *   A watermark template cannot be updated if it is being used by a job that has been submitted.
  * ### QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
 */
async function updateWaterMarkTemplate(request: UpdateWaterMarkTemplateRequest): UpdateWaterMarkTemplateResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'UpdateWaterMarkTemplate', 'POST', '/', 'json', false, 'json', request);
}

