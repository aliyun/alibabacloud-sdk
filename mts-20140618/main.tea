/**
 *
 */
import Util;
import OpenApi;
import OpenApiUtil;
import EndpointUtil;

extends OpenApi;


init(config: OpenApi.Config){
  super(config);
  @endpointRule = 'regional';
  @endpointMap = {
    ap-northeast-2-pop = 'mts.aliyuncs.com',
    ap-southeast-2 = 'mts.aliyuncs.com',
    ap-southeast-3 = 'mts.aliyuncs.com',
    cn-beijing-finance-1 = 'mts.aliyuncs.com',
    cn-beijing-finance-pop = 'mts.aliyuncs.com',
    cn-beijing-gov-1 = 'mts.aliyuncs.com',
    cn-beijing-nu16-b01 = 'mts.aliyuncs.com',
    cn-chengdu = 'mts.aliyuncs.com',
    cn-edge-1 = 'mts.aliyuncs.com',
    cn-fujian = 'mts.aliyuncs.com',
    cn-haidian-cm12-c01 = 'mts.aliyuncs.com',
    cn-hangzhou-bj-b01 = 'mts.aliyuncs.com',
    cn-hangzhou-finance = 'mts.aliyuncs.com',
    cn-hangzhou-internal-prod-1 = 'mts.aliyuncs.com',
    cn-hangzhou-internal-test-1 = 'mts.aliyuncs.com',
    cn-hangzhou-internal-test-2 = 'mts.aliyuncs.com',
    cn-hangzhou-internal-test-3 = 'mts.aliyuncs.com',
    cn-hangzhou-test-306 = 'mts.aliyuncs.com',
    cn-hongkong-finance-pop = 'mts.aliyuncs.com',
    cn-huhehaote-nebula-1 = 'mts.aliyuncs.com',
    cn-north-2-gov-1 = 'mts.aliyuncs.com',
    cn-qingdao-nebula = 'mts.aliyuncs.com',
    cn-shanghai-et15-b01 = 'mts.aliyuncs.com',
    cn-shanghai-et2-b01 = 'mts.aliyuncs.com',
    cn-shanghai-finance-1 = 'mts.aliyuncs.com',
    cn-shanghai-inner = 'mts.aliyuncs.com',
    cn-shanghai-internal-test-1 = 'mts.aliyuncs.com',
    cn-shenzhen-finance-1 = 'mts.aliyuncs.com',
    cn-shenzhen-inner = 'mts.aliyuncs.com',
    cn-shenzhen-st4-d01 = 'mts.aliyuncs.com',
    cn-shenzhen-su18-b01 = 'mts.aliyuncs.com',
    cn-wuhan = 'mts.aliyuncs.com',
    cn-wulanchabu = 'mts.aliyuncs.com',
    cn-yushanfang = 'mts.aliyuncs.com',
    cn-zhangbei = 'mts.aliyuncs.com',
    cn-zhangbei-na61-b01 = 'mts.aliyuncs.com',
    cn-zhangjiakou-na62-a01 = 'mts.aliyuncs.com',
    cn-zhengzhou-nebula-1 = 'mts.aliyuncs.com',
    eu-west-1-oxs = 'mts.aliyuncs.com',
    me-east-1 = 'mts.aliyuncs.com',
    rus-west-1-pop = 'mts.aliyuncs.com',
    us-east-1 = 'mts.aliyuncs.com',
  };

  checkConfig(config);
  @endpoint = getEndpoint('mts', @regionId, @endpointRule, @network, @suffix, @endpointMap, @endpoint);
}

function getEndpoint(productId: string, regionId: string, endpointRule: string, network: string, suffix: string, endpointMap: map[string]string, endpoint: string) throws: string{
  if (!Util.empty(endpoint)) {
    return endpoint;
  }
  
  if (!Util.isUnset(endpointMap) && !Util.empty(endpointMap[regionId])) {
    return endpointMap[regionId];
  }
  return EndpointUtil.getEndpointRules(productId, regionId, endpointRule, network, suffix);
}

model ActivateMediaWorkflowRequest {
  mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the media workflow. You can obtain the ID from the response of the [AddMediaWorkflow](~~44437~~) operation.', example='93ab850b4f6f44eab54b6e9181d4****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model ActivateMediaWorkflowResponseBody = {
  mediaWorkflow?: {
    creationTime?: string(name='CreationTime', description='The time when the media workflow was created.', example='2016-04-01T05:29:37Z'),
    mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the media workflow.', example='93ab850b4f6f44eab54b6e9181d4****'),
    name?: string(name='Name', description='The name of the media workflow.', example='mediaworkflow-example'),
    state?: string(name='State', description='The status of the media workflow. The value is **Active**.', example='Active'),
    topology?: string(name='Topology', description='The topology of the media workflow.', example='{\\"Activities\\":{\\"Act-Start\\":{\\"Parameters\\":{\\"PipelineId\\":\\"130266f58161436a80bf07cb12c8****\\",\\"InputFile\\":\\"{\\\\\\"Bucket\\\\\\": \\\\\\"example\\\\\\",\\\\\\"Location\\\\\\": \\\\\\"oss-cn-hangzhou\\\\\\"}\\"},\\"Type\\":\\"Start\\"},\\"Act-Report\\":{\\"Parameters\\":{},\\"Type\\":\\"Report\\"},\\"Act-Transcode-M3U8\\":{\\"Parameters\\":{\\"Outputs\\":\\"[{\\\\\\"OutputObject\\\\\\":\\\\\\"transcode%2F%7BObjectPrefix%7D%7BFileName%7D\\\\\\",\\\\\\"TemplateId\\\\\\": \\\\\\"957d1719ee85ed6527b90cf62726****\\\\\\"}]\\",\\"OutputBucket\\":\\"panda-vod-hls\\",\\"OutputLocation\\":\\"oss-cn-hangzhou\\"},\\"Type\\":\\"Transcode\\"}},\\"Dependencies\\":{\\"Act-Start\\":[\\"Act-Transcode-M3U8\\"],\\"Act-Report\\":[],\\"Act-Transcode-M3U8\\":[\\"Act-Report\\"]}}'),
  }(name='MediaWorkflow', description='The details of the media workflow.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='A1326BD4-30B1-4CB6-Q123-3330B877B0D4'),
}

model ActivateMediaWorkflowResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: ActivateMediaWorkflowResponseBody(name='body'),
}

/**
  * You can call this operation to activate a media workflow that has been deactivated. After you activate a media workflow, you cannot modify the workflow information, such as the name, topology, or trigger mode. A media workflow is activated by default after it is created.  
  * ## Limits on QPS
  * You can call this operation up to 100 times per second. If the number of the calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request ActivateMediaWorkflowRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return ActivateMediaWorkflowResponse
 */
async function activateMediaWorkflowWithOptions(request: ActivateMediaWorkflowRequest, runtime: Util.RuntimeOptions): ActivateMediaWorkflowResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.mediaWorkflowId)) {
    query['MediaWorkflowId'] = request.mediaWorkflowId;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'ActivateMediaWorkflow',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * You can call this operation to activate a media workflow that has been deactivated. After you activate a media workflow, you cannot modify the workflow information, such as the name, topology, or trigger mode. A media workflow is activated by default after it is created.  
  * ## Limits on QPS
  * You can call this operation up to 100 times per second. If the number of the calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request ActivateMediaWorkflowRequest
  * @return ActivateMediaWorkflowResponse
 */
async function activateMediaWorkflow(request: ActivateMediaWorkflowRequest): ActivateMediaWorkflowResponse {
  var runtime = new Util.RuntimeOptions{};
  return activateMediaWorkflowWithOptions(request, runtime);
}

model AddMediaRequest {
  cateId?: long(name='CateId', description='The ID of the category to which the media file belongs. The value cannot be negative.', example='123'),
  coverURL?: string(name='CoverURL', description='The storage location of the thumbnail that you want to specify for the media file. To obtain the URL, you can log on to the **MPS console** and choose **Workflows** > **Media Buckets**. Alternatively, you can log on to the **OSS console** and click **My OSS Paths**.

*   The value can be up to 3,200 bytes in length.
*   The URL complies with RFC 2396 and is encoded in UTF-8, with reserved characters being percent-encoded.', example='http://bucket.oss-cn-hangzhou.aliyuncs.com/example/1.png'),
  description?: string(name='Description', description='The description of the media file.

*   The description can be up to 1,024 bytes in length.
*   The value is encoded in UTF-8.', example='A test video'),
  fileURL?: string(name='FileURL', description='The path of the input file. You can query the path of the input file in the MPS or OSS console. For more information, see the **Triggering and matching rule for a workflow** section of this topic.

*   The value can be up to 3,200 bytes in length.
*   The URL complies with RFC 2396 and is encoded in UTF-8, with reserved characters being percent-encoded.', example='http://bucket.oss-cn-hangzhou.aliyuncs.com/A/B/C/test.mp4'),
  inputUnbind?: boolean(name='InputUnbind', description='Specifies whether to check if the media workflow supports the specified input path. We recommend that you set this parameter to true to avoid errors that may result from invalid paths. Valid values:

*   **true**: checks whether the workflow supports the specified input path.
*   **false**: does not check whether the workflow supports the specified input path.', example='false'),
  mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the media workflow that you want to run for the media file. To query the ID of a media workflow, you can log on to the MPS console or call the [AddMediaWorkflow](~~44437~~) operation.', example='07da6c65da7f458997336e0de192****'),
  mediaWorkflowUserData?: string(name='MediaWorkflowUserData', description='The custom data of the media workflow.

*   The value can be up to 1,024 bytes in length.
*   The value is encoded in UTF-8.', example='test'),
  overrideParams?: string(name='OverrideParams', description='The subtitle settings that are used to overwrite the original settings.

*   Example 1: Use `{"WebVTTSubtitleOverrides",[{"RefActivityName":"subtitleNode","WebVTTSubtitleURL":"http://test.oss-cn-hangzhou.aliyuncs.com/example1.vtt"}]}` to overwrite the original subtitle settings during HTTP Live Streaming (HLS) packaging.
*   Example 2: Use `{"subtitleTransNodeName":{"InputConfig":{"Format":"stl","InputFile":{"URL":"http://subtitleBucket.oss-cn-hangzhou.aliyuncs.com/package/example/CENG.stl"}}}}` to overwrite the original subtitle settings during Dynamic Adaptive Streaming over HTTP (DASH) packaging.', example='{“subtitleTransNodeName”:{“InputConfig”:{“Format”:”stl”,”InputFile”:{“URL”:”http://exampleBucket.oss-cn-hangzhou.aliyuncs.com/package/example/CENG.stl"}}}}'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  tags?: string(name='Tags', description='The tags that you want to add for the media file.

>  In MPS, each tag that is specified for a media file is independent. You can search for all the media files that have the same tags in the Media Library.

*   Separate multiple tags with commas (,). You can specify up to 16 tags for a media file.
*   Each tag can be up to 32 bytes in length.
*   The value is encoded in UTF-8.', example='tag1,tag2'),
  title?: string(name='Title', description='The title of the media file.

*   The title can be up to 128 bytes in length.
*   The value is encoded in UTF-8.', example='mytest'),
}

model AddMediaResponseBody = {
  media?: {
    bitrate?: string(name='Bitrate', description='The bitrate of the media file.', example='1148.77'),
    cateId?: long(name='CateId', description='The ID of the category to which the media file belongs.', example='1'),
    censorState?: string(name='CensorState', description='The review status of the video. Valid values:

*   **Initiated**: The media file is uploaded but not reviewed.
*   **Pass**: The media file is uploaded and passes the review.', example='Initiated'),
    coverURL?: string(name='CoverURL', description='The storage location of the media thumbnail.', example='http://bucket.oss-cn-hangzhou.aliyuncs.com/example/1.png'),
    creationTime?: string(name='CreationTime', description='The time when the media file was created.', example='2016-09-20T03:02:40Z'),
    description?: string(name='Description', description='The description of the media file. The value is no longer than 1,024 bytes.', example='A test video'),
    duration?: string(name='Duration', description='The duration of the media file.', example='2.645333'),
    file?: {
      state?: string(name='State', description='The status of the input file. The default value is **Normal**.', example='Normal'),
      URL?: string(name='URL', description='The URL of the input file.', example='http://bucket.oss-cn-hangzhou.aliyuncs.com/A/B/C/test.mp4'),
    }(name='File', description='The information about the input file.'),
    format?: string(name='Format', description='The format of the media file. Valid values: mov, mp4, m4a, 3gp, 3g2, and mj2.', example='mp4'),
    fps?: string(name='Fps', description='The frame rate of the media file.', example='25.0'),
    height?: string(name='Height', description='The height of the media file.', example='1280'),
    mediaId?: string(name='MediaId', description='The ID of the media file.', example='3e6149d5a8c944c09b1a8d2dc3e4****'),
    publishState?: string(name='PublishState', description='The publishing status of the media file. Valid values:

*   **Initiated**: The media file is in the initial state.
*   **UnPublish**: The media file has not been published, and the playback permission on the OSS object is Private.
*   **Published**: The media file has been published, and the playback permission on the OSS object is Default.', example='Published'),
    runIdList?: {
      runId?: [ string ](name='RunId')
    }(name='RunIdList', description='The IDs of the executed workflow execution instances. The IDs are separated by commas (,).'),
    size?: string(name='Size', description='The size of the media file.', example='379860'),
    tags?: {
      tag?: [ string ](name='Tag')
    }(name='Tags', description='The tags of the media file.'),
    title?: string(name='Title', description='The title of the media file. The title is no longer than 128 bytes.', example='mytest.mp4'),
    width?: string(name='Width', description='The width of the media file.', example='1280'),
  }(name='Media', description='The detailed information about the media file.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='05F8B913-E9F3-4A6F-9922-48CADA0FFAAD'),
}

model AddMediaResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: AddMediaResponseBody(name='body'),
}

/**
  * - You can call this operation to process videos that are uploaded to Object Storage Service (OSS) but not processed. This way, you do not need to upload the videos to OSS again. If you have configured media workflows, OSS automatically notifies MPS when a media file is uploaded to OSS. MPS automatically finds the corresponding workflow in the active state based on the specified OSS bucket and object. Therefore, in most cases, you do not need to manually call the AddMedia operation to process the media file.
  * - Media information is automatically obtained only when the specified media workflow is in the active state. If no media workflow is specified or the specified media workflow is not in the active state, media information is not obtained.
  * ## Limits on QPS
  * You can call this operation up to 100 times per second. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request AddMediaRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return AddMediaResponse
 */
async function addMediaWithOptions(request: AddMediaRequest, runtime: Util.RuntimeOptions): AddMediaResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.cateId)) {
    query['CateId'] = request.cateId;
  }
  if (!Util.isUnset(request.coverURL)) {
    query['CoverURL'] = request.coverURL;
  }
  if (!Util.isUnset(request.description)) {
    query['Description'] = request.description;
  }
  if (!Util.isUnset(request.fileURL)) {
    query['FileURL'] = request.fileURL;
  }
  if (!Util.isUnset(request.inputUnbind)) {
    query['InputUnbind'] = request.inputUnbind;
  }
  if (!Util.isUnset(request.mediaWorkflowId)) {
    query['MediaWorkflowId'] = request.mediaWorkflowId;
  }
  if (!Util.isUnset(request.mediaWorkflowUserData)) {
    query['MediaWorkflowUserData'] = request.mediaWorkflowUserData;
  }
  if (!Util.isUnset(request.overrideParams)) {
    query['OverrideParams'] = request.overrideParams;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.tags)) {
    query['Tags'] = request.tags;
  }
  if (!Util.isUnset(request.title)) {
    query['Title'] = request.title;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'AddMedia',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * - You can call this operation to process videos that are uploaded to Object Storage Service (OSS) but not processed. This way, you do not need to upload the videos to OSS again. If you have configured media workflows, OSS automatically notifies MPS when a media file is uploaded to OSS. MPS automatically finds the corresponding workflow in the active state based on the specified OSS bucket and object. Therefore, in most cases, you do not need to manually call the AddMedia operation to process the media file.
  * - Media information is automatically obtained only when the specified media workflow is in the active state. If no media workflow is specified or the specified media workflow is not in the active state, media information is not obtained.
  * ## Limits on QPS
  * You can call this operation up to 100 times per second. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request AddMediaRequest
  * @return AddMediaResponse
 */
async function addMedia(request: AddMediaRequest): AddMediaResponse {
  var runtime = new Util.RuntimeOptions{};
  return addMediaWithOptions(request, runtime);
}

model AddMediaTagRequest {
  mediaId?: string(name='MediaId', example='3e6149d5a8c944c09b1a8d2dc3e4****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  tag?: string(name='Tag', example='tag1'),
}

model AddMediaTagResponseBody = {
  requestId?: string(name='RequestId', example='91B6CAB9-034C-4E4E-A40B-E7F5C81E1A2K'),
}

model AddMediaTagResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: AddMediaTagResponseBody(name='body'),
}

async function addMediaTagWithOptions(request: AddMediaTagRequest, runtime: Util.RuntimeOptions): AddMediaTagResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.mediaId)) {
    query['MediaId'] = request.mediaId;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.tag)) {
    query['Tag'] = request.tag;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'AddMediaTag',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

async function addMediaTag(request: AddMediaTagRequest): AddMediaTagResponse {
  var runtime = new Util.RuntimeOptions{};
  return addMediaTagWithOptions(request, runtime);
}

model AddMediaWorkflowRequest {
  name?: string(name='Name', description='The name of the media workflow.

*   The value cannot be empty.
*   The name cannot be the same as that of an existing media workflow within the current Alibaba Cloud account.
*   The name can be up to 64 characters in length.
*   The name can contain only UTF-8 characters.', example='mediaworkflow-example'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  topology?: string(name='Topology', description='The topology of the media workflow. The value must be a JSON object that contains the activity list and activity dependencies. For more information, see the **Sample topology** section of this topic.'),
  triggerMode?: string(name='TriggerMode', description='The trigger mode of the media workflow. Valid values:

*   **OssAutoTrigger**: The media workflow is automatically triggered.
*   **NotInAuto**: The media workflow is not automatically triggered.', example='OssAutoTrigger'),
}

model AddMediaWorkflowResponseBody = {
  mediaWorkflow?: {
    creationTime?: string(name='CreationTime', description='The time when the media workflow was created.', example='016-04-01T05:29:37Z'),
    mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the media workflow. We recommend that you keep this ID for later operations on this workflow.', example='e00732b977da427d9177a4deb1aa****'),
    name?: string(name='Name', description='The name of the media workflow.', example='mediaworkflow-example'),
    state?: string(name='State', description='The status of the media workflow. By default, the created workflow is in the **Active** state.', example='Active'),
    topology?: string(name='Topology', description='The topology of the media workflow. The value is a JSON object that contains the activity list and activity dependencies.'),
    triggerMode?: string(name='TriggerMode', description='The trigger mode of the media workflow. Valid values:

*   **OssAutoTrigger**: The media workflow is automatically triggered.
*   **NotInAuto**: The media workflow is not automatically triggered.', example='OssAutoTrigger'),
  }(name='MediaWorkflow', description='The information about the media workflow.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='F1D21261-ADB9-406A-1234-491382139D59'),
}

model AddMediaWorkflowResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: AddMediaWorkflowResponseBody(name='body'),
}

/**
  * You can call this operation to define the topology, activities, and dependencies of a media workflow. The topology is represented by a directed acyclic graph (DAG) in the console. For more information, see [Media workflow activities](~~68494~~). You can view and run the workflows that are created by calling this operation in the ApsaraVideo Media Processing (MPS) console.
  * ## Limits on QPS
  * You can call this operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request AddMediaWorkflowRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return AddMediaWorkflowResponse
 */
async function addMediaWorkflowWithOptions(request: AddMediaWorkflowRequest, runtime: Util.RuntimeOptions): AddMediaWorkflowResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.name)) {
    query['Name'] = request.name;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.topology)) {
    query['Topology'] = request.topology;
  }
  if (!Util.isUnset(request.triggerMode)) {
    query['TriggerMode'] = request.triggerMode;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'AddMediaWorkflow',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * You can call this operation to define the topology, activities, and dependencies of a media workflow. The topology is represented by a directed acyclic graph (DAG) in the console. For more information, see [Media workflow activities](~~68494~~). You can view and run the workflows that are created by calling this operation in the ApsaraVideo Media Processing (MPS) console.
  * ## Limits on QPS
  * You can call this operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request AddMediaWorkflowRequest
  * @return AddMediaWorkflowResponse
 */
async function addMediaWorkflow(request: AddMediaWorkflowRequest): AddMediaWorkflowResponse {
  var runtime = new Util.RuntimeOptions{};
  return addMediaWorkflowWithOptions(request, runtime);
}

model AddPipelineRequest {
  extendConfig?: string(name='ExtendConfig'),
  name?: string(name='Name', example='test-pipeline'),
  notifyConfig?: string(name='NotifyConfig', example='{"Topic":"mts-topic-1"}'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  role?: string(name='Role', example='AliyunMTSDefaultRole'),
  speed?: string(name='Speed', example='Standard'),
  speedLevel?: long(name='SpeedLevel', example='1'),
}

model AddPipelineResponseBody = {
  pipeline?: {
    extendConfig?: {
      isBoostNew?: boolean(name='IsBoostNew'),
      maxMultiSpeed?: int32(name='MaxMultiSpeed'),
      multiSpeedDowngradePolicy?: string(name='MultiSpeedDowngradePolicy'),
    }(name='ExtendConfig'),
    id?: string(name='Id', example='ed450ea0bfbd41e29f80a401fb4d****'),
    name?: string(name='Name'),
    notifyConfig?: {
      mqTag?: string(name='MqTag', example='mts-test'),
      mqTopic?: string(name='MqTopic', example='example1'),
      queueName?: string(name='QueueName', example='mts-queue-1'),
      topic?: string(name='Topic', example='mts-topic-1'),
    }(name='NotifyConfig'),
    quotaAllocate?: long(name='QuotaAllocate', example='10'),
    role?: string(name='Role', example='AliyunMTSDefaultRole'),
    speed?: string(name='Speed', example='Standard'),
    speedLevel?: long(name='SpeedLevel', example='1'),
    state?: string(name='State', example='Active'),
  }(name='Pipeline'),
  requestId?: string(name='RequestId', example='CFEA608A-5A1C-4C83-A54B-6197BC250D23'),
}

model AddPipelineResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: AddPipelineResponseBody(name='body'),
}

async function addPipelineWithOptions(request: AddPipelineRequest, runtime: Util.RuntimeOptions): AddPipelineResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.extendConfig)) {
    query['ExtendConfig'] = request.extendConfig;
  }
  if (!Util.isUnset(request.name)) {
    query['Name'] = request.name;
  }
  if (!Util.isUnset(request.notifyConfig)) {
    query['NotifyConfig'] = request.notifyConfig;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.role)) {
    query['Role'] = request.role;
  }
  if (!Util.isUnset(request.speed)) {
    query['Speed'] = request.speed;
  }
  if (!Util.isUnset(request.speedLevel)) {
    query['SpeedLevel'] = request.speedLevel;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'AddPipeline',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

async function addPipeline(request: AddPipelineRequest): AddPipelineResponse {
  var runtime = new Util.RuntimeOptions{};
  return addPipelineWithOptions(request, runtime);
}

model AddSmarttagTemplateRequest {
  analyseTypes?: string(name='AnalyseTypes', example='ocr'),
  faceCategoryIds?: string(name='FaceCategoryIds', example='celebrity'),
  faceCustomParamsConfig?: string(name='FaceCustomParamsConfig', example='{ "faceDetThreshold":0.999, "faceRegThreshold":0.9 }'),
  industry?: string(name='Industry', example='common'),
  isDefault?: boolean(name='IsDefault', example='true'),
  keywordConfig?: string(name='KeywordConfig', example='"type": "name,location,organization,other" }'),
  knowledgeConfig?: string(name='KnowledgeConfig', example='{ "movie":"name,alias,chnl,genre", "music":"songName,artistName", "person":"name,gender" }'),
  labelType?: string(name='LabelType', example='hmi'),
  labelVersion?: string(name='LabelVersion', example='1.0'),
  landmarkGroupIds?: string(name='LandmarkGroupIds', example='common'),
  objectGroupIds?: string(name='ObjectGroupIds', example='general,item,weapon,animal'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  scene?: string(name='Scene', example='search'),
  templateName?: string(name='TemplateName', example='template-example-****'),
}

model AddSmarttagTemplateResponseBody = {
  requestId?: string(name='RequestId', example='7B117AF5-2A16-412C-B127-FA6175EDAS3Q'),
  templateId?: string(name='TemplateId', example='05de22f255284c7a8d2aab535dde****'),
}

model AddSmarttagTemplateResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: AddSmarttagTemplateResponseBody(name='body'),
}

async function addSmarttagTemplateWithOptions(request: AddSmarttagTemplateRequest, runtime: Util.RuntimeOptions): AddSmarttagTemplateResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.analyseTypes)) {
    query['AnalyseTypes'] = request.analyseTypes;
  }
  if (!Util.isUnset(request.faceCategoryIds)) {
    query['FaceCategoryIds'] = request.faceCategoryIds;
  }
  if (!Util.isUnset(request.faceCustomParamsConfig)) {
    query['FaceCustomParamsConfig'] = request.faceCustomParamsConfig;
  }
  if (!Util.isUnset(request.industry)) {
    query['Industry'] = request.industry;
  }
  if (!Util.isUnset(request.isDefault)) {
    query['IsDefault'] = request.isDefault;
  }
  if (!Util.isUnset(request.keywordConfig)) {
    query['KeywordConfig'] = request.keywordConfig;
  }
  if (!Util.isUnset(request.knowledgeConfig)) {
    query['KnowledgeConfig'] = request.knowledgeConfig;
  }
  if (!Util.isUnset(request.labelType)) {
    query['LabelType'] = request.labelType;
  }
  if (!Util.isUnset(request.labelVersion)) {
    query['LabelVersion'] = request.labelVersion;
  }
  if (!Util.isUnset(request.landmarkGroupIds)) {
    query['LandmarkGroupIds'] = request.landmarkGroupIds;
  }
  if (!Util.isUnset(request.objectGroupIds)) {
    query['ObjectGroupIds'] = request.objectGroupIds;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.scene)) {
    query['Scene'] = request.scene;
  }
  if (!Util.isUnset(request.templateName)) {
    query['TemplateName'] = request.templateName;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'AddSmarttagTemplate',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

async function addSmarttagTemplate(request: AddSmarttagTemplateRequest): AddSmarttagTemplateResponse {
  var runtime = new Util.RuntimeOptions{};
  return addSmarttagTemplateWithOptions(request, runtime);
}

model AddTemplateRequest {
  audio?: string(name='Audio', description='The audio stream configuration. The value must be a JSON object. For more information, see the [Parameters nested under the Audio parameter](~~29253~~) section of this topic.

>  If you do not set this parameter, output files do not contain audio streams. If you need to retain the audio streams, this parameter is required.', example='{"Codec":"H.264","Samplerate":"44100","Bitrate":"500","Channels":"2"}'),
  container?: string(name='Container', description='The container configuration. The value must be a JSON object and contains the Format parameter. If you do not configure the container, the transcoded media file is in MP4 format by default. If you want to use the transcoding template to generate media files in other formats, this parameter is required. For more information, see the [Parameter nested under the Container parameter](~~29253~~) section of this topic.

*   Default value of the Format parameter: mp4.
*   Video formats include FLV, MP4, HLS (M3U8 + TS), and MPEG-DASH (MPD + fMP4)

>  If the container format is FLV, the video codec cannot be set to H.265.

*   Audio formats include MP3, MP4, Ogg, FLAC, and M4A

*   Image formats include GIF and WebP.

>*   If the container format is GIF, the video codec must be set to GIF.
  *   If the container format is WebP, the video codec must be set to WebP.', example='{"Format":"mp4"}'),
  muxConfig?: string(name='MuxConfig', description='The segment configuration. The value must be a JSON object. For more information, see the [Parameters nested under the MuxConfig parameter](~~29253~~) section of this topic. If you do not set this parameter, media segment files are not generated. If you want to generate media segment files, this parameter is required.', example='{"Segment":{"Duration":"10"}}'),
  name?: string(name='Name', description='The name of the template. The name can be up to 128 bytes in length.', example='mps-example'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  transConfig?: string(name='TransConfig', description='The general transcoding configuration. The value must be a JSON object. For more information, see the [Parameters nested under the TransConfig parameter](~~29253~~) section of this topic. If you do not set this parameter, the default settings of this parameter are used. If the default settings cannot meet your transcoding needs, set the parameters nested under the TransConfig parameter as required.', example='{"TransMode":"onepass"}'),
  video?: string(name='Video', description='The video stream configuration. The value must be a JSON object. For more information, see the [Parameters nested under the Video parameter](~~29253~~) section of this topic.

>  If you do not set this parameter, output files do not contain video streams. If you need to retain the video streams, this parameter is required.', example='{"Codec":"H.264","Profile":"high","Bitrate":"500","Crf":"15","Width":"256","Height":"800","Fps":"25","Gop":"10s"}'),
}

model AddTemplateResponseBody = {
  requestId?: string(name='RequestId', description='The ID of the request.', example='FA258E67-09B8-4EAA-8F33-BA567834A2C3'),
  template?: {
    audio?: {
      bitrate?: string(name='Bitrate', description='The audio bitrate of the output file.

*   Unit: Kbit/s.
*   Default value: **128**.', example='500'),
      channels?: string(name='Channels', description='The number of sound channels. Default value: **2**.', example='2'),
      codec?: string(name='Codec', description='The audio codec. Default audio codec: **AAC**. Valid audio codecs:

*   **AAC**
*   **MP3**
*   **VORBIS**
*   **FLAC**', example='aac'),
      profile?: string(name='Profile', description='The codec profile of the audio. Valid values when the audio codec is set to **AAC**:****

*   **aac_low**
*   **aac_he**
*   **aac_he_v2**
*   **aac_ld**
*   **aac_eld**', example='aac_low'),
      qscale?: string(name='Qscale', description='The level of the independent denoising algorithm.', example='5'),
      remove?: string(name='Remove', description='Indicates whether to delete the audio stream.

*   **true**: The audio stream is deleted.
*   **false**: The audio stream is retained.
*   Default value: **false**.', example='true'),
      samplerate?: string(name='Samplerate', description='The sampling rate.

*   Unit: Hz.
*   Default value: **44100**.', example='44100'),
      volume?: {
        level?: string(name='Level', description='The volume adjustment range.

*   Default value: **-20**.
*   Unit: dB.', example='-20'),
        method?: string(name='Method', description='The volume adjustment method. Valid values:

*   **auto**: The volume is automatically adjusted.
*   **dynamic**: The volume is dynamically adjusted.
*   **linear**: The volume is linearly adjusted.', example='auto'),
      }(name='Volume', description='The details of the volume.'),
    }(name='Audio', description='The audio codec configuration.'),
    container?: {
      format?: string(name='Format', description='The container format.', example='mp4'),
    }(name='Container', description='The container configuration.'),
    id?: string(name='Id', description='The ID of the transcoding template. We recommend that you keep this ID for subsequent operation calls.', example='16f01ad6175e4230ac42bb5182cd****'),
    muxConfig?: {
      gif?: {
        ditherMode?: string(name='DitherMode', description='The color dithering algorithm of the palette. Valid values: sierra and bayer.', example='sierra'),
        finalDelay?: string(name='FinalDelay', description='The duration for which the final frame is paused. Unit: centiseconds.', example='0'),
        isCustomPalette?: string(name='IsCustomPalette', description='Indicates whether the custom palette is used.', example='false'),
        loop?: string(name='Loop', description='The loop count.', example='0'),
      }(name='Gif', description='The transmuxing configuration for GIF.'),
      segment?: {
        duration?: string(name='Duration', description='The length of the segment. Unit: seconds.', example='10'),
      }(name='Segment', description='The segment configuration.'),
      webp?: {
        loop?: string(name='Loop', description='The loop count.', example='0'),
      }(name='Webp', description='The transmuxing configuration for WebP.'),
    }(name='MuxConfig', description='The transmuxing configuration.'),
    name?: string(name='Name', description='The name of the template.', example='mps-example'),
    state?: string(name='State', description='The status of the template.

*   **Normal**: The template is normal.
*   **Deleted**: The template is deleted.', example='Narmal'),
    transConfig?: {
      adjDarMethod?: string(name='AdjDarMethod', description='The method of resolution adjustment. Default value: **none** Valid values:

*   **rescale**: The input video is rescaled.
*   **crop**: The input video is cropped.
*   **none**: No change is made.', example='rescale'),
      isCheckAudioBitrate?: string(name='IsCheckAudioBitrate', description='Indicates whether to check the audio bitrate.

If this feature is enabled and the system detects that the audio bitrate of the output file is greater than that of the input file, the audio bitrate of the input file is retained after transcoding.

*   **true**: The audio bitrate is checked.
*   **false**: The audio bitrate is not checked.
*   Default value: **false**.', example='true'),
      isCheckAudioBitrateFail?: string(name='IsCheckAudioBitrateFail', description='Indicates whether to check the audio bitrate. If the bitrate of the output audio is greater than that of the input audio, this parameter has a higher priority than the **IsCheckAudioBitrate** parameter.

*   **true**: The audio bitrate is checked. If the bitrate of the output audio is greater than that of the input audio, the input file is not transcoded.
*   **false**: The audio bitrate is not checked.
*   Default value: **false**.', example='true'),
      isCheckReso?: string(name='IsCheckReso', description='Indicates whether to check the resolution.

*   **true**: The resolution is checked.
*   **false**: The resolution is not checked.
*   Default value: **false**.

>  If this feature is enabled and the system detects that the resolution of the output file is higher than that of the input file based on the width or height, the resolution of the input file is retained after transcoding.', example='true'),
      isCheckResoFail?: string(name='IsCheckResoFail', description='Indicates whether to check the resolution.

*   **true**: The resolution is checked.
*   **false**: The resolution is not checked.
*   Default value: **false**.

>  If this feature is enabled and the system detects that the resolution of the output file is higher than that of the input file based on the width or height, an error that indicates a transcoding failure is returned.', example='true'),
      isCheckVideoBitrate?: string(name='IsCheckVideoBitrate', description='Indicates whether to check the video bitrate.

*   **true**: The video bitrate is checked.
*   **false**: The video bitrate is not checked.
*   Default value: **false**.

>  If this feature is enabled and the system detects that the video bitrate of the output file is greater than that of the input file, the video bitrate of the input file is retained after transcoding.', example='true'),
      isCheckVideoBitrateFail?: string(name='IsCheckVideoBitrateFail', description='If the video bitrate of the output file is found to be greater than that of the input file, this parameter has a higher priority than the IsCheckVideoBitrate parameter. Valid values:

*   **true**: The video bitrate is checked. If the video bitrate of the output file is higher than that of the input file, the input file is not transcoded.
*   **false**: The video bitrate is not checked.
*   Default value: **false**.', example='true'),
      transMode?: string(name='TransMode', description='The transcoding mode. Valid values:

*   **onepass**
*   **twopass**
*   **CBR**
*   Default value: **onepass**.', example='onepass'),
    }(name='TransConfig', description='The general transcoding configuration.'),
    video?: {
      bitrate?: string(name='Bitrate', description='The bitrate of the output video. Unit: Kbit/s.', example='500'),
      bitrateBnd?: {
        max?: string(name='Max', description='The maximum bitrate.', example='1500'),
        min?: string(name='Min', description='The minimum bitrate.', example='800'),
      }(name='BitrateBnd', description='The bitrate range of the video.'),
      bufsize?: string(name='Bufsize', description='The size of the buffer.

*   Default value: **6000**.
*   Unit: KB.', example='6000'),
      codec?: string(name='Codec', description='The video codec. Valid video codecs: H.264, H.265, GIF, and WebP. Default video codec: **H.264**.', example='H.264'),
      crf?: string(name='Crf', description='The constant rate factor. Default value when the video codec is set to H.264: **23**. Default value when the video codec is set to H.265: **26**.

>  If this parameter is specified, the setting of the Bitrate parameter becomes invalid.', example='15'),
      crop?: string(name='Crop', description='The method of video cropping. Valid values:

*   **border**: automatically detects and removes borders.
*   Value in the format of **width:height:left:top**: crops the video image based on the custom setting. Example: 1280:800:0:140.', example='border'),
      degrain?: string(name='Degrain', description='The level of quality control on the video.', example='10'),
      fps?: string(name='Fps', description='The frame rate of the video. Default value: the frame rate of the input file. The value is 60 if the frame rate of the input file exceeds 60. Unit: frames per second.', example='25'),
      gop?: string(name='Gop', description='The GOP size. The GOP size can be the maximum interval of keyframes or the maximum number of frames in a frame group. If you specified the maximum interval, the value contains the unit (s). If you specified the maximum number of frames, the value does not contain a unit. Default value: **10s**.', example='10s'),
      height?: string(name='Height', description='The height of the video.

*   Unit: pixel.
*   Default value: the height of the input video.', example='800'),
      longShortMode?: string(name='LongShortMode', description='Indicates whether to enable the auto-rotate screen feature. Default value: **false**. Valid values:

*   **true**: The auto-rotate screen feature is enabled.
*   **false**: The auto-rotate screen feature is disabled.

>  If this feature is enabled, the width of the output video corresponds to the long side of the input video, which is the height of the input video in portrait mode. The height of the output video corresponds to the short side of the input video, which is the width of the input video in portrait mode.', example='false'),
      maxFps?: string(name='MaxFps', description='The maximum frame rate.', example='60'),
      maxrate?: string(name='Maxrate', description='The maximum bitrate of the output video. Unit: Kbit/s.', example='500'),
      pad?: string(name='Pad', description='The black borders to be added to the video. Format: width:height:left:top.', example='1280:800:0:140'),
      pixFmt?: string(name='PixFmt', description='The pixel format for video color encoding. Standard pixel formats such as yuv420p and yuvj420p are supported. The default pixel format can be yuv420p or the original color format.', example='yuv420p'),
      preset?: string(name='Preset', description='The preset video algorithm. Default value: **medium**. Valid values:

*   **veryfast**
*   **fast**
*   **medium**
*   **slow**
*   **slower**

>  This parameter is valid only when the video codec is set to H.264.', example='fast'),
      profile?: string(name='Profile', description='The codec profile. Valid values:

*   **baseline**: suitable for mobile devices
*   **main**: suitable for standard-definition devices
*   **high**: suitable for high-definition devices
*   Default value: **high**.

If multiple definitions exist, we recommend that you set this parameter to baseline for the lowest definition to ensure normal playback on low-end devices. Set this parameter to main or high for other definitions.

>  This parameter is valid only when the video codec is set to H.264.', example='high'),
      qscale?: string(name='Qscale', description='The level of the independent denoising algorithm.', example='1'),
      remove?: string(name='Remove', description='Indicates whether to delete the video stream.

*   **true**: The video stream is deleted.
*   **false**: The video stream is retained.
*   Default value: **false**.', example='false'),
      resoPriority?: string(name='ResoPriority', description='The policy of resolution adjustment.', example='0'),
      scanMode?: string(name='ScanMode', description='The scan mode. Valid values:

*   **interlaced**
*   **progressive**', example='interlaced'),
      width?: string(name='Width', description='The width of the video.

*   Default value: the width of the input video.****
*   Unit: pixel.', example='256'),
    }(name='Video', description='The video codec configuration.'),
  }(name='Template', description='The details of the transcoding template.'),
}

model AddTemplateResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: AddTemplateResponseBody(name='body'),
}

/**
  * You can call this operation to set the parameters that are related to the container, audio streams, and video streams. For some parameters, if you do not specify them, streams that are generated by using the template do not have the corresponding settings.
  * ## Limits on QPS
  * You can call this operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request AddTemplateRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return AddTemplateResponse
 */
async function addTemplateWithOptions(request: AddTemplateRequest, runtime: Util.RuntimeOptions): AddTemplateResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.audio)) {
    query['Audio'] = request.audio;
  }
  if (!Util.isUnset(request.container)) {
    query['Container'] = request.container;
  }
  if (!Util.isUnset(request.muxConfig)) {
    query['MuxConfig'] = request.muxConfig;
  }
  if (!Util.isUnset(request.name)) {
    query['Name'] = request.name;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.transConfig)) {
    query['TransConfig'] = request.transConfig;
  }
  if (!Util.isUnset(request.video)) {
    query['Video'] = request.video;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'AddTemplate',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * You can call this operation to set the parameters that are related to the container, audio streams, and video streams. For some parameters, if you do not specify them, streams that are generated by using the template do not have the corresponding settings.
  * ## Limits on QPS
  * You can call this operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request AddTemplateRequest
  * @return AddTemplateResponse
 */
async function addTemplate(request: AddTemplateRequest): AddTemplateResponse {
  var runtime = new Util.RuntimeOptions{};
  return addTemplateWithOptions(request, runtime);
}

model AddWaterMarkTemplateRequest {
  config?: string(name='Config', description='The configuration of the watermark template. The value is a JSON object. For more information, see [Watermark templates](~~29253~~).

>  If you do not require a positive correlation between the size of text in the watermark and the resolution, you can enable adaptation for the watermark. To do so, add `[\\"adaptive\\"]=true` to the text parameter of the watermark.', example='{"Width":"10","Height":"30","Dx":"10","Dy":"5","ReferPos":"TopRight","Type":"Image","Timeline":{"Start":"0","Duration":"10"}}'),
  name?: string(name='Name', description='The name of the watermark template. The value can contain letters and digits and can be up to 128 bytes in size.', example='example-watermark-****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model AddWaterMarkTemplateResponseBody = {
  requestId?: string(name='RequestId', description='The ID of the request.', example='54BB917F-DD35-4F32-BABA-E60E31B21W63'),
  waterMarkTemplate?: {
    dx?: string(name='Dx', description='The horizontal offset. Unit: pixel.', example='10'),
    dy?: string(name='Dy', description='The vertical offset. Unit: pixel.', example='5'),
    height?: string(name='Height', description='The height of the watermark image. Unit: pixel.', example='30'),
    id?: string(name='Id', description='The ID of the watermark template. We recommend that you keep this ID for subsequent operation calls.', example='3780bd69b2b74540bc7b1096f564****'),
    name?: string(name='Name', description='The name of the watermark template.', example='example-watermark-****'),
    ratioRefer?: {
      dx?: string(name='Dx', description='The horizontal offset of the watermark relative to the output video image. Default value: **0**. The default value indicates no offset.

The value can be an integer or a decimal.

*   **Integer**: the vertical offset. This indicates the absolute position. Unit: pixel.
*   **Decimal**: the ratio of the horizontal offset to the width of the output video. The ratio varies based on the size of the video. Four decimal places are supported, such as 0.9999. More decimal places are discarded.', example='0.51'),
      dy?: string(name='Dy', description='The vertical offset of the watermark relative to the output video image. Default value: **0**. The default value indicates no offset. The value can be an integer or a decimal.

*   **Integer**: the vertical offset. This indicates the absolute position. Unit: pixel.
*   **Decimal**: the ratio of the vertical offset to the height of the output video. The ratio varies based on the size of the video. Four decimal places are supported, such as 0.9999. More decimal places are discarded.', example='0.28'),
      height?: string(name='Height', description='The height of the watermark image in the output video. The value can be an integer or a decimal.

*   **Integer**: the height of the watermark image. This indicates the absolute position. Unit: pixel.
*   **Decimal**: the ratio of the height of the watermark image to the height of the output video. The ratio varies based on the size of the video. Four decimal places are supported, such as 0.9999. More decimal places are discarded.', example='0.33'),
      width?: string(name='Width', description='The width of the watermark image in the output video. The value can be an integer or a decimal.

*   **Integer**: the width of the watermark image. This indicates the absolute position. Unit: pixel.
*   **Decimal**: the ratio of the width of the watermark image to the width of the output video. The ratio varies based on the size of the video. Four decimal places are supported, such as 0.9999. More decimal places are discarded.', example='0.36'),
    }(name='RatioRefer', description='The values of the Height, Width, Dx, and Dy parameters relative to the reference edges. If the values of the Height, Width, Dx, and Dy parameters are decimals between 0 and 1, the values are calculated by referring to the following edges in sequence:

*   **Width**: the width edge.
*   **Height**: the height edge.
*   **Long**: the long edge.
*   **Short**: the short edge.'),
    referPos?: string(name='ReferPos', description='The position of the watermark. Valid values:

*   **TopRight**: the upper-right corner.
*   **TopLeft**: the upper-left corner.
*   **BottomRight**: the lower-right corner.
*   **BottomLeft**: the lower-left corner.', example='TopRight'),
    state?: string(name='State', description='The status of the watermark template. Valid values:

*   **Normal**: The watermark template is normal.
*   **Deleted**: The watermark template is deleted.', example='Normal'),
    timeline?: {
      duration?: string(name='Duration', description='The display duration of the watermark. Default value: **ToEND**. The default value indicates that the watermark is displayed until the video ends.', example='ToEND'),
      start?: string(name='Start', description='The beginning of the time range during which the watermark is displayed.

*   Unit: seconds.
*   Default value: **0**.', example='0'),
    }(name='Timeline', description='The timeline of the watermark.'),
    type?: string(name='Type', description='The type of the watermark. Valid values:

*   Image: an image watermark.
*   Text: a text watermark.', example='Image'),
    width?: string(name='Width', description='The width of the watermark image. Unit: pixel.', example='10'),
  }(name='WaterMarkTemplate', description='The details of the watermark template.'),
}

model AddWaterMarkTemplateResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: AddWaterMarkTemplateResponseBody(name='body'),
}

/**
  * After you create a watermark template by calling this operation, you can specify the watermark template and watermark asset when you [submit a transcoding job](~~29226~~). This allows you to add watermark information to the output video.
  * ## QPS limit
  * You can call this API operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request AddWaterMarkTemplateRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return AddWaterMarkTemplateResponse
 */
async function addWaterMarkTemplateWithOptions(request: AddWaterMarkTemplateRequest, runtime: Util.RuntimeOptions): AddWaterMarkTemplateResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.config)) {
    query['Config'] = request.config;
  }
  if (!Util.isUnset(request.name)) {
    query['Name'] = request.name;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'AddWaterMarkTemplate',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * After you create a watermark template by calling this operation, you can specify the watermark template and watermark asset when you [submit a transcoding job](~~29226~~). This allows you to add watermark information to the output video.
  * ## QPS limit
  * You can call this API operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request AddWaterMarkTemplateRequest
  * @return AddWaterMarkTemplateResponse
 */
async function addWaterMarkTemplate(request: AddWaterMarkTemplateRequest): AddWaterMarkTemplateResponse {
  var runtime = new Util.RuntimeOptions{};
  return addWaterMarkTemplateWithOptions(request, runtime);
}

model BindInputBucketRequest {
  bucket?: string(name='Bucket', description='The name of the input media bucket to be bound. The name can be up to 64 bytes in size. To obtain the media bucket name, you can log on to the **ApsaraVideo Media Processing (MPS) console** and choose **Workflows** > **Media Buckets** in the left-side navigation pane.

>  The bucket name can contain lowercase letters, digits, and hyphens (-), and cannot start or end with a hyphen (-).', example='example-bucket-****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  referer?: string(name='Referer', description='The settings of Object Storage Service (OSS) hotlink protection. For more information, see [Hotlink protection](~~31869~~).', example='http://www.example.com'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model BindInputBucketResponseBody = {
  requestId?: string(name='RequestId', description='The ID of the request.', example='4AEA0480-32F4-1656-92B3-F4D4CDE6BBB3'),
}

model BindInputBucketResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: BindInputBucketResponseBody(name='body'),
}

/**
  * Before you call this operation to bind a media bucket, you must create a media bucket. For more information, see [Library settings](~~42430~~).
  * ##  QPS limit
  * You can call this operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request BindInputBucketRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return BindInputBucketResponse
 */
async function bindInputBucketWithOptions(request: BindInputBucketRequest, runtime: Util.RuntimeOptions): BindInputBucketResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.bucket)) {
    query['Bucket'] = request.bucket;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.referer)) {
    query['Referer'] = request.referer;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'BindInputBucket',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * Before you call this operation to bind a media bucket, you must create a media bucket. For more information, see [Library settings](~~42430~~).
  * ##  QPS limit
  * You can call this operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request BindInputBucketRequest
  * @return BindInputBucketResponse
 */
async function bindInputBucket(request: BindInputBucketRequest): BindInputBucketResponse {
  var runtime = new Util.RuntimeOptions{};
  return bindInputBucketWithOptions(request, runtime);
}

model BindOutputBucketRequest {
  bucket?: string(name='Bucket', description='The name of the Object Storage Service (OSS) bucket that you want to bind. The name can be up to 64 bytes in size and can contain letters, digits, and hyphens (-). The name cannot start with a special character.', example='example-bucket-****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model BindOutputBucketResponseBody = {
  requestId?: string(name='RequestId', description='The ID of the request.', example='D0F80646-90D4-402F-9D56-CEFEAA6BCC9B'),
}

model BindOutputBucketResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: BindOutputBucketResponseBody(name='body'),
}

/**
  * ## Usage notes
  * You must create a media bucket before you call this operation. For more information, see [Add media buckets](~~42430~~).
  * ## QPS limits
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
  * @param request BindOutputBucketRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return BindOutputBucketResponse
 */
async function bindOutputBucketWithOptions(request: BindOutputBucketRequest, runtime: Util.RuntimeOptions): BindOutputBucketResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.bucket)) {
    query['Bucket'] = request.bucket;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'BindOutputBucket',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * ## Usage notes
  * You must create a media bucket before you call this operation. For more information, see [Add media buckets](~~42430~~).
  * ## QPS limits
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](~~342832~~).
  *
  * @param request BindOutputBucketRequest
  * @return BindOutputBucketResponse
 */
async function bindOutputBucket(request: BindOutputBucketRequest): BindOutputBucketResponse {
  var runtime = new Util.RuntimeOptions{};
  return bindOutputBucketWithOptions(request, runtime);
}

model CancelJobRequest {
  jobId?: string(name='JobId', example='d1ce4d3efcb549419193f50f1fcd****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model CancelJobResponseBody = {
  jobId?: string(name='JobId', example='d1ce4d3efcb549419193f50f1fcd****'),
  requestId?: string(name='RequestId', example='338CA33A-AE83-5DF4-B6F2-C6D3ED8143F5'),
}

model CancelJobResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: CancelJobResponseBody(name='body'),
}

/**
  * > 
  * *   Only tasks in “Submitted” state can be canceled.
  * *   We recommend that you first call the UpdatePipeline API (UpdatePipeline) to set the MPS queue status to Paused to stop task scheduling, and then call this API to cancel the tasks. After canceling the tasks, recover the MPS queue status to Active. In this way, tasks in the MPS queue can be scheduled and executed.
  *
  * @param request CancelJobRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return CancelJobResponse
 */
async function cancelJobWithOptions(request: CancelJobRequest, runtime: Util.RuntimeOptions): CancelJobResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.jobId)) {
    query['JobId'] = request.jobId;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'CancelJob',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * > 
  * *   Only tasks in “Submitted” state can be canceled.
  * *   We recommend that you first call the UpdatePipeline API (UpdatePipeline) to set the MPS queue status to Paused to stop task scheduling, and then call this API to cancel the tasks. After canceling the tasks, recover the MPS queue status to Active. In this way, tasks in the MPS queue can be scheduled and executed.
  *
  * @param request CancelJobRequest
  * @return CancelJobResponse
 */
async function cancelJob(request: CancelJobRequest): CancelJobResponse {
  var runtime = new Util.RuntimeOptions{};
  return cancelJobWithOptions(request, runtime);
}

model CreateCustomEntityRequest {
  algorithm?: string(name='Algorithm', example='landmark'),
  customEntityInfo?: string(name='CustomEntityInfo', example='{ "finegrainName":"examplName" }'),
  customEntityName?: string(name='CustomEntityName'),
  customGroupId?: string(name='CustomGroupId', example='1'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model CreateCustomEntityResponseBody = {
  customEntityId?: string(name='CustomEntityId', example='1'),
  requestId?: string(name='RequestId', example='580e8ce3-3b80-44c5-9f3f-36ac3cc5bdd5'),
}

model CreateCustomEntityResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: CreateCustomEntityResponseBody(name='body'),
}

async function createCustomEntityWithOptions(request: CreateCustomEntityRequest, runtime: Util.RuntimeOptions): CreateCustomEntityResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.algorithm)) {
    query['Algorithm'] = request.algorithm;
  }
  if (!Util.isUnset(request.customEntityInfo)) {
    query['CustomEntityInfo'] = request.customEntityInfo;
  }
  if (!Util.isUnset(request.customEntityName)) {
    query['CustomEntityName'] = request.customEntityName;
  }
  if (!Util.isUnset(request.customGroupId)) {
    query['CustomGroupId'] = request.customGroupId;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'CreateCustomEntity',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

async function createCustomEntity(request: CreateCustomEntityRequest): CreateCustomEntityResponse {
  var runtime = new Util.RuntimeOptions{};
  return createCustomEntityWithOptions(request, runtime);
}

model CreateCustomGroupRequest {
  algorithm?: string(name='Algorithm', example='landmark'),
  customGroupDescription?: string(name='CustomGroupDescription'),
  customGroupName?: string(name='CustomGroupName'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model CreateCustomGroupResponseBody = {
  customGroupId?: string(name='CustomGroupId', example='129****'),
  requestId?: string(name='RequestId', example='580e8ce3-3b80-44c5-9f3f-36ac3cc5bdd5'),
}

model CreateCustomGroupResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: CreateCustomGroupResponseBody(name='body'),
}

async function createCustomGroupWithOptions(request: CreateCustomGroupRequest, runtime: Util.RuntimeOptions): CreateCustomGroupResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.algorithm)) {
    query['Algorithm'] = request.algorithm;
  }
  if (!Util.isUnset(request.customGroupDescription)) {
    query['CustomGroupDescription'] = request.customGroupDescription;
  }
  if (!Util.isUnset(request.customGroupName)) {
    query['CustomGroupName'] = request.customGroupName;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'CreateCustomGroup',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

async function createCustomGroup(request: CreateCustomGroupRequest): CreateCustomGroupResponse {
  var runtime = new Util.RuntimeOptions{};
  return createCustomGroupWithOptions(request, runtime);
}

model CreateFpShotDBRequest {
  config?: string(name='Config', description='The configurations of the media fingerprint library. By default, this parameter is empty. You can customize the configurations based on your business requirements. The value is a string in the JSON format.', example='null'),
  description?: string(name='Description', description='The description of the media fingerprint library.'),
  modelId?: int32(name='ModelId', description='The model ID of the media fingerprint library. To create a text fingerprint library, set the parameter to **11**. To create a video fingerprint library, set the parameter to **12**. To create an audio fingerprint library, set the parameter to **13**. A value of **14** indicates that the library is an image fingerprint library.', example='11'),
  name?: string(name='Name', description='The name of the media fingerprint library to be created.', example='example name'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model CreateFpShotDBResponseBody = {
  fpShotDB?: {
    config?: string(name='Config', description='The configurations of the media fingerprint library.', example='null'),
    description?: string(name='Description', description='The description of the media fingerprint library.'),
    fpDBId?: string(name='FpDBId', description='The ID of the media fingerprint library. We recommend that you keep this ID for subsequent operation calls.', example='88c6ca184c0e47098a5b665e2a12****'),
    modelId?: int32(name='ModelId', description='The model ID of the media fingerprint library.', example='11'),
    name?: string(name='Name', description='The name of the media fingerprint library.', example='example-name-****'),
    state?: string(name='State', description='The status of the media fingerprint library. After the media fingerprint library is created, it enters the **offline** state. After the media fingerprint library is processed at the backend, it enters the **active** state.', example='offline'),
  }(name='FpShotDB', description='The details of the media fingerprint library.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='25818875-5F78-4A13-BEF6-D7393642CA58'),
}

model CreateFpShotDBResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: CreateFpShotDBResponseBody(name='body'),
}

/**
  * *   You can call this operation to submit a job to create a video or text fingerprint library. You can use a text fingerprint library to store fingerprints for text.
  * *   You can submit a job of creating a text fingerprint library only in the China (Shanghai) region. By default, you can submit up to 10 jobs of creating a video fingerprint library to an ApsaraVideo Media Processing (MPS) queue at a time. If you submit more than 10 jobs at a time, the call may fail.
  * ## QPS limit
  * You can call this operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request CreateFpShotDBRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return CreateFpShotDBResponse
 */
async function createFpShotDBWithOptions(request: CreateFpShotDBRequest, runtime: Util.RuntimeOptions): CreateFpShotDBResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.config)) {
    query['Config'] = request.config;
  }
  if (!Util.isUnset(request.description)) {
    query['Description'] = request.description;
  }
  if (!Util.isUnset(request.modelId)) {
    query['ModelId'] = request.modelId;
  }
  if (!Util.isUnset(request.name)) {
    query['Name'] = request.name;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'CreateFpShotDB',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * *   You can call this operation to submit a job to create a video or text fingerprint library. You can use a text fingerprint library to store fingerprints for text.
  * *   You can submit a job of creating a text fingerprint library only in the China (Shanghai) region. By default, you can submit up to 10 jobs of creating a video fingerprint library to an ApsaraVideo Media Processing (MPS) queue at a time. If you submit more than 10 jobs at a time, the call may fail.
  * ## QPS limit
  * You can call this operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request CreateFpShotDBRequest
  * @return CreateFpShotDBResponse
 */
async function createFpShotDB(request: CreateFpShotDBRequest): CreateFpShotDBResponse {
  var runtime = new Util.RuntimeOptions{};
  return createFpShotDBWithOptions(request, runtime);
}

model DeactivateMediaWorkflowRequest {
  mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the media workflow that you want to deactivate. To obtain the ID of the media workflow, you can log on to the **ApsaraVideo Media Processing (MPS) console** and choose **Workflows** > **Workflow Settings** in the left-side navigation pane.', example='93ab850b4f6f44eab54b6e9181d4****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model DeactivateMediaWorkflowResponseBody = {
  mediaWorkflow?: {
    creationTime?: string(name='CreationTime', description='The time when the media workflow was created.', example='2016-04-01T05:29:37Z'),
    mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the media workflow that is deactivated.', example='93ab850b4f6f44eab54b6e9181d4****'),
    name?: string(name='Name', description='The name of the media workflow that is deactivated.', example='example-mediaworkflow-****'),
    state?: string(name='State', description='The status of the media workflow. The value is **Inactive**.', example='Inactive'),
    topology?: string(name='Topology', description='The topology of the media workflow.', example='{mediaworkflow","State":"Active","Topology":"{\\"Activities\\":{\\"Act-Start\\":{\\"Parameters\\":{\\"PipelineId\\":\\"130266f58161436a80bf07cb12c8****\\",\\"InputFile\\":\\"{\\\\\\"Bucket\\\\\\": \\\\\\"example-bucket-****\\\\\\",\\\\\\"Location\\\\\\": \\\\\\"cn-shanghai\\\\\\"}\\"},\\"Type\\":\\"Start\\"},\\"Act-Report\\":{\\"Parameters\\":{},\\"Type\\":\\"Report\\"},\\"Act-Transcode-M3U8\\":{\\"Parameters\\":{\\"Outputs\\":\\"[{\\\\\\"Object\\\\\\":\\\\\\"transcode/{ObjectPrefix}{FileName}\\\\\\",\\\\\\"TemplateId\\\\\\": \\\\\\"957d1719ee85ed6527b90cf62726****\\\\\\"}]\\",\\"OutputBucket\\":\\"example-bucket-****\\",\\"OutputLocation\\":\\"cn-shanghai\\"},\\"Type\\":\\"Transcode\\"}},\\"Dependencies\\":{\\"Act-Start\\":[\\"Act-Transcode-M3U8\\"],\\"Act-Report\\":[],\\"Act-Transcode-M3U8\\":[\\"Act-Report\\"]}}","MediaWorkflowId":"93ab850b4f6f44eab54b6e91d24d****"}]},"RequestId":"16CD0CDD-457E-420D-9755-8385075A1234"}'),
  }(name='MediaWorkflow', description='The details of the media workflow.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='16CD0CDD-457E-420D-9755-8385075A1234'),
}

model DeactivateMediaWorkflowResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: DeactivateMediaWorkflowResponseBody(name='body'),
}

/**
  * *   After you deactivate a media workflow, you can modify the workflow information.
  * *   After you delete or deactivate a media workflow, the workflow cannot be used. In this case, the workflow is not automatically triggered when you upload a file to the bucket specified by the workflow.
  * ## Limits on QPS
  * You can call this operation up to 100 times per second. If the number of the calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request DeactivateMediaWorkflowRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return DeactivateMediaWorkflowResponse
 */
async function deactivateMediaWorkflowWithOptions(request: DeactivateMediaWorkflowRequest, runtime: Util.RuntimeOptions): DeactivateMediaWorkflowResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.mediaWorkflowId)) {
    query['MediaWorkflowId'] = request.mediaWorkflowId;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'DeactivateMediaWorkflow',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * *   After you deactivate a media workflow, you can modify the workflow information.
  * *   After you delete or deactivate a media workflow, the workflow cannot be used. In this case, the workflow is not automatically triggered when you upload a file to the bucket specified by the workflow.
  * ## Limits on QPS
  * You can call this operation up to 100 times per second. If the number of the calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request DeactivateMediaWorkflowRequest
  * @return DeactivateMediaWorkflowResponse
 */
async function deactivateMediaWorkflow(request: DeactivateMediaWorkflowRequest): DeactivateMediaWorkflowResponse {
  var runtime = new Util.RuntimeOptions{};
  return deactivateMediaWorkflowWithOptions(request, runtime);
}

model DeleteCustomEntityRequest {
  algorithm?: string(name='Algorithm', example='landmark'),
  customEntityId?: string(name='CustomEntityId', example='1'),
  customGroupId?: string(name='CustomGroupId', example='1'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model DeleteCustomEntityResponseBody = {
  requestId?: string(name='RequestId', example='580e8ce3-3b80-44c5-9f3f-36ac3cc5bdd5'),
}

model DeleteCustomEntityResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: DeleteCustomEntityResponseBody(name='body'),
}

async function deleteCustomEntityWithOptions(request: DeleteCustomEntityRequest, runtime: Util.RuntimeOptions): DeleteCustomEntityResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.algorithm)) {
    query['Algorithm'] = request.algorithm;
  }
  if (!Util.isUnset(request.customEntityId)) {
    query['CustomEntityId'] = request.customEntityId;
  }
  if (!Util.isUnset(request.customGroupId)) {
    query['CustomGroupId'] = request.customGroupId;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'DeleteCustomEntity',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

async function deleteCustomEntity(request: DeleteCustomEntityRequest): DeleteCustomEntityResponse {
  var runtime = new Util.RuntimeOptions{};
  return deleteCustomEntityWithOptions(request, runtime);
}

model DeleteCustomGroupRequest {
  algorithm?: string(name='Algorithm', example='landmark'),
  customGroupId?: string(name='CustomGroupId', example='1'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model DeleteCustomGroupResponseBody = {
  requestId?: string(name='RequestId', example='580e8ce3-3b80-44c5-9f3f-36ac3cc5bdd5'),
}

model DeleteCustomGroupResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: DeleteCustomGroupResponseBody(name='body'),
}

async function deleteCustomGroupWithOptions(request: DeleteCustomGroupRequest, runtime: Util.RuntimeOptions): DeleteCustomGroupResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.algorithm)) {
    query['Algorithm'] = request.algorithm;
  }
  if (!Util.isUnset(request.customGroupId)) {
    query['CustomGroupId'] = request.customGroupId;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'DeleteCustomGroup',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

async function deleteCustomGroup(request: DeleteCustomGroupRequest): DeleteCustomGroupResponse {
  var runtime = new Util.RuntimeOptions{};
  return deleteCustomGroupWithOptions(request, runtime);
}

model DeleteCustomViewRequest {
  algorithm?: string(name='Algorithm', example='landmark'),
  customEntityId?: string(name='CustomEntityId', example='1'),
  customGroupId?: string(name='CustomGroupId', example='1'),
  customViewId?: string(name='CustomViewId', example='1'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model DeleteCustomViewResponseBody = {
  requestId?: string(name='RequestId', example='580e8ce3-3b80-44c5-9f3f-36ac3cc5bdd5'),
}

model DeleteCustomViewResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: DeleteCustomViewResponseBody(name='body'),
}

async function deleteCustomViewWithOptions(request: DeleteCustomViewRequest, runtime: Util.RuntimeOptions): DeleteCustomViewResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.algorithm)) {
    query['Algorithm'] = request.algorithm;
  }
  if (!Util.isUnset(request.customEntityId)) {
    query['CustomEntityId'] = request.customEntityId;
  }
  if (!Util.isUnset(request.customGroupId)) {
    query['CustomGroupId'] = request.customGroupId;
  }
  if (!Util.isUnset(request.customViewId)) {
    query['CustomViewId'] = request.customViewId;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'DeleteCustomView',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

async function deleteCustomView(request: DeleteCustomViewRequest): DeleteCustomViewResponse {
  var runtime = new Util.RuntimeOptions{};
  return deleteCustomViewWithOptions(request, runtime);
}

model DeleteMediaRequest {
  mediaIds?: string(name='MediaIds', example='3e1cd21131a94525be55acf65888****,3e6149d5a8c944c09b1a8d2dc3e4****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model DeleteMediaResponseBody = {
  requestId?: string(name='RequestId', example='05F8B913-E9F3-4A6F-9922-48CADA0FFAAD'),
}

model DeleteMediaResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: DeleteMediaResponseBody(name='body'),
}

async function deleteMediaWithOptions(request: DeleteMediaRequest, runtime: Util.RuntimeOptions): DeleteMediaResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.mediaIds)) {
    query['MediaIds'] = request.mediaIds;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'DeleteMedia',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

async function deleteMedia(request: DeleteMediaRequest): DeleteMediaResponse {
  var runtime = new Util.RuntimeOptions{};
  return deleteMediaWithOptions(request, runtime);
}

model DeleteMediaTagRequest {
  mediaId?: string(name='MediaId', description='The ID of the media file for which you want to remove a tag. To obtain the ID of a media file, you can call the [AddMedia](~~44458~~) operation. Alternatively, perform the following operations in the ApsaraVideo Media Processing (MPS) console: In the left-side navigation pane, choose **Media Management** > **Media List**. Find the required video and click **Manage**. The ID of the video is displayed on the Basics tab.', example='3e6149d5a8c944c09b1a8d2dc3e4****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  tag?: string(name='Tag', description='The media tag that you want to remove. The value is encoded in UTF-8 and can be up to 32 bytes in length.

>  You can remove only one tag at a time.', example='tag1'),
}

model DeleteMediaTagResponseBody = {
  requestId?: string(name='RequestId', description='The ID of the request.', example='283DC68C-146F-4489-A2A1-2F88F1472A56'),
}

model DeleteMediaTagResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: DeleteMediaTagResponseBody(name='body'),
}

/**
  * You can call this operation to remove only one tag at a time.
  * ## Limits on QPS
  * You can call this operation up to 100 times per second. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request DeleteMediaTagRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return DeleteMediaTagResponse
 */
async function deleteMediaTagWithOptions(request: DeleteMediaTagRequest, runtime: Util.RuntimeOptions): DeleteMediaTagResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.mediaId)) {
    query['MediaId'] = request.mediaId;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.tag)) {
    query['Tag'] = request.tag;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'DeleteMediaTag',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * You can call this operation to remove only one tag at a time.
  * ## Limits on QPS
  * You can call this operation up to 100 times per second. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request DeleteMediaTagRequest
  * @return DeleteMediaTagResponse
 */
async function deleteMediaTag(request: DeleteMediaTagRequest): DeleteMediaTagResponse {
  var runtime = new Util.RuntimeOptions{};
  return deleteMediaTagWithOptions(request, runtime);
}

model DeleteMediaWorkflowRequest {
  mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the media workflow that you want to delete. To obtain the ID of the media workflow, you can log on to the **ApsaraVideo Media Processing (MPS) console** and choose **Workflows** > **Workflow Settings** in the left-side navigation pane.', example='93ab850b4f6f44eab54b6e9181d4****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model DeleteMediaWorkflowResponseBody = {
  mediaWorkflow?: {
    creationTime?: string(name='CreationTime', description='The time when the media workflow was created.', example='2016-04-01T05:29:37Z'),
    mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the media workflow that is deleted.', example='93ab850b4f6f44eab54b6e9181d4****'),
    name?: string(name='Name', description='The name of the media workflow that is deleted.', example='example-mediaworkflow-****'),
    state?: string(name='State', description='The status of the media workflow. The value is **Deleted**.', example='Deleted'),
    topology?: string(name='Topology', description='The topology of the media workflow.', example='{mediaworkflow","State":"Active","Topology":"{\\"Activities\\":{\\"Act-Start\\":{\\"Parameters\\":{\\"PipelineId\\":\\"130266f58161436a80bf07cb12c8****\\",\\"InputFile\\":\\"{\\\\\\"Bucket\\\\\\": \\\\\\"example-bucket-****\\\\\\",\\\\\\"Location\\\\\\": \\\\\\"cn-shanghai\\\\\\"}\\"},\\"Type\\":\\"Start\\"},\\"Act-Report\\":{\\"Parameters\\":{},\\"Type\\":\\"Report\\"},\\"Act-Transcode-M3U8\\":{\\"Parameters\\":{\\"Outputs\\":\\"[{\\\\\\"Object\\\\\\":\\\\\\"transcode/{ObjectPrefix}{FileName}\\\\\\",\\\\\\"TemplateId\\\\\\": \\\\\\"957d1719ee85ed6527b90cf62726****\\\\\\"}]\\",\\"OutputBucket\\":\\"example-bucket-****\\",\\"OutputLocation\\":\\"cn-shanghai\\"},\\"Type\\":\\"Transcode\\"}},\\"Dependencies\\":{\\"Act-Start\\":[\\"Act-Transcode-M3U8\\"],\\"Act-Report\\":[],\\"Act-Transcode-M3U8\\":[\\"Act-Report\\"]}}","MediaWorkflowId":"93ab850b4f6f44eab54b6e91d24d****"}]},"RequestId":"16CD0CDD-457E-420D-9755-8385075A1234"}'),
  }(name='MediaWorkflow', description='The details of the media workflow.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='7D752035-97DA-54E5-88E2-E8405EEA4394'),
}

model DeleteMediaWorkflowResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: DeleteMediaWorkflowResponseBody(name='body'),
}

/**
  * After you delete or deactivate a media workflow, the workflow cannot be used. In this case, the workflow is not automatically triggered when you upload a file to the bucket specified by the workflow.
  * ## Limits on QPS
  * You can call this operation up to 100 times per second. If the number of the calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request DeleteMediaWorkflowRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return DeleteMediaWorkflowResponse
 */
async function deleteMediaWorkflowWithOptions(request: DeleteMediaWorkflowRequest, runtime: Util.RuntimeOptions): DeleteMediaWorkflowResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.mediaWorkflowId)) {
    query['MediaWorkflowId'] = request.mediaWorkflowId;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'DeleteMediaWorkflow',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * After you delete or deactivate a media workflow, the workflow cannot be used. In this case, the workflow is not automatically triggered when you upload a file to the bucket specified by the workflow.
  * ## Limits on QPS
  * You can call this operation up to 100 times per second. If the number of the calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request DeleteMediaWorkflowRequest
  * @return DeleteMediaWorkflowResponse
 */
async function deleteMediaWorkflow(request: DeleteMediaWorkflowRequest): DeleteMediaWorkflowResponse {
  var runtime = new Util.RuntimeOptions{};
  return deleteMediaWorkflowWithOptions(request, runtime);
}

model DeletePipelineRequest {
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  pipelineId?: string(name='PipelineId', description='The ID of the MPS queue that you want to delete. To obtain the ID of the MPS queue, you can log on to the **MPS console** and choose **Global Settings** > **Pipelines** in the left-side navigation pane.', example='d1ce4d3efcb549419193f50f1fcd****'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model DeletePipelineResponseBody = {
  pipelineId?: string(name='PipelineId', description='The ID of the MPS queue that is deleted.', example='d1ce4d3efcb549419193f50f1fcd****'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='338CA33A-AE83-5DF4-B6F2-C6D3ED8143F5'),
}

model DeletePipelineResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: DeletePipelineResponseBody(name='body'),
}

/**
  * You can call this operation to delete only one MPS queue at a time.
  * ## Limits on QPS
  * You can call this operation up to 100 times per second. If the number of the calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request DeletePipelineRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return DeletePipelineResponse
 */
async function deletePipelineWithOptions(request: DeletePipelineRequest, runtime: Util.RuntimeOptions): DeletePipelineResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.pipelineId)) {
    query['PipelineId'] = request.pipelineId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'DeletePipeline',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * You can call this operation to delete only one MPS queue at a time.
  * ## Limits on QPS
  * You can call this operation up to 100 times per second. If the number of the calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request DeletePipelineRequest
  * @return DeletePipelineResponse
 */
async function deletePipeline(request: DeletePipelineRequest): DeletePipelineResponse {
  var runtime = new Util.RuntimeOptions{};
  return deletePipelineWithOptions(request, runtime);
}

model DeleteSmarttagTemplateRequest {
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  templateId?: string(name='TemplateId', example='05de22f255284c7a8d2aab535dde****'),
}

model DeleteSmarttagTemplateResponseBody = {
  requestId?: string(name='RequestId', example='5F37036F-5267-43F1-AE47-10A18E840739'),
}

model DeleteSmarttagTemplateResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: DeleteSmarttagTemplateResponseBody(name='body'),
}

async function deleteSmarttagTemplateWithOptions(request: DeleteSmarttagTemplateRequest, runtime: Util.RuntimeOptions): DeleteSmarttagTemplateResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.templateId)) {
    query['TemplateId'] = request.templateId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'DeleteSmarttagTemplate',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

async function deleteSmarttagTemplate(request: DeleteSmarttagTemplateRequest): DeleteSmarttagTemplateResponse {
  var runtime = new Util.RuntimeOptions{};
  return deleteSmarttagTemplateWithOptions(request, runtime);
}

model DeleteTemplateRequest {
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  templateId?: string(name='TemplateId', description='The ID of the custom transcoding template that you want to delete. To obtain the ID of the custom transcoding template, you can log on to the **ApsaraVideo Media Processing (MPS) console** and choose **Global Settings** > **Encoding Templates** in the left-side navigation pane.', example='16f01ad6175e4230ac42bb5182cd****'),
}

model DeleteTemplateResponseBody = {
  requestId?: string(name='RequestId', description='The ID of the request.', example='2247541A-9F27-47EE-B6EC-484B5475****'),
  templateId?: string(name='TemplateId', description='The ID of the custom transcoding template that is deleted.', example='16f01ad6175e4230ac42bb5182cd****'),
}

model DeleteTemplateResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: DeleteTemplateResponseBody(name='body'),
}

/**
  * A custom transcoding template cannot be deleted if it is being used by a job that has been submitted.
  * ## Limits on QPS
  * You can call this operation up to 100 times per second. If the number of the calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request DeleteTemplateRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return DeleteTemplateResponse
 */
async function deleteTemplateWithOptions(request: DeleteTemplateRequest, runtime: Util.RuntimeOptions): DeleteTemplateResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.templateId)) {
    query['TemplateId'] = request.templateId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'DeleteTemplate',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * A custom transcoding template cannot be deleted if it is being used by a job that has been submitted.
  * ## Limits on QPS
  * You can call this operation up to 100 times per second. If the number of the calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request DeleteTemplateRequest
  * @return DeleteTemplateResponse
 */
async function deleteTemplate(request: DeleteTemplateRequest): DeleteTemplateResponse {
  var runtime = new Util.RuntimeOptions{};
  return deleteTemplateWithOptions(request, runtime);
}

model DeleteWaterMarkTemplateRequest {
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  waterMarkTemplateId?: string(name='WaterMarkTemplateId', example='3780bd69b2b74540bc7b1096f564****'),
}

model DeleteWaterMarkTemplateResponseBody = {
  requestId?: string(name='RequestId', example='030E2671-806A-52AF-A93C-DA8E308603A6'),
  waterMarkTemplateId?: string(name='WaterMarkTemplateId', example='3780bd69b2b74540bc7b1096f564****'),
}

model DeleteWaterMarkTemplateResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: DeleteWaterMarkTemplateResponseBody(name='body'),
}

async function deleteWaterMarkTemplateWithOptions(request: DeleteWaterMarkTemplateRequest, runtime: Util.RuntimeOptions): DeleteWaterMarkTemplateResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.waterMarkTemplateId)) {
    query['WaterMarkTemplateId'] = request.waterMarkTemplateId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'DeleteWaterMarkTemplate',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

async function deleteWaterMarkTemplate(request: DeleteWaterMarkTemplateRequest): DeleteWaterMarkTemplateResponse {
  var runtime = new Util.RuntimeOptions{};
  return deleteWaterMarkTemplateWithOptions(request, runtime);
}

model ImAuditRequest {
  bizType?: string(name='BizType', example='139440480445****'),
  contents?: string(name='Contents'),
  images?: string(name='Images', example='["http://127.66.**.**/image.jpeg","http://127.66.**.**/photo.jpeg"]'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  scenes?: string(name='Scenes', example='["porn","terrorism","ad"]'),
}

model ImAuditResponseBody = {
  imageQuotaExceed?: boolean(name='ImageQuotaExceed', example='false'),
  imageResults?: {
    result?: [ 
      {
        code?: long(name='code', example='200'),
        dataId?: string(name='dataId', example='uuid-1234-1234-1234'),
        extras?: map[string]any(name='extras'),
        msg?: string(name='msg', example='ok'),
        results?: [ 
          {
            label?: string(name='Label', example='sexy'),
            rate?: double(name='Rate', example='91.54'),
            scene?: string(name='Scene', example='porn'),
            suggestion?: string(name='Suggestion', example='block'),
            frames?: [ 
              {
                rate?: float(name='rate'),
                url?: string(name='url'),
              }
            ](name='frames'),
            hintWordsInfo?: [ 
              {
                context?: string(name='context'),
              }
            ](name='hintWordsInfo'),
            logoData?: [ 
              {
                h?: float(name='h'),
                name?: string(name='name'),
                type?: string(name='type'),
                w?: float(name='w'),
                x?: float(name='x'),
                y?: float(name='y'),
              }
            ](name='logoData'),
            ocrData?: [ string ](name='ocrData'),
            programCodeData?: [ 
              {
                h?: float(name='h'),
                w?: float(name='w'),
                x?: float(name='x'),
                y?: float(name='y'),
              }
            ](name='programCodeData'),
            qrcodeData?: [ string ](name='qrcodeData'),
            qrcodeLocations?: [ 
              {
                h?: float(name='h'),
                qrcode?: string(name='qrcode'),
                w?: float(name='w'),
                x?: float(name='x'),
                y?: float(name='y'),
              }
            ](name='qrcodeLocations'),
            sfaceData?: [ 
              {
                faces?: [ 
                  {
                    idid?: string(name='idid'),
                    name?: string(name='name'),
                    re?: float(name='re'),
                  }
                ](name='faces'),
                h?: float(name='h'),
                w?: float(name='w'),
                x?: float(name='x'),
                y?: float(name='y'),
              }
            ](name='sfaceData'),
          }
        ](name='results'),
        taskId?: string(name='taskId', example='img4wlJcb7p4wH4lAP3111111-12****'),
        url?: string(name='url', example='http://example.com/example-****.jpg'),
      }
    ](name='result'),
  }(name='ImageResults'),
  requestId?: string(name='RequestId', example='5210DBB0-E327-4D45-ADBC-0B83C8793421'),
  textQuotaExceed?: boolean(name='TextQuotaExceed', example='false'),
  textResults?: {
    result?: [ 
      {
        code?: long(name='code', example='200'),
        content?: string(name='content'),
        dataId?: string(name='dataId', example='cfd33235-71a4-468b-8137-a5ffe323****'),
        msg?: string(name='msg', example='OK'),
        results?: [ 
          {
            details?: [ 
              {
                label?: string(name='Label', example='porn'),
                contexts?: [ 
                  {
                    context?: string(name='context'),
                    libCode?: string(name='libCode'),
                    libName?: string(name='libName'),
                    positions?: [ string ](name='positions'),
                    ruleType?: string(name='ruleType'),
                  }
                ](name='contexts'),
              }
            ](name='details'),
            label?: string(name='label', example='porn'),
            rate?: double(name='rate', example='99.90'),
            scene?: string(name='scene', example='antispam'),
            suggestion?: string(name='suggestion', example='block'),
          }
        ](name='results'),
        taskId?: string(name='taskId', example='txt6HB8NQoEbU@5fosnj2xVEM-1t****'),
      }
    ](name='result'),
  }(name='TextResults'),
}

model ImAuditResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: ImAuditResponseBody(name='body'),
}

async function imAuditWithOptions(request: ImAuditRequest, runtime: Util.RuntimeOptions): ImAuditResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.bizType)) {
    query['BizType'] = request.bizType;
  }
  if (!Util.isUnset(request.contents)) {
    query['Contents'] = request.contents;
  }
  if (!Util.isUnset(request.images)) {
    query['Images'] = request.images;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.scenes)) {
    query['Scenes'] = request.scenes;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'ImAudit',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

async function imAudit(request: ImAuditRequest): ImAuditResponse {
  var runtime = new Util.RuntimeOptions{};
  return imAuditWithOptions(request, runtime);
}

model ImportFpShotJobRequest {
  fpDBId?: string(name='FpDBId', example='88c6ca184c0e47098a5b665e2a12****'),
  fpImportConfig?: string(name='FpImportConfig', example='{"SaveType":"onlysave"}'),
  input?: string(name='Input', example='{“Bucket”:”example-bucket”,“Location”:”oss-cn-shanghai”,“Object”:”example.txt”}'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  pipelineId?: string(name='PipelineId', example='ae687c02fe944327ba9631e50da2****'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  userData?: string(name='UserData'),
}

model ImportFpShotJobResponseBody = {
  jobId?: string(name='JobId', example='c074b118ace44395a02063a5ab94****'),
  requestId?: string(name='RequestId', example='25818875-5F78-4A13-BEF6-D7393642CA58'),
}

model ImportFpShotJobResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: ImportFpShotJobResponseBody(name='body'),
}

async function importFpShotJobWithOptions(request: ImportFpShotJobRequest, runtime: Util.RuntimeOptions): ImportFpShotJobResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.fpDBId)) {
    query['FpDBId'] = request.fpDBId;
  }
  if (!Util.isUnset(request.fpImportConfig)) {
    query['FpImportConfig'] = request.fpImportConfig;
  }
  if (!Util.isUnset(request.input)) {
    query['Input'] = request.input;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.pipelineId)) {
    query['PipelineId'] = request.pipelineId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.userData)) {
    query['UserData'] = request.userData;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'ImportFpShotJob',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

async function importFpShotJob(request: ImportFpShotJobRequest): ImportFpShotJobResponse {
  var runtime = new Util.RuntimeOptions{};
  return importFpShotJobWithOptions(request, runtime);
}

model ListAllMediaBucketRequest {
  maximumPageSize?: int32(name='MaximumPageSize', description='The maximum number of media buckets to return. Valid values: 1 to 100. Default value: 50.', example='10'),
  nextPageToken?: string(name='NextPageToken', description='The token that is used to retrieve the next page of the query results. You do not need to specify this parameter in the first request. The response to the first request contains this parameter, which is added to the next request.', example='P2Zqo1PLGhZdygo-ajSsjUX5zrBHCgXy6j4hEvv****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model ListAllMediaBucketResponseBody = {
  mediaBucketList?: {
    mediaBucket?: [ 
    {
      bucket?: string(name='Bucket', description='The name of the media bucket.', example='example-bucket-****'),
      referer?: string(name='Referer', description='The settings of Object Storage Service (OSS) hotlink protection. For more information, see [Hotlink protection](~~31869~~).', example='http://www.example.com'),
      type?: string(name='Type', description='The type of the media bucket. Valid values:

*   Input: input media bucket
*   Output: output media bucket', example='Input'),
    }
  ](name='MediaBucket')
  }(name='MediaBucketList', description='The list of returned media buckets.'),
  nextPageToken?: string(name='NextPageToken', description='The identifier of the next page.', example='P2Zqo1PLGhZdygo-ajSsjUX5zrBHCgXy6j4hEvv****'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='79760D91-D3CF-4165-****-B7E2836EF62A'),
}

model ListAllMediaBucketResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: ListAllMediaBucketResponseBody(name='body'),
}

/**
  * A maximum of 100 media buckets can be returned.
  * ## QPS limit
  * You can call this operation up to 10 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request ListAllMediaBucketRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return ListAllMediaBucketResponse
 */
async function listAllMediaBucketWithOptions(request: ListAllMediaBucketRequest, runtime: Util.RuntimeOptions): ListAllMediaBucketResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.maximumPageSize)) {
    query['MaximumPageSize'] = request.maximumPageSize;
  }
  if (!Util.isUnset(request.nextPageToken)) {
    query['NextPageToken'] = request.nextPageToken;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'ListAllMediaBucket',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * A maximum of 100 media buckets can be returned.
  * ## QPS limit
  * You can call this operation up to 10 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request ListAllMediaBucketRequest
  * @return ListAllMediaBucketResponse
 */
async function listAllMediaBucket(request: ListAllMediaBucketRequest): ListAllMediaBucketResponse {
  var runtime = new Util.RuntimeOptions{};
  return listAllMediaBucketWithOptions(request, runtime);
}

model ListCustomEntitiesRequest {
  algorithm?: string(name='Algorithm', example='landmark'),
  customGroupId?: string(name='CustomGroupId', example='1'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  pageNumber?: int32(name='PageNumber', example='1'),
  pageSize?: int32(name='PageSize', example='10'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model ListCustomEntitiesResponseBody = {
  customEntities?: {
    customEntity?: [ 
    {
      customEntityId?: string(name='CustomEntityId', example='1'),
      customEntityInfo?: string(name='CustomEntityInfo', example='{ "finegrainName":"example" }'),
      customEntityName?: string(name='CustomEntityName', example='exampleName'),
    }
  ](name='CustomEntity')
  }(name='CustomEntities'),
  pageNumber?: int32(name='PageNumber', example='1'),
  pageSize?: int32(name='PageSize', example='10'),
  requestId?: string(name='RequestId', example='580e8ce3-3b80-44c5-9f3f-36ac3cc5bdd5'),
  totalCount?: long(name='TotalCount', example='1'),
}

model ListCustomEntitiesResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: ListCustomEntitiesResponseBody(name='body'),
}

async function listCustomEntitiesWithOptions(request: ListCustomEntitiesRequest, runtime: Util.RuntimeOptions): ListCustomEntitiesResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.algorithm)) {
    query['Algorithm'] = request.algorithm;
  }
  if (!Util.isUnset(request.customGroupId)) {
    query['CustomGroupId'] = request.customGroupId;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.pageNumber)) {
    query['PageNumber'] = request.pageNumber;
  }
  if (!Util.isUnset(request.pageSize)) {
    query['PageSize'] = request.pageSize;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'ListCustomEntities',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

async function listCustomEntities(request: ListCustomEntitiesRequest): ListCustomEntitiesResponse {
  var runtime = new Util.RuntimeOptions{};
  return listCustomEntitiesWithOptions(request, runtime);
}

model ListCustomGroupsRequest {
  algorithm?: string(name='Algorithm', example='landmark'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  pageNumber?: int32(name='PageNumber', example='1'),
  pageSize?: int32(name='PageSize', example='10'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model ListCustomGroupsResponseBody = {
  customGroups?: {
    customGroup?: [ 
    {
      customGroupDescription?: string(name='CustomGroupDescription'),
      customGroupId?: string(name='CustomGroupId', example='1'),
      customGroupName?: string(name='CustomGroupName'),
    }
  ](name='CustomGroup')
  }(name='CustomGroups'),
  pageNumber?: int32(name='PageNumber', example='1'),
  pageSize?: int32(name='PageSize', example='10'),
  requestId?: string(name='RequestId', example='580e8ce3-3b80-44c5-9f3f-36ac3cc5bdd5'),
  totalCount?: long(name='TotalCount', example='1'),
}

model ListCustomGroupsResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: ListCustomGroupsResponseBody(name='body'),
}

async function listCustomGroupsWithOptions(request: ListCustomGroupsRequest, runtime: Util.RuntimeOptions): ListCustomGroupsResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.algorithm)) {
    query['Algorithm'] = request.algorithm;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.pageNumber)) {
    query['PageNumber'] = request.pageNumber;
  }
  if (!Util.isUnset(request.pageSize)) {
    query['PageSize'] = request.pageSize;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'ListCustomGroups',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

async function listCustomGroups(request: ListCustomGroupsRequest): ListCustomGroupsResponse {
  var runtime = new Util.RuntimeOptions{};
  return listCustomGroupsWithOptions(request, runtime);
}

model ListCustomPersonsRequest {
  categoryId?: string(name='CategoryId', example='CategoryId-****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  personId?: string(name='PersonId', example='PersonId-****'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model ListCustomPersonsResponseBody = {
  categories?: {
    category?: [ 
    {
      categoryDescription?: string(name='CategoryDescription', example='CategoryDescription-****'),
      categoryId?: string(name='CategoryId', example='CategoryId-****'),
      categoryName?: string(name='CategoryName', example='CategoryName-****'),
      persons?: {
        person?: [ 
        {
          faces?: {
            face?: [ 
            {
              faceId?: string(name='FaceId', example='15****'),
              imageUrl?: string(name='ImageUrl', example='http://example-****.jpeg'),
            }
          ](name='Face')
          }(name='Faces'),
          personDescription?: string(name='PersonDescription', example='PersonDescription-****'),
          personId?: string(name='PersonId', example='PersonId-****'),
          personName?: string(name='PersonName', example='PersonName-****'),
        }
      ](name='Person')
      }(name='Persons'),
    }
  ](name='Category')
  }(name='Categories'),
  requestId?: string(name='RequestId', example='FD4DED6B-0C26-5A8B-A6BE-4FA542AE4D57'),
}

model ListCustomPersonsResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: ListCustomPersonsResponseBody(name='body'),
}

async function listCustomPersonsWithOptions(request: ListCustomPersonsRequest, runtime: Util.RuntimeOptions): ListCustomPersonsResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.categoryId)) {
    query['CategoryId'] = request.categoryId;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.personId)) {
    query['PersonId'] = request.personId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'ListCustomPersons',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

async function listCustomPersons(request: ListCustomPersonsRequest): ListCustomPersonsResponse {
  var runtime = new Util.RuntimeOptions{};
  return listCustomPersonsWithOptions(request, runtime);
}

model ListCustomViewsRequest {
  algorithm?: string(name='Algorithm', example='landmark'),
  customEntityId?: string(name='CustomEntityId', example='1'),
  customGroupId?: string(name='CustomGroupId', example='1'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  pageNumber?: int32(name='PageNumber', example='1'),
  pageSize?: int32(name='PageSize', example='10'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model ListCustomViewsResponseBody = {
  customViews?: {
    customView?: [ 
    {
      customViewId?: string(name='CustomViewId', example='1'),
      imageUrl?: string(name='ImageUrl', example='http://127.66.**.**/photo.jpeg'),
    }
  ](name='CustomView')
  }(name='CustomViews'),
  pageNumber?: int32(name='PageNumber', example='1'),
  pageSize?: int32(name='PageSize', example='10'),
  requestId?: string(name='RequestId', example='580e8ce3-3b80-44c5-9f3f-36ac3cc5bdd5'),
  totalCount?: long(name='TotalCount', example='1'),
}

model ListCustomViewsResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: ListCustomViewsResponseBody(name='body'),
}

async function listCustomViewsWithOptions(request: ListCustomViewsRequest, runtime: Util.RuntimeOptions): ListCustomViewsResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.algorithm)) {
    query['Algorithm'] = request.algorithm;
  }
  if (!Util.isUnset(request.customEntityId)) {
    query['CustomEntityId'] = request.customEntityId;
  }
  if (!Util.isUnset(request.customGroupId)) {
    query['CustomGroupId'] = request.customGroupId;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.pageNumber)) {
    query['PageNumber'] = request.pageNumber;
  }
  if (!Util.isUnset(request.pageSize)) {
    query['PageSize'] = request.pageSize;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'ListCustomViews',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

async function listCustomViews(request: ListCustomViewsRequest): ListCustomViewsResponse {
  var runtime = new Util.RuntimeOptions{};
  return listCustomViewsWithOptions(request, runtime);
}

model ListFpShotDBRequest {
  fpDBIds?: string(name='FpDBIds', description='The IDs of the media fingerprint libraries. You can obtain the library IDs from the response parameters of the [CreateFpShotDB](~~170149~~) operation. You can query up to 10 libraries at a time. Separate multiple library IDs with commas (,).', example='2288c6ca184c0e47098a5b665e2a12****,ae687c02fe944327ba9631e50da2****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model ListFpShotDBResponseBody = {
  fpShotDBList?: {
    fpShotDB?: [ 
    {
      description?: string(name='Description', description='The description of the media fingerprint library.'),
      fpDBId?: string(name='FpDBId', description='The ID of the media fingerprint library.', example='88c6ca184c0e47098a5b665e2a12****'),
      modelId?: int32(name='ModelId', description='The model ID of the media fingerprint library. A value of **11** indicates that the library is a text fingerprint library. A value of **12** indicates that the library is a video fingerprint library. A value of **13** indicates that the library is an audio fingerprint library. A value of **14** indicates that the library is an image fingerprint library.', example='11'),
      name?: string(name='Name', description='The name of the media fingerprint library.', example='test-****'),
      status?: string(name='Status', description='The status of the media fingerprint library. Default value: **offline**. ****Valid values:

*   **offline**: The media fingerprint library is offline.
*   **active**: The media fingerprint library is online.
*   **paused**: The media fingerprint library is paused.
*   **deleted**: The media fingerprint library is deleted.', example='active'),
    }
  ](name='FpShotDB')
  }(name='FpShotDBList', description='The array of media fingerprint libraries.'),
  nonExistIds?: {
    string?: [ string ](name='String')
  }(name='NonExistIds', description='The IDs of the media fingerprint libraries that do not exist. If all the queried libraries exist, the response does not contain this parameter.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='25818875-5F78-4A13-BEF6-D7393642CA58'),
}

model ListFpShotDBResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: ListFpShotDBResponseBody(name='body'),
}

/**
  * *   You can call this operation to query the status and information of the media fingerprint libraries with the specified IDs.
  * *   You can query text fingerprint libraries only in the China (Shanghai) region.
  * *   You can call this operation to query up to 10 media fingerprint libraries.
  * ## QPS limit
  * You can call this operation up to 500 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request ListFpShotDBRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return ListFpShotDBResponse
 */
async function listFpShotDBWithOptions(request: ListFpShotDBRequest, runtime: Util.RuntimeOptions): ListFpShotDBResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.fpDBIds)) {
    query['FpDBIds'] = request.fpDBIds;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'ListFpShotDB',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * *   You can call this operation to query the status and information of the media fingerprint libraries with the specified IDs.
  * *   You can query text fingerprint libraries only in the China (Shanghai) region.
  * *   You can call this operation to query up to 10 media fingerprint libraries.
  * ## QPS limit
  * You can call this operation up to 500 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request ListFpShotDBRequest
  * @return ListFpShotDBResponse
 */
async function listFpShotDB(request: ListFpShotDBRequest): ListFpShotDBResponse {
  var runtime = new Util.RuntimeOptions{};
  return listFpShotDBWithOptions(request, runtime);
}

model ListFpShotFilesRequest {
  endTime?: string(name='EndTime', example='2022-09-08T23:32:56Z'),
  fpDBId?: string(name='FpDBId', description='The ID of the media fingerprint library whose files you want to query. You can obtain the library ID from the response parameters of the [CreateFpShotDB](~~170149~~) operation.', example='2288c6ca184c0e47098a5b665e2a12****'),
  nextPageToken?: string(name='NextPageToken', description='The token of the next page. This parameter is specified if you require paged queries. When you request the first page of query results, leave the NextPageToken parameter empty. When you request more query results, specify the value of the NextPageToken parameter returned in the query results on the previous page.', example='ae0fd49c0840e14daf0d66a75b83****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  pageSize?: int32(name='PageSize', description='The number of entries to return on each page. Default value: 20.', example='20'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  startTime?: string(name='StartTime', example='2022-09-01T00:00:28Z'),
}

model ListFpShotFilesResponseBody = {
  fpShotFileList?: {
    fpShotFile?: [ 
    {
      fileId?: string(name='FileId', description='The ID of the video file.', example='41e6536e4f2250e2e9bf26cdea19****'),
      inputFile?: {
        bucket?: string(name='Bucket', description='The name of the OSS bucket in which the input file is stored.', example='example-bucket-****'),
        location?: string(name='Location', description='The ID of the OSS region in which the input file resides.', example='oss-cn-beijing'),
        object?: string(name='Object', description='The name of the Object Storage Service (OSS) object that is used as the input file.', example='example-****.mp4'),
      }(name='InputFile', description='The information about the job input.'),
      primaryKey?: string(name='PrimaryKey', description='The unique primary key of the video.', example='fb712a6890464059b1b2ea7c8647****'),
      storeTime?: string(name='StoreTime', example='2022-09-08T23:32:56Z'),
    }
  ](name='FpShotFile')
  }(name='FpShotFileList', description='The media fingerprint files. For more information, see the "FpShotFile" section of the [Data types](~~29251~~) topic.'),
  nextPageToken?: string(name='NextPageToken', description='The token of the next page.', example='ae0fd49c0840e14daf0d66a75b83****'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='25818875-5F78-4A13-BEF6-D7393642CA58'),
}

model ListFpShotFilesResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: ListFpShotFilesResponseBody(name='body'),
}

/**
  * You can call this operation to query media files in a specified media fingerprint library based on the library ID. This operation supports paged queries.
  * ## QPS limit
  * You can call this operation up to 500 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request ListFpShotFilesRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return ListFpShotFilesResponse
 */
async function listFpShotFilesWithOptions(request: ListFpShotFilesRequest, runtime: Util.RuntimeOptions): ListFpShotFilesResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.endTime)) {
    query['EndTime'] = request.endTime;
  }
  if (!Util.isUnset(request.fpDBId)) {
    query['FpDBId'] = request.fpDBId;
  }
  if (!Util.isUnset(request.nextPageToken)) {
    query['NextPageToken'] = request.nextPageToken;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.pageSize)) {
    query['PageSize'] = request.pageSize;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.startTime)) {
    query['StartTime'] = request.startTime;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'ListFpShotFiles',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * You can call this operation to query media files in a specified media fingerprint library based on the library ID. This operation supports paged queries.
  * ## QPS limit
  * You can call this operation up to 500 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request ListFpShotFilesRequest
  * @return ListFpShotFilesResponse
 */
async function listFpShotFiles(request: ListFpShotFilesRequest): ListFpShotFilesResponse {
  var runtime = new Util.RuntimeOptions{};
  return listFpShotFilesWithOptions(request, runtime);
}

model ListFpShotImportJobRequest {
  jobIds?: string(name='JobIds', example='88c6ca184c0e47098a5b665e2a12****,c074b118ace44395a02063a5ab94****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model ListFpShotImportJobResponseBody = {
  fpShotImportJobList?: [ 
    {
      code?: string(name='Code', example='0'),
      createTime?: string(name='CreateTime', example='2020-06-30T00:33:18Z'),
      finishTime?: string(name='FinishTime', example='2020-06-30T00:34:02Z'),
      fpDBId?: string(name='FpDBId', example='2288c6ca184c0e47098a5b665e2a12****'),
      fpImportConfig?: string(name='FpImportConfig', example='""'),
      id?: string(name='Id', example='25bacf2824614bcf9273dc0744db****'),
      input?: string(name='Input', example='{\\"Bucket\\":\\"mts-example****\\",\\"Location\\":\\"oss-cn-shanghai\\",\\"Object\\":\\"test-0828/video/test.mp4\\"}'),
      message?: string(name='Message', example='Success'),
      pipelineId?: string(name='PipelineId', example='ebb51ee30f0b49aba959823fa991****'),
      processMessage?: string(name='ProcessMessage', example='http://testbucket.oss-cn-shanghai.aliyuncs.com/932ajjw***32ssoj_importResult.txt'),
      status?: string(name='Status', example='Success'),
      userData?: string(name='UserData', example='001'),
    }
  ](name='FpShotImportJobList'),
  nonExistIds?: [ string ](name='NonExistIds'),
  requestId?: string(name='RequestId', example='25818875-5F78-4A13-BEF6-D7393642CA58'),
}

model ListFpShotImportJobResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: ListFpShotImportJobResponseBody(name='body'),
}

async function listFpShotImportJobWithOptions(request: ListFpShotImportJobRequest, runtime: Util.RuntimeOptions): ListFpShotImportJobResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.jobIds)) {
    query['JobIds'] = request.jobIds;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'ListFpShotImportJob',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

async function listFpShotImportJob(request: ListFpShotImportJobRequest): ListFpShotImportJobResponse {
  var runtime = new Util.RuntimeOptions{};
  return listFpShotImportJobWithOptions(request, runtime);
}

model ListJobRequest {
  endOfJobCreatedTimeRange?: string(name='EndOfJobCreatedTimeRange', description='The end of the time range to query. Specify the time in the ISO 8601 standard in the `YYYY-MM-DDThh:mm:ssZ` format. The time is displayed in UTC.', example='2014-01-11T12:00:00Z'),
  maximumPageSize?: long(name='MaximumPageSize', description='The maximum number of media workflow execution instances to return on each page.

*   Default value: **10**.
*   Valid values: **1 to 100**.', example='10'),
  nextPageToken?: string(name='NextPageToken', description='The token that is used to retrieve the next page of the query results. You do not need to specify this parameter in the first request. The response to the first request contains this parameter, which you add to the next request.', example='16f01ad6175e4230ac42bb5182cd****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  pipelineId?: string(name='PipelineId', description='The ID of the MPS queue. To view the ID of the MPS queue, log on to the [MPS console](https://mps.console.aliyun.com/overview) and choose **Global Settings** > **Pipelines** in the left-side navigation pane.', example='88c6ca184c0e424d5w5b665e2a12****'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  startOfJobCreatedTimeRange?: string(name='StartOfJobCreatedTimeRange', description='The beginning of the time range to query. Specify the time in the ISO 8601 standard in the `YYYY-MM-DDThh:mm:ssZ` format. The time is displayed in UTC.', example='2014-01-10T12:00:00Z'),
  state?: string(name='State', description='The status of the transcoding job. Default value: **All**. Valid values:

*   **All**: All statuses.
*   **Submitted**: The job is submitted.
*   **Transcoding**: The job is being transcoded.
*   **TranscodeSuccess**: Transcoding is successful.
*   **TranscodeFail**: Transcoding failed.
*   **TranscodeCancelled**: Transcoding is canceled.', example='All'),
}

model ListJobResponseBody = {
  jobList?: {
    job?: [ 
    {
      code?: string(name='Code', description='The error code that is returned when the job fails. This parameter is not returned if the job is successful.', example='InternalError'),
      creationTime?: string(name='CreationTime', description='The time when the job was created.', example='2014-01-10T12:00:00Z'),
      finishTime?: string(name='FinishTime', description='The time when the transcoding job was complete.', example='2014-01-10T12:20:25Z'),
      input?: {
        bucket?: string(name='Bucket', description='The OSS bucket in which the job input is stored.', example='example-bucket'),
        location?: string(name='Location', description='The OSS region where the job input resides.', example='oss-cn-hangzhou'),
        object?: string(name='Object', description='The name of the OSS object of the job input.', example='example.flv'),
      }(name='Input', description='The information about the job input.'),
      jobId?: string(name='JobId', description='The ID of the job.', example='31fa3c9ca8134fb4b0b0f7878301****'),
      MNSMessageResult?: {
        errorCode?: string(name='ErrorCode', description='The error code that is returned when the job fails. This parameter is not returned if the job is created.', example='InvalidParameter.ResourceNotFound'),
        errorMessage?: string(name='ErrorMessage', description='The error message that is returned when the job fails. This parameter is not returned if the job is created.', example='The resource operated “%s” cannot be found.'),
        messageId?: string(name='MessageId', description='The ID of the success message.', example='123'),
      }(name='MNSMessageResult', description='The message sent by Message Service (MNS) to notify the user of the job result.'),
      message?: string(name='Message', description='The message that is returned when the job fails. This parameter is not returned if the job is successful.', example='The operation has failed due to some unknown error, exception or failure.'),
      output?: {
        audio?: {
          bitrate?: string(name='Bitrate', description='The audio bitrate of the output file.

*   Unit: Kbit/s.
*   Default value: **128**.', example='128'),
          channels?: string(name='Channels', description='The number of sound channels.

*   If the Codec parameter is set to mp3, this parameter can be set only to 1 or 2.
*   If the Codec parameter is set to aac, this parameter can be set only to 1, 2, 4, 5, 6, or 8.
*   Default value: 2.', example='2'),
          codec?: string(name='Codec', description='The audio codec.

*   Valid values: aac, mp3, vorbis, and flac.
*   Default value: **aac**.', example='aac'),
          profile?: string(name='Profile', description='The codec profile of the audio. Valid values when the Codec parameter is set to aac: aaclow, aache, aachev2, aacld, and aaceld.', example='aaclow'),
          qscale?: string(name='Qscale', description='The level of quality control on the audio.', example='15'),
          samplerate?: string(name='Samplerate', description='The sampling rate.

*   Valid values: 22050, 32000, 44100, 48000, and 96000.
*   Unit: Hz.
*   Default value: 44100.
*   If the video container format is FLV and the audio codec is MP3, you cannot set this parameter to 32000, 48000, or 96000. If the audio codec is MP3, you cannot set this parameter to 96000.', example='44100'),
          volume?: {
            level?: string(name='Level', description='The volume adjustment range.

*   Unit: dB.
*   Default: **-20db**.', example='-20'),
            method?: string(name='Method', description='The volume adjustment method. Valid values:

*   **auto**: The volume is automatically adjusted.
*   **dynamic**: The volume is dynamically adjusted.
*   **linear**: The volume is linearly adjusted.', example='auto'),
          }(name='Volume', description='The volume configuration.'),
        }(name='Audio', description='The audio configuration.'),
        audioStreamMap?: string(name='AudioStreamMap', description='The sequence number of the audio stream.

*   Format: `0:a:{sequence number}`.
*   The sequence number is the index of the audio stream in the list and starts from 0. If you do not set the corresponding parameter in the request, the default audio stream is selected.', example='0:a:0'),
        clip?: {
          timeSpan?: {
            duration?: string(name='Duration', description='The duration of the clip.

*   Format: `hh:mm:ss[.SSS]` or `sssss[.SSS]`.
*   Valid values: `[00:00:00.000,23:59:59.999]` or `[0.000,86399.999]`.
*   Examples: 01:00:59.999 and 32000.23.', example='01:00:59.999'),
            seek?: string(name='Seek', description='The time when the clip starts.

*   Format: `hh:mm:ss[.SSS]` or `sssss[.SSS]`.
*   Valid values: `[00:00:00.000,23:59:59.999]` or `[0.000,86399.999]`.
*   Examples: 01:59:59.999 and 32000.23.', example='01:00:59.999'),
          }(name='TimeSpan', description='The time span of the clip.'),
        }(name='Clip', description='The information about the clip.'),
        container?: {
          format?: string(name='Format', description='The format of the container.

*   Default value: mp4.
*   Video formats include FLV, MP4, HLS (M3U8 + TS), and MPEG-DASH (MPD + fMP4)
*   Audio formats include MP3, MP4, Ogg, FLAC, and M4A.
*   Image formats include GIF and WebP.
*   If the container format is GIF, the video codec must be set to GIF.
*   If the container format is WebP, the video codec must be set to WebP.
*   If the container format is FLV, the video codec cannot be set to H.265.', example='flv'),
        }(name='Container', description='The information about the container.'),
        deWatermark?: string(name='DeWatermark', description='The configuration of watermark blurring. The value must be a JSON object. For more information, see [Parameter details](~~29253~~).', example='{"0": [{"l": 10,"t": 10,"w": 10,"h": 10},{"l": 100,"t": 0.1,"w": 10,"h": 10}],"128000": [],"250000": [{"l": 0.2,"t": 0.1,"w": 0.01,"h": 0.05}]}'),
        encryption?: {
          id?: string(name='Id', description='The encryption ID.', example='31fa3c9ca8134f9cec2b4b0b0f78****'),
          key?: string(name='Key', description='The key that is used to encrypt the video.', example='encryptionkey128'),
          keyType?: string(name='KeyType', description='The key encryption method. Valid values: Base64 and KMS.

>  For example, if the key is `"encryptionkey128”`, you can encrypt the key in the Base64 format or use Key Management Service (KMS) to encrypt the key.````', example='Base64'),
          keyUri?: string(name='KeyUri', description='The URL that is used to request the key. The URL is Base64-encoded.', example='https://1161758785*****.cn-shanghai.fc.aliyuncs.com/2016-08-15/proxy/HLS-decyptServer/decyptServer/'),
          skipCnt?: string(name='SkipCnt', description='The number of unencrypted frames at the beginning of the video. Leaving these frames unencrypted enables video playback to start quickly.', example='3'),
          type?: string(name='Type', description='The encryption type. Valid value: hls-aes-128.', example='hls-aes-128'),
        }(name='Encryption', description='The encryption configuration. Only outputs in the M3U8 format are supported.'),
        m3U8NonStandardSupport?: {
          TS?: {
            md5Support?: boolean(name='Md5Support', description='Indicates whether to support the output of the md5 value of the TS file in the M3U8 video. Valid values:

*   **true**: supported.
*   **false**: not supported.', example='true'),
            sizeSupport?: boolean(name='SizeSupport', description='Indicates whether to support the output of the size of the TS file in the M3U8 video.

*   **true**: supported.
*   **false**: not supported.', example='true'),
          }(name='TS', description='The non-standard support configuration for TS files. The value must be a JSON object. For more information, see [Parameter details](~~29253~~).'),
        }(name='M3U8NonStandardSupport', description='The non-standard support configuration for M3U8. The value must be a JSON object. For more information, see [Parameter details](~~29253~~).'),
        mergeConfigUrl?: string(name='MergeConfigUrl', description='The URL of the merging configuration file. You can specify either the MergeList or MergeConfigUrl parameter.  

- The configuration file that you specify by using the MergeConfigUrl parameter can contain up to 50 merged clips.
- The MergeConfigUrl parameter specifies the URL of the configuration file for clip merging.
- Make sure that the configuration file is stored as an object in OSS and that MPS can access the OSS object. For information about the file content, see the details about merging parameters.
- Example of the content of mergeConfigfile: `{"MergeList":[{"MergeURL":"http://exampleBucket****.oss-cn-hangzhou.aliyuncs.com/video_01.mp4"}]}`.', example='{"MergeList":[{"MergeURL":"http://exampleBucket****.oss-cn-hangzhou.aliyuncs.com/video_01.mp4"}]}'),
        mergeList?: {
          merge?: [ 
          {
            duration?: string(name='Duration', description='The start point in time of the clip.

*   Format: `hh:mm:ss[.SSS]` or `sssss[.SSS]`.
*   Example values: 01:59:59.999 and 32000.23.', example='01:59:59.999'),
            mergeURL?: string(name='MergeURL', description='The OSS URL of the clip.

*   Example: `http://example-bucket-****.oss-cn-hangzhou.aliyuncs.com/example-object.flv`.
*   The object must be URL-encoded by using the UTF-8 standard.', example='http://example-bucket.oss-cn-hangzhou.aliyuncs.com/example-object.flv'),
            roleArn?: string(name='RoleArn', description='The Alibaba Cloud Resource Name (ARN) of the role used for proxy authorization.', example='acs:ram::<your uid>:role/<your role name>'),
            start?: string(name='Start', description='The start point in time of the clip.

*   Format: `hh:mm:ss[.SSS]` or `sssss[.SSS]`.
*   Example values: 01:59:59.999 and 32000.23.'),
          }
        ](name='Merge')
        }(name='MergeList', description='The clip merging configuration.'),
        muxConfig?: {
          gif?: {
            ditherMode?: string(name='DitherMode', description='The color dithering algorithm of the palette. Valid values: **sierra** and **bayer**.', example='bayer'),
            finalDelay?: string(name='FinalDelay', description='The duration for which the final frame is paused. Unit: centiseconds.', example='0'),
            isCustomPalette?: string(name='IsCustomPalette', description='Indicates whether the custom palette was used. Valid values:

- **true**: custom.
- **false**: non-custom.', example='true'),
            loop?: string(name='Loop', description='The loop count.', example='0'),
          }(name='Gif', description='The transmuxing configuration for GIF.'),
          segment?: {
            duration?: string(name='Duration', description='The length of the segment. The value must be an integer. Unit: seconds. 

- Valid values: [1,10].
- Default value: 10.', example='10'),
          }(name='Segment', description='The segment configuration. The value is a JSON object.'),
          webp?: {
            loop?: string(name='Loop', description='The loop count.', example='0'),
          }(name='Webp', description='The transmuxing configuration for WebP.'),
        }(name='MuxConfig', description='The transmuxing configuration.'),
        openingList?: {
          opening?: [ 
          {
            height?: string(name='Height', description='The height of the opening part. Valid values: values in the range of (0, 4096), -1, and full.

*   Default value: **-1**.
*   A value of -1 indicates that the height of the source of the opening part is retained.
*   A value of full indicates that the height of the main part is used for the opening part.', example='-1'),
            start?: string(name='Start', description='The amount of time after which the opening part is played. The value starts from 0.

*   Unit: seconds.
*   Default value: **0**.', example='0'),
            width?: string(name='Width', description='The width of the opening part. Valid values: values in the range of (0, 4096), -1, and full.

*   Default value: **-1**.
*   A value of -1 indicates that the width of the source of the opening part is retained.
*   A value of full indicates that the width of the main part is used for the opening part.', example='-1'),
            openUrl?: string(name='openUrl', description='The OSS URL of the opening part of the video.', example='http://example-bucket-****.oss-cn-hangzhou.aliyuncs.com/opening_01.flv'),
          }
        ](name='Opening')
        }(name='OpeningList', description='The list of opening parts.'),
        outSubtitleList?: {
          outSubtitle?: [ 
          {
            map?: string(name='Map', description='The video track. Format: 0:{stream}:{stream sequence number}, that is, 0:v:{video_index}. The value of stream is v, which indicates an audio stream of a video. The sequence number is the index of the audio stream in the list and starts from 0.', example='0:v:0'),
            message?: string(name='Message', description='The error message that is returned when the job fails to be created. This parameter is not returned if the job is created.', example='The specified parameter “%s” cannot be null.'),
            outSubtitleFile?: {
              bucket?: string(name='Bucket', description='The name of the OSS bucket in which the input file is stored.', example='example-bucket-****'),
              location?: string(name='Location', description='The OSS region where the output file resides.', example='oss-cn-hangzhou'),
              object?: string(name='Object', description='The name of the OSS object of the output file.', example='example-output.flv'),
              roleArn?: string(name='RoleArn', description='The ARN of the role used for proxy authorization.', example='acs:ram::<your uid>:role/<your role name>'),
            }(name='OutSubtitleFile', description='The details of the output file.'),
            success?: boolean(name='Success', description='Indicates whether the job was successful. Valid values:

*   **true**: The job was successful.
*   **fasle**: The job failed.', example='true'),
          }
        ](name='OutSubtitle')
        }(name='OutSubtitleList', description='The output subtitle list.'),
        outputFile?: {
          bucket?: string(name='Bucket', description='The name of the OSS bucket in which the input file is stored.', example='example-bucket'),
          location?: string(name='Location', description='The OSS region where the output file resides.', example='oss-cn-hangzhou'),
          object?: string(name='Object', description='The name of the OSS object of the output file.', example='example-output.flv'),
          roleArn?: string(name='RoleArn', description='The ARN of the role used for proxy authorization.', example='acs:ram::<your uid>:role/<your role name>'),
        }(name='OutputFile', description='The information about the output file.'),
        priority?: string(name='Priority', description='The priority of the job in the MPS queue to which the job is added. 

- A value of 10 indicates the highest priority.
- Default value: **6**.', example='6'),
        properties?: {
          bitrate?: string(name='Bitrate', description='The bitrate.', example='490'),
          duration?: string(name='Duration', description='The duration.', example='17'),
          fileFormat?: string(name='FileFormat', description='The format of the video file.', example='mp4'),
          fileSize?: string(name='FileSize', description='The size of the file.', example='1057273'),
          format?: {
            bitrate?: string(name='Bitrate', description='The total bitrate.', example='490.784'),
            duration?: string(name='Duration', description='The total duration.', example='17.234000'),
            formatLongName?: string(name='FormatLongName', description='The full name of the container format.', example='QuickTime / MOV'),
            formatName?: string(name='FormatName', description='The short name of the container format. Valid values: mov, mp4, m4a, 3gp, 3g2, and mj2.', example='mov'),
            numPrograms?: string(name='NumPrograms', description='The total number of program streams.', example='0'),
            numStreams?: string(name='NumStreams', description='The total number of media streams.', example='2'),
            size?: string(name='Size', description='The size of the file.', example='1057273'),
            startTime?: string(name='StartTime', description='The start time.', example='0.064000'),
          }(name='Format', description='The format information.'),
          fps?: string(name='Fps', description='The frame rate.', example='30'),
          height?: string(name='Height', description='The length of the video.', example='1280'),
          streams?: {
            audioStreamList?: {
              audioStream?: [ 
              {
                bitrate?: string(name='Bitrate', description='The bitrate.', example='64.136'),
                channelLayout?: string(name='ChannelLayout', description='The output layout of the sound channels.', example='mono'),
                channels?: string(name='Channels', description='The number of sound channels.', example='1'),
                codecLongName?: string(name='CodecLongName', description='The full name of the encoding format.', example='AAC (Advanced Audio Coding)'),
                codecName?: string(name='CodecName', description='The short name of the encoding format.', example='aac'),
                codecTag?: string(name='CodecTag', description='The tag of the encoding format.', example='0x6134706d'),
                codecTagString?: string(name='CodecTagString', description='The tag string of the encoding format.', example='mp4a'),
                codecTimeBase?: string(name='CodecTimeBase', description='The codec time base.', example='1/32000'),
                duration?: string(name='Duration', description='The duration.', example='17.223562'),
                index?: string(name='Index', description='The sequence number of the audio stream. The value indicates the position of the audio stream in all audio streams.', example='1'),
                lang?: string(name='Lang', description='The language. For more information, see [FFmeg language definition](https://www.ffmpeg.org/ffmpeg-all.html#Metadata) and [ISO-639](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes).', example='und'),
                numFrames?: string(name='NumFrames', description='The total number of frames.', example='30'),
                sampleFmt?: string(name='SampleFmt', description='The sampling format.', example='fltp'),
                samplerate?: string(name='Samplerate', description='The sampling rate.', example='32000'),
                startTime?: string(name='StartTime', description='The start time.', example='0.000000'),
                timebase?: string(name='Timebase', description='The time base.', example='1/32000'),
              }
            ](name='AudioStream')
            }(name='AudioStreamList', description='The list of audio streams.'),
            subtitleStreamList?: {
              subtitleStream?: [ 
              {
                index?: string(name='Index', description='The sequence number of the subtitle stream. 

The value specifies the position of the subtitle stream in all subtitle streams.', example='1'),
                lang?: string(name='Lang', description='The language.', example='und'),
              }
            ](name='SubtitleStream')
            }(name='SubtitleStreamList', description='The list of subtitle streams.'),
            videoStreamList?: {
              videoStream?: [ 
              {
                avgFPS?: string(name='AvgFPS', description='The average frame rate.', example='30.0'),
                bitrate?: string(name='Bitrate', description='The bitrate.', example='421.117'),
                codecLongName?: string(name='CodecLongName', description='The full name of the encoding format.', example='H.264 / AVC / MPEG-4 AVC / MPEG-4 part 10'),
                codecName?: string(name='CodecName', description='The short name of the encoding format.', example='h264'),
                codecTag?: string(name='CodecTag', description='The tag of the encoding format.', example='0x31637661'),
                codecTagString?: string(name='CodecTagString', description='The tag string of the encoding format.', example='avc1'),
                codecTimeBase?: string(name='CodecTimeBase', description='The codec time base.', example='1/60'),
                dar?: string(name='Dar', description='The display aspect ratio (DAR).', example='9:16'),
                duration?: string(name='Duration', description='The duration.', example='17.233333'),
                fps?: string(name='Fps', description='The frame rate.', example='30.0'),
                hasBFrames?: string(name='HasBFrames', description='Indicates whether the video stream contains bidirectional frames (B-frames).', example='2'),
                height?: string(name='Height', description='The height of the video in pixels.', example='1280'),
                index?: string(name='Index', description='The sequence number of the video stream. The value indicates the position of the video stream in all video streams.', example='0'),
                lang?: string(name='Lang', description='The language. For more information, see [FFmeg language definition](https://www.ffmpeg.org/ffmpeg-all.html#Metadata) and [ISO-639](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes).', example='und'),
                level?: string(name='Level', description='The codec level.', example='31'),
                networkCost?: {
                  avgBitrate?: string(name='AvgBitrate', description='The average bitrate.', example='300'),
                  costBandwidth?: string(name='CostBandwidth', description='The maximum bandwidth that was consumed.', example='10'),
                  preloadTime?: string(name='PreloadTime', description='The amount of preload time.', example='8'),
                }(name='NetworkCost', description='The network bandwidth consumption.'),
                numFrames?: string(name='NumFrames', description='The total frame rate.', example='30'),
                pixFmt?: string(name='PixFmt', description='The pixel format.', example='yuv420p'),
                profile?: string(name='Profile', description='The codec profile.', example='high'),
                sar?: string(name='Sar', description='The sample aspect ratio (SAR) of the video stream.', example='1:1'),
                startTime?: string(name='StartTime', description='The start time.', example='0.000000'),
                timebase?: string(name='Timebase', description='The time base.', example='1/15360'),
                width?: string(name='Width', description='The width of the video in pixels.', example='720'),
              }
            ](name='VideoStream')
            }(name='VideoStreamList', description='The list of video streams.'),
          }(name='Streams', description='The stream information.'),
          width?: string(name='Width', description='The width of the video.', example='720'),
        }(name='Properties', description='The media properties.'),
        rotate?: string(name='Rotate', description='The video rotation angle.', example='90'),
        subtitleConfig?: {
          extSubtitleList?: {
            extSubtitle?: [ 
            {
              charEnc?: string(name='CharEnc', description='The character set used by the external subtitle. 

- Valid values: UTF-8, GBK, BIG5, and auto.
- Default value: **auto**.

>  If you set this parameter to auto, the detected character set may not be the actual character set. We recommend that you set this parameter to another value.', example='auto'),
              fontName?: string(name='FontName', description='The font of the hard subtitles converted from external subtitles.', example='"WenQuanYi Zen Hei", "Yuanti SC Regular", "SimSun"'),
              input?: {
                bucket?: string(name='Bucket', description='The OSS bucket in which the input file is stored.', example='example-bucket-****'),
                location?: string(name='Location', description='The OSS region where the input file resides.', example='oss-cn-hangzhou'),
                object?: string(name='Object', description='The name of the OSS object that is used as the input file.', example='example-output.flv'),
              }(name='Input', description='The input subtitle file. 

- Files in the SRT or ASS format are supported. For more information, see [Parameter details](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/parameter-details).
- Example: `{“Bucket”:”example-bucket”,“Location”:”oss-cn-hangzhou”,“Object”:”example.srt”}`.'),
            }
          ](name='ExtSubtitle')
          }(name='ExtSubtitleList', description='The list of external subtitles. The value is a JSON array.'),
          subtitleList?: {
            subtitle?: [ 
            {
              map?: string(name='Map', description='The sequence number of the video stream. The sequence number is the index of the video stream in the list and starts from 0. If you do not set the corresponding parameter in the request, the default video stream is selected.', example='0'),
            }
          ](name='Subtitle')
          }(name='SubtitleList', description='The list of subtitles.'),
        }(name='SubtitleConfig', description='The subtitle configuration.'),
        superReso?: {
          isHalfSample?: string(name='IsHalfSample', description='Indicates whether to obtain parameters related to the sampling rate. Valid values:

- **true**: The parameters are obtained.
- **false**: The parameters are not obtained.', example='true'),
        }(name='SuperReso', description='Indicates that the resolution of the source video is used.'),
        tailSlateList?: {
          tailSlate?: [ 
          {
            bgColor?: string(name='BgColor', description='The color of the bars that are added to the ending part if the size of the ending part is smaller than that of the main part. Default value: **White**. For more information, see [Background colors](https://docs-aliyun.cn-hangzhou.oss.aliyun-inc.com/assets/attach/29253/cn_zh/1502784952344/color.txt?spm=a2c4g.11186623.2.63.1df840f74IH4Eq&file=color.txt).', example='White'),
            blendDuration?: string(name='BlendDuration', description='The amount of time between the end of the main part and the beginning of the ending part. During the transition, the last frame of the main part fades out, and the first frame of the ending part fades in. Unit: seconds. Default value: 0.', example='0'),
            height?: string(name='Height', description='The height of the ending part. Valid values: values in the range of (0, 4096), -1, and full. 

- A value of -1 indicates that the height of the source of the ending part is retained.
- A value of full indicates that the height of the main part is used for the ending part.
- Default value: **-1**.', example='-1'),
            isMergeAudio?: boolean(name='IsMergeAudio', description='Indicates whether to merge the audio content of the ending part. Valid values:

- **true**: The audio content of the ending part is merged.
- **false**: The audio content of the ending part is not merged.', example='true'),
            start?: string(name='Start', description='The start time.', example='1'),
            tailUrl?: string(name='TailUrl', description='The OSS URL of the ending part.', example='http://example-bucket-****.oss-cn-hangzhou.aliyuncs.com/opening_01.flv'),
            width?: string(name='Width', description='The width of the ending part. Valid values: values in the range of (0, 4096), -1, and full.  

- A value of -1 indicates that the width of the source of the ending part is retained. A value of full indicates that the width of the main part is used for the ending part.
- Default value: **-1**.', example='-1'),
          }
        ](name='TailSlate')
        }(name='TailSlateList', description='The list of ending parts.'),
        templateId?: string(name='TemplateId', description='The ID of the template.', example='S00000000-000010'),
        transConfig?: {
          adjDarMethod?: string(name='AdjDarMethod', description='The method of resolution adjustment. Default value: **none**. Valid values: rescale, crop, pad, and none.', example='none'),
          isCheckAudioBitrate?: string(name='IsCheckAudioBitrate', description='Indicates whether the audio bitrate is checked. If the output audio bitrate is higher than the input audio bitrate, the system considers that the output bitrate equals the input bitrate.

*   **true**: The audio bitrate is checked.
*   **false**: The audio bitrate is not checked.
*   Default value: **false**.', example='false'),
          isCheckAudioBitrateFail?: string(name='IsCheckAudioBitrateFail', description='Indicates whether the audio bitrate is checked. If the output audio bitrate is higher than the audio bitrate of the media source, a transcoding failure is returned without transcoding the media. This parameter takes precedence over the IsCheckAudioBitrate parameter.

*   **true**: The audio bitrate is checked.
*   **false**: The audio bitrate is not checked.', example='false'),
          isCheckReso?: string(name='IsCheckReso', description='Indicates whether the resolution is checked. If the output resolution is higher than the input resolution based on the width or height, the system considers that the output resolution equals the input resolution.

*   **true**: The resolution is checked.
*   **false**: The resolution is not checked.
*   Default value: **false**.', example='false'),
          isCheckResoFail?: string(name='IsCheckResoFail', description='Indicates whether the resolution is checked. If the output resolution is higher than the input resolution based on the width or height, a transcoding failure is returned.

*   **true**: The resolution is checked.
*   **false**: The resolution is not checked.
*   Default value: **false**.', example='false'),
          isCheckVideoBitrate?: string(name='IsCheckVideoBitrate', description='Indicates whether the video bitrate is checked. If the output video bitrate is higher than the input video bitrate, the system considers that the output bitrate equals the input bitrate.

*   **true**: The video bitrate is checked.
*   **false**: The video bitrate is not checked.
*   Default value: **false**.', example='false'),
          isCheckVideoBitrateFail?: string(name='IsCheckVideoBitrateFail', description='Indicates whether the video bitrate is checked. If the bitrate of the output video is higher than that of the input video, a transcoding failure is returned without transcoding the video. This parameter takes precedence over the IsCheckVideoBitrate parameter.

*   **true**: The video bitrate is checked.
*   **false**: The video bitrate is not checked.
*   Default value: false.', example='false'),
          transMode?: string(name='TransMode', description='The transcoding mode.

*   Valid values: onepass, twopass, and CBR.
*   Default value: onepass.', example='onepass'),
        }(name='TransConfig', description='The general transcoding configuration. If this parameter was specified in the request, the value overwrites the corresponding parameter in the specified transcoding template.'),
        userData?: string(name='UserData', description='The custom data.', example='test-001'),
        video?: {
          bitrate?: string(name='Bitrate', description='The average bitrate of the video. Unit: Kbit/s.', example='428'),
          bitrateBnd?: {
            max?: string(name='Max', description='The maximum bitrate. Unit: Kbit/s.', example='1000'),
            min?: string(name='Min', description='The minimum bitrate. Unit: Kbit/s.', example='200'),
          }(name='BitrateBnd', description='The bitrate range of the video.'),
          bufsize?: string(name='Bufsize', description='The size of the buffer.', example='6000'),
          codec?: string(name='Codec', description='The codec. Valid values: **H.264**, **H.265**, **GIF**, and **WEBP**.', example='H.264'),
          crf?: string(name='Crf', description='The constant rate factor. If this parameter is specified, the setting of the Bitrate parameter becomes invalid. Default value: **26**.', example='26'),
          crop?: string(name='Crop', description='The video cropping mode. The following modes are supported:

*   **border**: automatically detects and removes borders.
*   Custom cropping. Specify a value in the format of width:height:left:top to crop the video based on the custom settings. Example: 1280:800:0:140.', example='border'),
          degrain?: string(name='Degrain', description='The level of the independent denoising algorithm.', example='5'),
          fps?: string(name='Fps', description='The frame rate.

*   The value is 60 if the frame rate of the input file exceeds 60.
*   Default value: the frame rate of the input file.', example='25'),
          gop?: string(name='Gop', description='The maximum number of frames between two keyframes. Default value: 250.', example='250'),
          height?: string(name='Height', description='The height.

*   Unit: pixels.
*   Default value: the original height of the video.', example='720'),
          maxFps?: string(name='MaxFps', description='The maximum frame rate.', example='60'),
          maxrate?: string(name='Maxrate', description='The maximum bitrate of the video. Unit: Kbit/s.', example='1000'),
          pad?: string(name='Pad', description='The black borders to be added to the video. Unit: pixels.

*   Format: width:height:left:top.
*   Example: 1280:800:0:140.', example='1280:800:0:140'),
          pixFmt?: string(name='PixFmt', description='The video color format. Valid values: standard pixel formats such as yuv420p and yuvj420p.', example='yuv420p'),
          preset?: string(name='Preset', description='The preset video algorithm. Default value: medium. Valid values:

*   **veryfast**
*   **fast**
*   **medium**
*   **slow**
*   **slower**', example='medium'),
          profile?: string(name='Profile', description='The codec profile. Valid values:

*   **baseline**: applicable to mobile devices.
*   **main**: applicable to standard-definition devices.
*   **high**: applicable to high-definition devices.
*   Default value: **high**.', example='high'),
          qscale?: string(name='Qscale', description='The level of quality control on the video.', example='15'),
          resoPriority?: string(name='ResoPriority', description='The priority of the resource.', example='1'),
          scanMode?: string(name='ScanMode', description='The scan mode. Valid values:

*   **interlaced**: An interlaced scan was performed.
*   **progressive**: A progressive scan was performed.
*   **auto**: A scan was performed based on the video source.', example='interlaced'),
          width?: string(name='Width', description='The width.

*   Unit: pixels.
*   Default value: the original width of the video.', example='1280'),
        }(name='Video', description='The video configuration.'),
        videoStreamMap?: string(name='VideoStreamMap', description='The sequence number of the video stream. 

- Format: 0:a:{Sequence number}. Example value: 0:a:0.
- The sequence number is the index of the video stream in the list and starts from 0.
- If you do not set the corresponding parameter in the request, the default video stream is selected.', example='0:a:0'),
        waterMarkConfigUrl?: string(name='WaterMarkConfigUrl', description='The URL of the watermark configuration file.', example='http://example.com/configure'),
        waterMarkList?: {
          waterMark?: [ 
          {
            dx?: string(name='Dx', description='The horizontal offset of the watermark relative to the output video. If this parameter was specified in the request, the value overwrites the corresponding parameter in the watermark template. Default value: 0. The value can be an integer or a decimal.

*   An integer indicates the pixel value of the horizontal offset.

    *   Valid values: \\[8,4096].
    *   Unit: pixels.

*   A decimal indicates the ratio of the horizontal offset to the width in the output video resolution.

    *   Valid values: (0,1).
    *   The decimal number can be accurate to four decimal places, such as 0.9999. Excessive digits are automatically deleted.', example='100'),
            dy?: string(name='Dy', description='The vertical offset of the watermark image relative to the output video. If this parameter was specified in the request, the value overwrites the corresponding parameter in the watermark template. The value can be an integer or a decimal.

*   An integer indicates the pixel value of the vertical offset.

    *   Valid values: \\[8,4096].
    *   Unit: pixels.

*   A decimal indicates the ratio of the vertical offset to the height in the output video resolution.

    *   Valid values: (0,1).
    *   The decimal number can be accurate to four decimal places, such as 0.9999. Excessive digits are automatically deleted.', example='100'),
            height?: string(name='Height', description='The height of the watermark. If this parameter was specified in the request, the value overwrites the corresponding parameter in the watermark template. The value can be an integer or a decimal.

*   An integer indicates the pixel value of the watermark height.

    *   Valid values: \\[8,4096].
    *   Unit: pixels.

*   A decimal indicates the ratio of the watermark height to the height in the output video resolution.

    *   Valid values: (0,1).
    *   The decimal number can be accurate to four decimal places, such as 0.9999. Excessive digits are automatically deleted.', example='50'),
            inputFile?: {
              bucket?: string(name='Bucket', description='The name of the OSS bucket in which the input file is stored.', example='example-bucket'),
              location?: string(name='Location', description='The OSS region where the input file resides.', example='oss-cn-hangzhou'),
              object?: string(name='Object', description='The name of the OSS object that is used as the input file.', example='example-logo-****.png'),
            }(name='InputFile', description='The watermark input file.'),
            referPos?: string(name='ReferPos', description='The position of the watermark.

*   **TopRight**: the upper-right corner.
*   **TopLeft**: the upper-left corner.
*   **BottomRight**: the lower-right corner.
*   **BottomLeft**: the lower-left corner.', example='TopRight'),
            type?: string(name='Type', description='The type of the watermark. If this parameter was specified in the request, the value overwrites the corresponding parameter in the watermark template. For more information, see [Parameter details](~~29253~~). Valid values:

*   **Image**: an image watermark.
*   **Text**: a text watermark.', example='Image'),
            waterMarkTemplateId?: string(name='WaterMarkTemplateId', description='The ID of the watermark template.', example='88c6ca184c0e47098a5b665e2a12****'),
            width?: string(name='Width', description='The width of the watermark. If this parameter was specified in the request, the value overwrites the corresponding parameter in the watermark template. The value can be an integer or a decimal.

*   An integer indicates the pixel value of the watermark width.

    *   Valid values: \\[8,4096].
    *   Unit: pixels.

*   A decimal indicates the ratio of the watermark width to the width in the output video resolution.

    *   Valid values: (0,1).
    *   The decimal number can be accurate to four decimal places, such as 0.9999. Excessive digits are automatically deleted.', example='50'),
          }
        ](name='WaterMark')
        }(name='WaterMarkList', description='The list of watermarks.'),
      }(name='Output', description='The output of the job.'),
      percent?: long(name='Percent', description='The transcoding progress.', example='100'),
      pipelineId?: string(name='PipelineId', description='The ID of the MPS queue.', example='88c6ca184c0e47b665e2a1267971****'),
      state?: string(name='State', description='The status of the job. Valid values:

*   **Submitted**: The job is submitted.
*   **Transcoding**: The job is being transcoded.
*   **TranscodeSuccess**: Transcoding is successful.
*   **TranscodeFail**: Transcoding failed.
*   **TranscodeCancelled**: Transcoding is canceled.', example='TranscodeSuccess'),
    }
  ](name='Job')
  }(name='JobList', description='The list of transcoding jobs.'),
  nextPageToken?: string(name='NextPageToken', description='The token that is used to retrieve the next page of the query results. You must record the token and specify it in the next request.', example='16f01ad6175e4230ac42bb5182cd****'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='BC860F04-778A-472F-AB39-E1BF329C1EA8'),
}

model ListJobResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: ListJobResponseBody(name='body'),
}

/**
  * *   By default, the returned transcoding jobs are sorted by CreationTime in descending order.
  * *   You can call this operation to return transcoding jobs of the last 90 days. The jobs are returned based on the actual configuration time.
  * *   You can filter query results by configuring request parameters such as job status, creation time interval, and MPS queue for transcoding.
  * ## QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request ListJobRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return ListJobResponse
 */
async function listJobWithOptions(request: ListJobRequest, runtime: Util.RuntimeOptions): ListJobResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.endOfJobCreatedTimeRange)) {
    query['EndOfJobCreatedTimeRange'] = request.endOfJobCreatedTimeRange;
  }
  if (!Util.isUnset(request.maximumPageSize)) {
    query['MaximumPageSize'] = request.maximumPageSize;
  }
  if (!Util.isUnset(request.nextPageToken)) {
    query['NextPageToken'] = request.nextPageToken;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.pipelineId)) {
    query['PipelineId'] = request.pipelineId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.startOfJobCreatedTimeRange)) {
    query['StartOfJobCreatedTimeRange'] = request.startOfJobCreatedTimeRange;
  }
  if (!Util.isUnset(request.state)) {
    query['State'] = request.state;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'ListJob',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * *   By default, the returned transcoding jobs are sorted by CreationTime in descending order.
  * *   You can call this operation to return transcoding jobs of the last 90 days. The jobs are returned based on the actual configuration time.
  * *   You can filter query results by configuring request parameters such as job status, creation time interval, and MPS queue for transcoding.
  * ## QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request ListJobRequest
  * @return ListJobResponse
 */
async function listJob(request: ListJobRequest): ListJobResponse {
  var runtime = new Util.RuntimeOptions{};
  return listJobWithOptions(request, runtime);
}

model ListMediaWorkflowExecutionsRequest {
  inputFileURL?: string(name='InputFileURL', description='The Object Storage Service (OSS) URL of the input file of the media workflow. The URL complies with RFC 3986 and is encoded in UTF-8, with reserved characters being percent-encoded.', example='http://example-****.cn-hangzhou.aliyuncs.com/test****.flv'),
  maximumPageSize?: long(name='MaximumPageSize', description='The maximum number of media workflow execution instances to return. Valid values: `[1,100]`. Default value: **10**.', example='1'),
  mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the media workflow whose execution instances you want to query. To obtain the workflow ID, you can log on to the **ApsaraVideo Media Processing (MPS) console** and choose **Workflows** > **Workflow Settings**.', example='43b7335a4b1d4fe883670036affb****'),
  mediaWorkflowName?: string(name='MediaWorkflowName', description='The name of the media workflow. To obtain the workflow name, you can log on to the **MPS console** and choose **Workflows** > **Workflow Settings**.', example='example-mediaworkflow-****'),
  nextPageToken?: string(name='NextPageToken', description='The token that is used to retrieve the next page of the query results. The value is a 32-bit UUID. When you request the first page of query results, leave the NextPageToken parameter empty. When you request more query results, specify the value of the NextPageToken parameter returned in the query results on the previous page.', example='39f8e0bc005e4f309379701645f4****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model ListMediaWorkflowExecutionsResponseBody = {
  mediaWorkflowExecutionList?: {
    mediaWorkflowExecution?: [ 
    {
      activityList?: {
        activity?: [ 
        {
          code?: string(name='Code', description='The error code returned.

*   The specific error code appears if the activity status is **Fail**.
*   This parameter is not returned if the activity status is **Success**.', example='null'),
          endTime?: string(name='EndTime', description='The end time of the activity.', example='2016-04-01T06:54:00Z'),
          jobId?: string(name='JobId', description='The ID of the job generated when the activity is executed. We recommend that you save this ID for subsequent calls of other operations.', example='2376030d9d0849399cd20e20c876****'),
          MNSMessageResult?: {
            errorCode?: string(name='ErrorCode', description='The error code returned when the job fails. If the job is successful, this parameter is not returned.', example='The Topic/Queue config is empty, not send message'),
            errorMessage?: string(name='ErrorMessage', description='The error message returned when the job fails. If the job is successful, this parameter is not returned.', example='MessageConfigEmpty'),
            messageId?: string(name='MessageId', description='The ID of the success message. If the job fails, this parameter is not returned.', example='4f3bc83233de4e2f81c7dade443e****'),
          }(name='MNSMessageResult', description='The message sent by Message Service (MNS) to notify the user of the job result.'),
          message?: string(name='Message', description='The error message returned.

*   The detailed error message appears if the activity status is **Fail**.
*   This parameter is not returned if the activity status is **Success**.', example='null'),
          name?: string(name='Name', description='The name of the media workflow activity. 

>  The name of an activity in a media workflow is unique.', example='Act-2'),
          startTime?: string(name='StartTime', description='The start time of the activity.', example='2016-04-01T06:53:45Z'),
          state?: string(name='State', description='The status of the activity. Valid values:

*   **Running**: The activity is being executed.
*   **Fail**: The activity failed to be executed.
*   **Skipped**: The activity was skipped.
*   **Success**: The activity was successfully executed.

>  For example, the high-definition and standard-definition transcoding activities are to be run after the analysis activity is complete. The system determines the activity to run based on the analysis result. If the definition of the input video content is insufficient, the high-definition transcoding activity may be skipped.', example='Success'),
          type?: string(name='Type', description='The type of the media workflow activity. Valid values: Start, Snapshot, Transcode, Analysis, and Report. For more information, see [Media workflow activities](~~68494~~).', example='Start'),
        }
      ](name='Activity')
      }(name='ActivityList', description='The list of activities that are executed in the media workflow.'),
      creationTime?: string(name='CreationTime', description='The time when the execution instance was created.', example='2016-04-01T06:53:43Z'),
      input?: {
        inputFile?: {
          bucket?: string(name='Bucket', description='The name of the OSS bucket in which the input file is stored.', example='example-bucket-****'),
          location?: string(name='Location', description='The ID of the OSS region where the input file resides.', example='cn-shanghai'),
          object?: string(name='Object', description='The name of the OSS object that is used as the input file.', example='example-mediaWorkflow-****/example-object-****/example.mp4'),
        }(name='InputFile', description='The information about the storage location of the input file of the media workflow in OSS.'),
        userData?: string(name='UserData', description='The custom data.', example='example data'),
      }(name='Input', description='The custom data of the media workflow.'),
      mediaId?: string(name='MediaId', description='The ID of the media file. This media file ID is associated with all information generated by the media workflow.', example='512046582a924698a41e0f8b0d2b****'),
      mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the media workflow.', example='43b7335a4b1d4fe883670036affb****'),
      name?: string(name='Name', description='The name of the media workflow.', example='example-mediaworkflow-****'),
      runId?: string(name='RunId', description='The ID of the execution instance.', example='48e33690ac19445488c706924321****'),
      state?: string(name='State', description='The status of the execution instance. Valid values:

*   **running**: The execution instance is in progress.
*   **Completed**: The execution instance is complete.

>  A value of Completed indicates that the execution instance is complete. For the information about whether each activity, such as Transcode or Snapshot, is successful, check the status of the activity.

*   **Fail**: The execution instance failed.
*   **Success**: The execution instance was successful.', example='Success'),
    }
  ](name='MediaWorkflowExecution')
  }(name='MediaWorkflowExecutionList', description='The list of media workflow execution instances.'),
  nextPageToken?: string(name='NextPageToken', description='The token that is used to retrieve the next page of the query results.', example='39f8e0bc005e4f309379701645f4****'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='D1D5C080-8E2F-5030-8AB4-13092F17631B'),
}

model ListMediaWorkflowExecutionsResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: ListMediaWorkflowExecutionsResponseBody(name='body'),
}

/**
  * This operation returns execution instances only in the last 90 days.
  * ## QPS limit
  * You can call this operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request ListMediaWorkflowExecutionsRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return ListMediaWorkflowExecutionsResponse
 */
async function listMediaWorkflowExecutionsWithOptions(request: ListMediaWorkflowExecutionsRequest, runtime: Util.RuntimeOptions): ListMediaWorkflowExecutionsResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.inputFileURL)) {
    query['InputFileURL'] = request.inputFileURL;
  }
  if (!Util.isUnset(request.maximumPageSize)) {
    query['MaximumPageSize'] = request.maximumPageSize;
  }
  if (!Util.isUnset(request.mediaWorkflowId)) {
    query['MediaWorkflowId'] = request.mediaWorkflowId;
  }
  if (!Util.isUnset(request.mediaWorkflowName)) {
    query['MediaWorkflowName'] = request.mediaWorkflowName;
  }
  if (!Util.isUnset(request.nextPageToken)) {
    query['NextPageToken'] = request.nextPageToken;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'ListMediaWorkflowExecutions',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * This operation returns execution instances only in the last 90 days.
  * ## QPS limit
  * You can call this operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request ListMediaWorkflowExecutionsRequest
  * @return ListMediaWorkflowExecutionsResponse
 */
async function listMediaWorkflowExecutions(request: ListMediaWorkflowExecutionsRequest): ListMediaWorkflowExecutionsResponse {
  var runtime = new Util.RuntimeOptions{};
  return listMediaWorkflowExecutionsWithOptions(request, runtime);
}

model QueryAnalysisJobListRequest {
  analysisJobIds?: string(name='AnalysisJobIds', description='The IDs of the template analysis jobs.

*   You can query up to 10 jobs at a time.
*   Separate multiple IDs with commas (,).', example='bb558c1cc25b45309aab5be44d19****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model QueryAnalysisJobListResponseBody = {
  analysisJobList?: {
    analysisJob?: [ 
    {
      analysisConfig?: {
        propertiesControl?: {
          crop?: {
            height?: string(name='Height', description='The height of the video after the margins were cropped out.

>  This parameter is invalid if the Mode parameter is set to Auto or None.', example='8'),
            left?: string(name='Left', description='The left margin that was cropped out.

>  This parameter is invalid if the Mode parameter is set to Auto or None.', example='8'),
            mode?: string(name='Mode', description='The cropping mode. Valid values: Valid values:

*   **Auto**: Cropping was automatically run.
*   **Force**: Cropping was forced to run.
*   **None**: Cropping was forced not to run.
*   This parameter is required if the value of the Crop parameter is not an empty JSON object.', example='Auto'),
            top?: string(name='Top', description='The top margin that was cropped out.

>  This parameter is invalid if the Mode parameter is set to Auto or None.', example='8'),
            width?: string(name='Width', description='The width of the video after the margins were cropped out.

>  This parameter is invalid if the Mode parameter is set to Auto or None.', example='8'),
          }(name='Crop', description='The cropping configuration of video images.'),
          deinterlace?: string(name='Deinterlace', description='Indicates whether deinterlacing was forced to run. Valid values:

*   **Auto**: Deinterlacing was automatically run.
*   **Force**: Deinterlacing was forced to run.
*   **None**: Deinterlacing was forced not to run.', example='Auto'),
        }(name='PropertiesControl', description='The control on the attributes of the job output.'),
        qualityControl?: {
          methodStreaming?: string(name='MethodStreaming', description='The playback mode. Valid values:

*   **network**: online playback
*   **local**: playback on on-premises devices
*   Default value: **network**.', example='network'),
          rateQuality?: string(name='RateQuality', description='The quality level of the job output. Default value: **25**.', example='25'),
        }(name='QualityControl', description='The quality control on the job output.'),
      }(name='AnalysisConfig', description='The job configuration.'),
      code?: string(name='Code', description='The error code returned when the job fails.', example='InvalidParameter.ResourceNotFound'),
      creationTime?: string(name='CreationTime', description='The time when the job was created.', example='2014-01-10T12:00:00Z'),
      id?: string(name='Id', description='The ID of the template analysis job.', example='57f6aa3f84824309bcba67231b406****'),
      inputFile?: {
        bucket?: string(name='Bucket', description='The name of the OSS bucket to which the input file is uploaded.', example='example-bucket'),
        location?: string(name='Location', description='The ID of the OSS region.', example='oss-cn-hangzhou'),
        object?: string(name='Object', description='The name of the input file uploaded to the Object Storage Service (OSS) bucket.', example='example.flv'),
      }(name='InputFile', description='The information about the job input.'),
      MNSMessageResult?: {
        errorCode?: string(name='ErrorCode', description='The error code returned when the job fails. This parameter is not returned if the job is successful.', example='InvalidParameter.ResourceNotFound'),
        errorMessage?: string(name='ErrorMessage', description='The error message returned when the job fails. This parameter is not returned if the job is successful.', example='The resource operated \\"PipelineId\\" cannot be found'),
        messageId?: string(name='MessageId', description='The ID of the success message. This parameter is not returned if the job fails.', example='3ca84a39a9024f19853b21be9cf9****'),
      }(name='MNSMessageResult', description='The message sent by Message Service (MNS) to notify the user of the job result.'),
      message?: string(name='Message', description='The error message returned when the job fails.', example='The resource operated \\"PipelineId\\" cannot be found'),
      percent?: long(name='Percent', description='The transcoding progress.', example='86'),
      pipelineId?: string(name='PipelineId', description='The ID of the ApsaraVideo Media Processing (MPS) queue to which the job is added.', example='bb558c1cc25b45309aab5be44d19****'),
      priority?: string(name='Priority', description='The priority of the job in the MPS queue to which the job is added. 

- Valid values: **1 to 10**. A value of 10 indicates the highest priority.
- Default value: **10**.', example='8'),
      state?: string(name='State', description='The status of the job. Valid values:

*   **Submitted**: The job has been submitted.
*   **Analyzing**: The job is being run.
*   **Success**: The job is successful.
*   **Fail**: The job fails.', example='Success'),
      templateList?: {
        template?: [ 
        {
          audio?: {
            bitrate?: string(name='Bitrate', description='The audio bitrate of the output file.

*   Valid values: **8 to 1000**.
*   Unit: Kbit/s.
*   Default value: **128**.', example='128'),
            channels?: string(name='Channels', description='The number of sound channels. Default value: **2**.', example='2'),
            codec?: string(name='Codec', description='The audio codec format. Default value: **aac**. Valid values:

*   **aac**
*   **mp3**
*   **vorbis**
*   **flac**', example='aac'),
            profile?: string(name='Profile', description='The codec profile of the audio file. Valid values when the **Codec** parameter is set to **aac**:

*   **aac_low**
*   **aac_he**
*   **aac_he_v2**
*   **aac_ld**
*   **aac_eld**', example='aac_low'),
            qscale?: string(name='Qscale', description='The level of quality control on the audio.', example='15'),
            samplerate?: string(name='Samplerate', description='The sampling rate.

*   Unit: Hz.
*   Default value: **44100**.', example='44100'),
          }(name='Audio', description='The audio codec configuration.'),
          container?: {
            format?: string(name='Format', description='The container format.', example='flv'),
          }(name='Container', description='The container configuration.'),
          id?: string(name='Id', description='The ID of the transcoding template.', example='S00000000-00****'),
          muxConfig?: {
            gif?: {
              finalDelay?: string(name='FinalDelay', description='The interval between two consecutive loops for the GIF format. Unit: 0.01s. For example, a value of 500 indicates 5 seconds.', example='0'),
              loop?: string(name='Loop', description='The number of loops for the GIF or WebP format. Default value: 0.', example='0'),
            }(name='Gif', description='The GIF format.'),
            segment?: {
              duration?: string(name='Duration', description='The length of the segment. Unit: seconds.', example='10'),
            }(name='Segment', description='The segment configuration.'),
          }(name='MuxConfig', description='The transmuxing configuration.'),
          name?: string(name='Name', description='The name of the template.', example='FLV-UD'),
          state?: string(name='State', description='The status of the template. Valid values:

*   **Normal**: The template is in the normal state.
*   **Deleted**: The template is deleted.', example='Normal'),
          transConfig?: {
            transMode?: string(name='TransMode', description='The transcoding mode. Default value: **onepass**. Valid values:

*   **onepass**
*   **twopass**
*   **CBR**', example='onepass'),
          }(name='TransConfig', description='The general transcoding configuration.'),
          video?: {
            bitrate?: string(name='Bitrate', description='The average bitrate of the video. Unit: Kbit/s.', example='1000'),
            bitrateBnd?: {
              max?: string(name='Max', description='The upper limit of the total bitrate. Unit: Kbit/s.', example='1500'),
              min?: string(name='Min', description='The lower limit of the total bitrate. Unit: Kbit/s.', example='800'),
            }(name='BitrateBnd', description='The average bit rate range of the video.'),
            bufsize?: string(name='Bufsize', description='The size of the buffer.

*   Unit: KB.
*   Default value: **6000**.', example='6000'),
            codec?: string(name='Codec', description='The video codec format. Valid values: H.264 and H.265. Default value: **H.264**.', example='H.264'),
            crf?: string(name='Crf', description='The constant rate factor, which is the default quality control setting.

*   Default value when the Codec parameter is set to H.264: **23**. Default value when the Codec parameter is set to H.265: **26**.
*   If this parameter is returned, the setting of the Bitrate parameter is invalid.', example='26'),
            degrain?: string(name='Degrain', description='The level of the independent denoising algorithm.', example='5'),
            fps?: string(name='Fps', description='The frame rate.

*   The value is 60 if the frame rate of the input file exceeds 60.
*   Default value: **the frame rate of the input file**.', example='25'),
            gop?: string(name='Gop', description='The maximum number of frames between two keyframes. Default value: **250**.', example='250'),
            height?: string(name='Height', description='The height of the video.

*   Unit: pixel.
*   Default value: **the original height of the video**.', example='720'),
            maxrate?: string(name='Maxrate', description='The maximum video bitrate.

*   Valid values: **10 to 50000**.
*   Unit: Kbit/s.', example='2000'),
            pixFmt?: string(name='PixFmt', description='The pixel format for video color encoding. Valid values: standard pixel formats such as yuv420p and yuvj420p.', example='yuv420p'),
            preset?: string(name='Preset', description='The preset video algorithm. Valid values: veryfast, fast, medium, slow, and slower. Default value: **medium**.', example='medium'),
            profile?: string(name='Profile', description='The codec profile. Valid values:

*   **baseline**: applicable to mobile devices.
*   **main**: applicable to standard-definition devices.
*   **high**: applicable to high-definition devices.
*   Default value: **high**.', example='high'),
            qscale?: string(name='Qscale', description='The level of quality control on the video.', example='15'),
            scanMode?: string(name='ScanMode', description='The scan mode. Valid values:

*   **interlaced**: An interlaced scan was performed.
*   **progressive**: A progressive scan was performed.', example='interlaced'),
            width?: string(name='Width', description='The width of the video.

*   Unit: pixel.
*   Default value: **the original width of the video**.', example='1280'),
          }(name='Video', description='The video codec configuration.'),
        }
      ](name='Template')
      }(name='TemplateList', description='The list of matched preset templates.'),
      userData?: string(name='UserData', description='The custom data.', example='testid-001'),
    }
  ](name='AnalysisJob')
  }(name='AnalysisJobList', description='The IDs of the template analysis jobs.'),
  nonExistAnalysisJobIds?: {
    string?: [ string ](name='String')
  }(name='NonExistAnalysisJobIds', description='The IDs of the template analysis jobs that do not exist. If all queried job IDs exist, the response does not contain this parameter.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='5CA6E020-4102-4FFF-AA56-5ED7ECD811A1'),
}

model QueryAnalysisJobListResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: QueryAnalysisJobListResponseBody(name='body'),
}

/**
  * You can call this operation to query up to 10 template analysis jobs at a time.
  * ## QPS limit
  * You can call this operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request QueryAnalysisJobListRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return QueryAnalysisJobListResponse
 */
async function queryAnalysisJobListWithOptions(request: QueryAnalysisJobListRequest, runtime: Util.RuntimeOptions): QueryAnalysisJobListResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.analysisJobIds)) {
    query['AnalysisJobIds'] = request.analysisJobIds;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'QueryAnalysisJobList',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * You can call this operation to query up to 10 template analysis jobs at a time.
  * ## QPS limit
  * You can call this operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request QueryAnalysisJobListRequest
  * @return QueryAnalysisJobListResponse
 */
async function queryAnalysisJobList(request: QueryAnalysisJobListRequest): QueryAnalysisJobListResponse {
  var runtime = new Util.RuntimeOptions{};
  return queryAnalysisJobListWithOptions(request, runtime);
}

model QueryFpDBDeleteJobListRequest {
  jobIds?: string(name='JobIds', description='The IDs of the jobs of clearing or deleting a media fingerprint library. You can obtain the job IDs from the response parameters of the [SubmitFpDBDeleteJob](~~209341~~) operation. Separate multiple job IDs with commas (,). If you leave this parameter empty, the system returns the latest 20 jobs that are submitted.', example='2288c6ca184c0e47098a5b665e2a12****,78dc866518b843259669df58ed30****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model QueryFpDBDeleteJobListResponseBody = {
  fpDBDeleteJobList?: {
    fpDBDeleteJob?: [ 
    {
      code?: string(name='Code', description='The error code returned when the job fails. This parameter is not returned if the job is successful.', example='ServiceUnavailable'),
      creationTime?: string(name='CreationTime', description='The point in time when the job was created.', example='2020-06-30T00:33:18Z'),
      delType?: string(name='DelType', description='The operation type.', example='Purge'),
      finishTime?: string(name='FinishTime', description='The point in time when the job was complete.', example='2020-06-30T00:34:02Z'),
      fpDBId?: string(name='FpDBId', description='The ID of the media fingerprint library.', example='88c6ca184c0e47098a5b665e2a12****'),
      id?: string(name='Id', description='The ID of the job.', example='25bacf2824614bcf9273dc0744db****'),
      message?: string(name='Message', description='The error message returned when the job fails. This parameter is not returned if the job is successful.', example='The request has failed due to a temporary failure of the server.'),
      pipelineId?: string(name='PipelineId', description='The ID of the ApsaraVideo Media Processing (MPS) queue.', example='fb712a6890464059b1b2ea7c8647****'),
      status?: string(name='Status', description='The status of the job. Valid values:

*   **Queuing**: The job is waiting in the queue.
*   **Analysing**: The job is in progress.
*   **Success**: The job is successful.
*   **Fail**: The job fails.', example='Success'),
      userData?: string(name='UserData', description='The custom data.', example='example data'),
    }
  ](name='FpDBDeleteJob')
  }(name='FpDBDeleteJobList', description='The jobs of deleting a media fingerprint library. For more information, see the "FpDBDeleteJob" section of the [Data types](https://icms.alibaba-inc.com/content/mps/cc2a58?l=1\\&m=16051\\&n=23657) topic.'),
  nonExistIds?: {
    string?: [ string ](name='String')
  }(name='NonExistIds', description='The IDs of the jobs that do not exist. This parameter is not returned if all specified jobs are found.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='4247B23C-26DE-529F-8D9F-FD6811AE979B'),
}

model QueryFpDBDeleteJobListResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: QueryFpDBDeleteJobListResponseBody(name='body'),
}

/**
  * You can call this operation to query the specified jobs of clearing or deleting a media fingerprint library based on the job IDs. If you do not specify job IDs, the system returns the latest 20 jobs that are submitted.
  * ## QPS limit
  * You can call this operation up to 500 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request QueryFpDBDeleteJobListRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return QueryFpDBDeleteJobListResponse
 */
async function queryFpDBDeleteJobListWithOptions(request: QueryFpDBDeleteJobListRequest, runtime: Util.RuntimeOptions): QueryFpDBDeleteJobListResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.jobIds)) {
    query['JobIds'] = request.jobIds;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'QueryFpDBDeleteJobList',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * You can call this operation to query the specified jobs of clearing or deleting a media fingerprint library based on the job IDs. If you do not specify job IDs, the system returns the latest 20 jobs that are submitted.
  * ## QPS limit
  * You can call this operation up to 500 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request QueryFpDBDeleteJobListRequest
  * @return QueryFpDBDeleteJobListResponse
 */
async function queryFpDBDeleteJobList(request: QueryFpDBDeleteJobListRequest): QueryFpDBDeleteJobListResponse {
  var runtime = new Util.RuntimeOptions{};
  return queryFpDBDeleteJobListWithOptions(request, runtime);
}

model QueryFpFileDeleteJobListRequest {
  jobIds?: string(name='JobIds', description='The IDs of the jobs of deleting media files from a media fingerprint library. You can obtain the job IDs from the response parameters of the [SubmitFpFileDeleteJob](~~209274~~) operation. Separate multiple job IDs with commas (,). If you leave this parameter empty, the system returns the latest 20 jobs that are submitted.', example='d98459323c024947a104f6a50cbf****,c2dc694696f1441591c5012a73c1****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model QueryFpFileDeleteJobListResponseBody = {
  fpFileDeleteJobList?: {
    fpFileDeleteJob?: [ 
    {
      code?: string(name='Code', description='The error code returned when the job fails. This parameter is not returned if the job is successful.', example='ServiceUnavailable'),
      creationTime?: string(name='CreationTime', description='The point in time when the job was created.', example='2020-06-30T00:33:18Z'),
      fileIds?: string(name='FileIds', description='The IDs of the files.', example='41e6536e4f2250e2e9bf26cdea19****'),
      finishTime?: string(name='FinishTime', description='The point in time when the job was complete.', example='2020-06-30T00:34:02Z'),
      fpDBId?: string(name='FpDBId', description='The ID of the media fingerprint library.', example='88c6ca184c0e47098a5b665e2a12****'),
      id?: string(name='Id', description='The ID of the job.', example='25bacf2824614bcf9273dc0744db****'),
      message?: string(name='Message', description='The error message returned when the job fails. This parameter is not returned if the job is successful.', example='The request has failed due to a temporary failure of the server.'),
      pipelineId?: string(name='PipelineId', description='The ID of the ApsaraVideo Media Processing (MPS) queue.', example='fb712a6890464059b1b2ea7c8647****'),
      status?: string(name='Status', description='The status of the job. Valid values:

*   **Queuing**: The job is waiting in the queue.
*   **Analysing**: The job is in progress.
*   **Success**: The job is successful.
*   **Fail**: The job fails.', example='Success'),
      userData?: string(name='UserData', description='The custom data.', example='example data'),
    }
  ](name='FpFileDeleteJob')
  }(name='FpFileDeleteJobList', description='The jobs of deleting media files from a media fingerprint library. For more information, see the "FpFileDeleteJob" section of the [Data types](https://icms.alibaba-inc.com/content/mps/cc2a58?l=1\\&m=16051\\&n=23657) topic.'),
  nonExistIds?: {
    string?: [ string ](name='String')
  }(name='NonExistIds', description='The IDs of the jobs that do not exist.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='D127C68E-F1A1-4CE5-A874-8FF724881A12'),
}

model QueryFpFileDeleteJobListResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: QueryFpFileDeleteJobListResponseBody(name='body'),
}

/**
  * You can call this operation to query the specified jobs of deleting media files from a media fingerprint library based on the job IDs. If you do not specify job IDs, the system returns the latest 20 jobs that are submitted.
  * ## QPS limit
  * You can call this operation up to 500 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request QueryFpFileDeleteJobListRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return QueryFpFileDeleteJobListResponse
 */
async function queryFpFileDeleteJobListWithOptions(request: QueryFpFileDeleteJobListRequest, runtime: Util.RuntimeOptions): QueryFpFileDeleteJobListResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.jobIds)) {
    query['JobIds'] = request.jobIds;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'QueryFpFileDeleteJobList',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * You can call this operation to query the specified jobs of deleting media files from a media fingerprint library based on the job IDs. If you do not specify job IDs, the system returns the latest 20 jobs that are submitted.
  * ## QPS limit
  * You can call this operation up to 500 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request QueryFpFileDeleteJobListRequest
  * @return QueryFpFileDeleteJobListResponse
 */
async function queryFpFileDeleteJobList(request: QueryFpFileDeleteJobListRequest): QueryFpFileDeleteJobListResponse {
  var runtime = new Util.RuntimeOptions{};
  return queryFpFileDeleteJobListWithOptions(request, runtime);
}

model QueryFpShotJobListRequest {
  endOfJobCreatedTimeRange?: string(name='EndOfJobCreatedTimeRange'),
  jobIds?: string(name='JobIds', description='The IDs of the media fingerprint analysis jobs that you want to query. To view the job IDs, log on to the **MPS console (https://mps.console.aliyun.com/overview)**, click **Tasks** in the left-side navigation pane, and then click the **Video DNA** tab on the Tasks page. You can query up to 10 media fingerprint analysis jobs at a time. Separate multiple job IDs with commas (,).', example='88c6ca184c0e47098a5b665e2a12****'),
  maximumPageSize?: long(name='MaximumPageSize'),
  nextPageToken?: string(name='NextPageToken'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  pipelineId?: string(name='PipelineId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  startOfJobCreatedTimeRange?: string(name='StartOfJobCreatedTimeRange'),
  state?: string(name='State'),
}

model QueryFpShotJobListResponseBody = {
  fpShotJobList?: {
    fpShotJob?: [ 
    {
      code?: string(name='Code', description='The error code returned when the job fails.', example='InvalidParameter.UUIDFormatInvalid'),
      creationTime?: string(name='CreationTime', description='The point in time when the job was created.', example='2017-01-10T12:00:00Z'),
      duration?: int32(name='Duration'),
      fileId?: string(name='FileId', description='The ID of the matched file in the media fingerprint library.', example='ebb51ee30f0b49aba959823fa991****'),
      finishTime?: string(name='FinishTime', description='The point in time when the job was complete.', example='0'),
      fpShotConfig?: {
        fpDBId?: string(name='FpDBId', description='The ID of the media fingerprint library.', example='2288c6ca184c0e47098a5b665e2a12****'),
        primaryKey?: string(name='PrimaryKey', description='The unique primary key of the input video.', example='3ca84a39a9024f19853b21be9cf9****'),
        saveType?: string(name='SaveType', description='The storage type. Valid values:

- **nosave**: The fingerprints of the job input are not saved to the media fingerprint library.
- **save**: The fingerprints of the job input are saved to the media fingerprint library only if the job input is not duplicated with media content in the media fingerprint library.
- **forcesave**: The fingerprints of the job input are forcibly saved to the media fingerprint library.', example='save'),
      }(name='FpShotConfig', description='The configurations of the job.'),
      fpShotResult?: {
        audioFpShots?: {
          fpShot?: [ 
          {
            fpShotSlices?: {
              fpShotSlice?: [ 
              {
                duplication?: {
                  duration?: string(name='Duration', description='The length.', example='3'),
                  start?: string(name='Start', description='The start point in time.', example='0'),
                }(name='Duplication', description='The start point in time and length of the similar audio clip in the audio file that has similar fingerprints to the input audio in the audio fingerprint library.'),
                input?: {
                  duration?: string(name='Duration', description='The length.', example='5'),
                  start?: string(name='Start', description='The start point in time.', example='0'),
                }(name='Input', description='The start point in time and length of the similar audio clip in the input audio.'),
                similarity?: string(name='Similarity', description='The overall similarity of the input audio against audio files that have similar fingerprints to the input audio in the audio fingerprint library.', example='0'),
              }
            ](name='FpShotSlice')
            }(name='FpShotSlices', description='The array of audio files that have similar fingerprints to the input audio in the audio fingerprint library.'),
            primaryKey?: string(name='PrimaryKey', description='The unique primary key of the input video.', example='498ac941373341599c4777c8d884****'),
            similarity?: string(name='Similarity', description='The overall similarity of the input audio against audio files that have similar fingerprints to the input audio in the audio fingerprint library.', example='0'),
          }
        ](name='FpShot')
        }(name='AudioFpShots', description='The audio fingerprint analysis results.'),
        fpShots?: {
          fpShot?: [ 
          {
            fpShotSlices?: {
              fpShotSlice?: [ 
              {
                duplication?: {
                  duration?: string(name='Duration', description='The length.', example='48'),
                  start?: string(name='Start', description='The start point in time.', example='1260'),
                }(name='Duplication', description='The start point in time and length of the similar video clip in the video that has similar fingerprints to the input video in the video fingerprint library.'),
                input?: {
                  duration?: string(name='Duration', description='The length.', example='48'),
                  start?: string(name='Start', description='The start point in time.', example='46'),
                }(name='Input', description='The start point in time and length of the similar video clip in the input video.'),
                similarity?: string(name='Similarity', description='The similarity of the input video clip against the video that has similar fingerprints to the input video in the video fingerprint library.', example='0'),
              }
            ](name='FpShotSlice')
            }(name='FpShotSlices', description='The array of videos that have similar fingerprints to the input video in the video fingerprint library.'),
            primaryKey?: string(name='PrimaryKey', description='The unique primary key of the input video.', example='498ac941373341599c4777c8d884****'),
            similarity?: string(name='Similarity', description='The overall similarity of the input video against videos that have similar fingerprints to the input video in the video fingerprint library. The overall similarity is the average value of the similarities of the input video against all similar video clips. If multiple similar video clips exist in the video fingerprint library, the similarities of the input video against multiple similar video clips are returned.', example='0.8914769887924194'),
          }
        ](name='FpShot')
        }(name='FpShots', description='The video fingerprint analysis results.'),
        textFpShots?: {
          textFpShot?: [ 
          {
            primaryKey?: string(name='PrimaryKey', description='The unique primary key of the input text.', example='3e34ac649945b53a1b0f863ce030****'),
            similarity?: string(name='Similarity', description='The similarity of the input text against text snippets that have similar fingerprints to the input text in the text fingerprint library.', example='1.0'),
            textFpShotSlices?: {
              textFpShotSlice?: [ 
              {
                duplicationText?: string(name='DuplicationText', description='The text snippet that has similar fingerprints to the input text in the text fingerprint library.'),
                inputFragment?: {
                  duration?: string(name='Duration', description='The length.', example='3'),
                  start?: string(name='Start', description='The start point in time.', example='0'),
                }(name='InputFragment', description='The start point in time and length of the similar text snippet in the input text.'),
                inputText?: string(name='InputText', description='The input text for text fingerprint analysis.'),
                similarity?: string(name='Similarity', description='The similarity of the input text against the text snippet that has similar fingerprints to the input text in the text fingerprint library.', example='1.0'),
              }
            ](name='TextFpShotSlice')
            }(name='TextFpShotSlices', description='The array of text snippets that have similar fingerprints to the input text in the text fingerprint library.'),
          }
        ](name='TextFpShot')
        }(name='TextFpShots', description='The text fingerprint analysis results.'),
      }(name='FpShotResult', description='The results of the media fingerprint analysis job.'),
      id?: string(name='Id', description='The ID of the job.', example='88c6ca184c0e47098a5b665e2a12****'),
      input?: string(name='Input', description='The information about the job input.', example='{"Bucket":"oss-test","Location":"oss-cn-beiing","Object":"test.mp4"}'),
      inputFile?: {
        bucket?: string(name='Bucket', description='The OSS bucket in which the job input resides.', example='oss-test'),
        location?: string(name='Location', description='The OSS region in which the job input resides.', example='oss-cn-beiing'),
        object?: string(name='Object', description='The Object Storage Service (OSS) object that is used as the job input.', example='test.mp4'),
      }(name='InputFile', description='The information about the job input.'),
      message?: string(name='Message', description='The error message returned when the job fails. This parameter is not returned if the job is successful.', example='The parameter \\"Id\\" is invalid.A uuid must:1)be comprised of chracters[a-f],numbers[0-9];2)be 32 characters long'),
      pipelineId?: string(name='PipelineId', description='The ID of the ApsaraVideo Media Processing (MPS) queue.', example='88c6ca184c0e47098a5b665e2a12****'),
      state?: string(name='State', description='The status of the job. Valid values:

- **Queuing**: The job is waiting in the queue.
- **Analysing**: The job is in progress.
- **Success**: The job is successful.
- **Fail**: The job fails.', example='Success'),
      userData?: string(name='UserData', description='The custom data.', example='testid-001'),
    }
  ](name='FpShotJob')
  }(name='FpShotJobList', description='The array of media fingerprint analysis jobs.'),
  nextPageToken?: string(name='NextPageToken'),
  nonExistIds?: {
    string?: [ string ](name='String')
  }(name='NonExistIds', description='The job IDs that do not exist. This parameter is not returned if all specified jobs are found.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='25818875-5F78-4A13-BEF6-D7393642CA58'),
}

model QueryFpShotJobListResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: QueryFpShotJobListResponseBody(name='body'),
}

/**
  * *   After a media fingerprint analysis job is submitted, the media fingerprint service compares the fingerprints of the job input with those of the media files in the media fingerprint library. You can call this operation to query the job results.
  * *   You can query the results of a text fingerprint analysis job only in the China (Shanghai) region.
  * *   You can call this operation to query the results of up to 10 media fingerprint analysis jobs at a time.
  * ## QPS limit
  * You can call this operation up to 500 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request QueryFpShotJobListRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return QueryFpShotJobListResponse
 */
async function queryFpShotJobListWithOptions(request: QueryFpShotJobListRequest, runtime: Util.RuntimeOptions): QueryFpShotJobListResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.endOfJobCreatedTimeRange)) {
    query['EndOfJobCreatedTimeRange'] = request.endOfJobCreatedTimeRange;
  }
  if (!Util.isUnset(request.jobIds)) {
    query['JobIds'] = request.jobIds;
  }
  if (!Util.isUnset(request.maximumPageSize)) {
    query['MaximumPageSize'] = request.maximumPageSize;
  }
  if (!Util.isUnset(request.nextPageToken)) {
    query['NextPageToken'] = request.nextPageToken;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.pipelineId)) {
    query['PipelineId'] = request.pipelineId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.startOfJobCreatedTimeRange)) {
    query['StartOfJobCreatedTimeRange'] = request.startOfJobCreatedTimeRange;
  }
  if (!Util.isUnset(request.state)) {
    query['State'] = request.state;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'QueryFpShotJobList',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * *   After a media fingerprint analysis job is submitted, the media fingerprint service compares the fingerprints of the job input with those of the media files in the media fingerprint library. You can call this operation to query the job results.
  * *   You can query the results of a text fingerprint analysis job only in the China (Shanghai) region.
  * *   You can call this operation to query the results of up to 10 media fingerprint analysis jobs at a time.
  * ## QPS limit
  * You can call this operation up to 500 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request QueryFpShotJobListRequest
  * @return QueryFpShotJobListResponse
 */
async function queryFpShotJobList(request: QueryFpShotJobListRequest): QueryFpShotJobListResponse {
  var runtime = new Util.RuntimeOptions{};
  return queryFpShotJobListWithOptions(request, runtime);
}

model QueryIProductionJobRequest {
  jobId?: string(name='JobId', example='88c6ca184c0e432bbf5b665e2a15****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model QueryIProductionJobResponseBody = {
  functionName?: string(name='FunctionName', example='ImageCartoonize'),
  input?: string(name='Input', example='oss://example-****.oss-cn-hangzhou.aliyuncs.com/example.mp4'),
  jobId?: string(name='JobId', example='88c6ca184c0e432bbf5b665e2a15****'),
  jobParams?: string(name='JobParams', example='{mode:"gif"}'),
  output?: string(name='Output', example='oss://example-****.oss-cn-hangzhou.aliyuncs.com/iproduction/{source}-{timestamp}-{sequenceId}.srt'),
  pipelineId?: string(name='PipelineId', example='39f8e0bc005e4f309379701645f4****'),
  requestId?: string(name='RequestId', example='D127C68E-F1A1-4CE5-A874-8FF724881A12'),
  result?: string(name='Result', example='{"Code":"Success","Data":"{\\"result\\":[{\\"file\\":\\"iproduction/test-result.jpg\\"},{\\"file\\":\\"iproduction/test-origin.jpg\\"}]}","Message":"Successful."}'),
  state?: string(name='State', example='Success'),
  userData?: string(name='UserData', example='null'),
}

model QueryIProductionJobResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: QueryIProductionJobResponseBody(name='body'),
}

async function queryIProductionJobWithOptions(request: QueryIProductionJobRequest, runtime: Util.RuntimeOptions): QueryIProductionJobResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.jobId)) {
    query['JobId'] = request.jobId;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'QueryIProductionJob',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

async function queryIProductionJob(request: QueryIProductionJobRequest): QueryIProductionJobResponse {
  var runtime = new Util.RuntimeOptions{};
  return queryIProductionJobWithOptions(request, runtime);
}

model QueryJobListRequest {
  includePipelineInfo?: boolean(name='IncludePipelineInfo'),
  jobIds?: string(name='JobIds', description='The IDs of transcoding jobs. Separate multiple IDs with commas (,). You can query a maximum of 10 transcoding jobs at a time. You can log on to the [ApsaraVideo Media Processing (MPS) console](https://mps.console.aliyun.com/overview) and click **Tasks** in the left-side navigation pane to obtain job IDs. Alternatively, you can obtain job IDs from the response to the [SubmitJobs](~~29226~~) operation.

>  If you do not set the JobIds parameter, the `InvalidParameter` error code is returned.', example='bb558c1cc25b45309aab5be44d19****,d1ce4d3efcb549419193f50f1fcd****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model QueryJobListResponseBody = {
  jobList?: {
    job?: [ 
    {
      code?: string(name='Code', description='The error code that is returned if the transcoding job fails. This parameter is not returned if the job succeeds.', example='InvalidParameter.NullValue'),
      creationTime?: string(name='CreationTime', description='The time when the job was created.', example='2014-01-10T12:00:00Z'),
      finishTime?: string(name='FinishTime', description='The time when the job was complete.', example='2014-01-10T12:20:25Z'),
      input?: {
        bucket?: string(name='Bucket', description='The name of the OSS bucket in which the input file is stored.', example='exampleBucket'),
        location?: string(name='Location', description='The ID of the OSS region in which the input file resides.', example='oss-cn-shanghai'),
        object?: string(name='Object', description='The name of the OSS object that is used as the input file.', example='video_01.mp4'),
      }(name='Input', description='The information about the job input.'),
      jobId?: string(name='JobId', description='The ID of the job.', example='31fa3c9ca8134fb4b0b0f7878301****'),
      MNSMessageResult?: {
        errorCode?: string(name='ErrorCode', description='The error code returned if the job fails. This parameter is not returned if the job is successful.', example='InvalidParameter.ResourceNotFound'),
        errorMessage?: string(name='ErrorMessage', description='The error message that is returned if the job fails. This parameter is not returned if the job is successful.', example='The resource operated “%s” cannot be found.'),
        messageId?: string(name='MessageId', description='The ID of the success message.', example='123'),
      }(name='MNSMessageResult', description='The message sent by Message Service (MNS) to notify the user of the job result.'),
      message?: string(name='Message', description='The message that is returned if the job fails. This parameter is not returned if the job succeeds.', example='The specified parameter "%s" cannot be null.'),
      output?: {
        audio?: {
          bitrate?: string(name='Bitrate', description='The audio bitrate of the output file.

*   Unit: Kbit/s.
*   Default value: **128**.', example='128'),
          channels?: string(name='Channels', description='The number of sound channels.

*   Valid values: 1, 2, 3, 4, 5, 6, 7, and 8.
*   Default value: **2**.', example='2'),
          codec?: string(name='Codec', description='The audio encoding and decoding format.

*   Valid values: aac, mp3, vorbis, and flac.
*   Default value: **aac**.', example='aac'),
          profile?: string(name='Profile', description='The codec profile of the audio. Valid values when the Codec parameter is set to aac: aaclow, aache, aachev2, aacld, and aaceld.', example='aaclow'),
          qscale?: string(name='Qscale', description='The level of quality control on the audio.', example='15'),
          samplerate?: string(name='Samplerate', description='The sampling rate.

*   Valid values: 22050, 32000, 44100, 48000, and 96000.
*   Unit: Hz
*   Default value: 44100.

>  If the video container format is FLV and the audio codec is MP3, you cannot set this parameter to 32000, 48000, or 96000. If the audio codec is MP3, you cannot set this parameter to 96000.', example='44100'),
          volume?: {
            level?: string(name='Level', description='The volume adjustment range. Default value: **-20db**.', example='-20'),
            method?: string(name='Method', description='The volume adjustment method. Valid values:

*   **auto**: The volume is automatically adjusted.
*   **dynamic**: The volume is dynamically adjusted.
*   **linear**: The volume is linearly adjusted.', example='auto'),
          }(name='Volume', description='The volume configuration.'),
        }(name='Audio', description='The audio configurations.

>  If this value was set in the request parameter, the corresponding parameters in the specified transcoding template are overwritten.'),
        audioStreamMap?: string(name='AudioStreamMap', description='The sequence number of the audio stream.

*   Format: 0:a:{sequence number}. Example: 0:a:0.
*   The sequence number is the index of the audio stream in the list and starts from 0.
*   If you do not specify a sequence number, the default audio stream is used.', example='0:a:0'),
        clip?: {
          timeSpan?: {
            duration?: string(name='Duration', description='The duration of the clip.

*   Format: hh:mm:ss\\[.SSS].
*   Example: 01:00:59.999.

Or

*   Format: sssss\\[.SSS].
*   Example: 32000.23.', example='01:00:59.999'),
            seek?: string(name='Seek', description='The time when the clip starts.

*   Format: hh:mm:ss\\[.SSS].
*   Example: 01:59:59.999.

Or

*   Format: sssss\\[.SSS].
*   Example: 32000.23.', example='01:59:59.999'),
          }(name='TimeSpan', description='The time span of the clip.'),
        }(name='Clip', description='The information about the media clip.'),
        container?: {
          format?: string(name='Format', description='The container format.

*   Default value: mp4.
*   Video formats include FLV, MP4, HLS (M3U8 + TS), and MPEG-DASH (MPD + fMP4)
*   Audio formats include MP3, MP4, Ogg, FLAC, and M4A.
*   Image formats include GIF and WebP. If the container format is GIF, the video codec must be set to GIF.
*   If the container format is WebP, the video codec must be set to WebP.
*   If the container format is FLV, the video codec cannot be set to H.265.', example='mp4'),
        }(name='Container', description='Information about the container.'),
        deWatermark?: string(name='DeWatermark', description='The configuration of watermark blurring. The value must be a JSON object. For more information, see [Parameter details](~~29253~~).', example='{"0":[{"l":10,"t":10,"w":10,"h":10},{"l":100,"t":0.1,"w":10,"h":10}],"128000":[],"250000":[{"l":0.2,"t":0.1,"w":0.01,"h":0.05}]}'),
        encryption?: {
          id?: string(name='Id', description='The encryption ID.', example='31fa3c9ca8134f9cec2b4b0b0f78****'),
          key?: string(name='Key', description='The key that is used to encrypt the video.', example='encryptionkey128'),
          keyType?: string(name='KeyType', description='The key encryption method. Valid values: Base64 and KMS.

>  For example, if the key is `encryptionkey128`, the value can be `Base64("encryptionkey128")` or `KMS(Base64("encryptionkey128")`, depending on the encryption method used.', example='Base64'),
          keyUri?: string(name='KeyUri', description='The URL that is used to request the key. The URL is Base64-encoded.', example='https://1161758785*****.cn-shanghai.fc.aliyuncs.com/2016-08-15/proxy/HLS-decyptServer/decyptServer/'),
          skipCnt?: string(name='SkipCnt', description='The number of unencrypted frames at the beginning of the video. Leaving these frames unencrypted enables video playback to quickly start.', example='3'),
          type?: string(name='Type', description='The encryption type. Valid value: **hls-aes-128**.', example='hls-aes-128'),
        }(name='Encryption', description='The encryption configuration. Only outputs in the M3U8 format are supported.'),
        extendData?: string(name='ExtendData', description='The custom fields.', example='testid-002'),
        m3U8NonStandardSupport?: {
          TS?: {
            md5Support?: boolean(name='Md5Support', description='Indicates whether to support the output of the md5 value of the TS file in the M3U8 file. Valid values:

*   **true**: supported.
*   **false**: not supported.', example='true'),
            sizeSupport?: boolean(name='SizeSupport', description='Indicates whether to support the output of the size of the TS file in the M3U8 file. Valid values:

*   **true**: supported.
*   **false**: not supported.', example='true'),
          }(name='TS', description='The non-standard support configuration for M3U8 about TS files. The value must be a JSON object. For more information, see [Parameter details](~~29253~~).'),
        }(name='M3U8NonStandardSupport', description='The non-standard support configuration for M3U8. The value must be a JSON object. For more information, see [Parameter details](~~29253~~).'),
        mergeConfigUrl?: string(name='MergeConfigUrl', description='The URL of the merging configuration file. You can specify only either the MergeList or MergeConfigUrl parameter.  

- The file that you specify for the MergeConfigUrl parameter can contain up to 50 merged clips.
- The MergeConfigUrl parameter specifies the URL of the configuration file for merging clips. Make sure that the configuration file is stored as an object in OSS and that MPS can access the OSS object. For information about the file content, see the details about merging parameters.
- Example of the content of mergeConfigfile: `{"MergeList":[{"MergeURL":"http://exampleBucket****.oss-cn-hangzhou.aliyuncs.com/video_01.mp4"}]}`.', example='https://ceshi-***.oss-cn-shanghai.aliyuncs.com/ccc/p0903q9wkkb.m3u8'),
        mergeList?: {
          merge?: [ 
          {
            duration?: string(name='Duration', description='The duration of the clip.

*   Valid formats: hh:mm:ss\\[.SSS] and sssss\\[.SSS].
*   Examples: 01:59:59.999 and 32000.23.', example='01:59:59.999'),
            mergeURL?: string(name='MergeURL', description='The OSS URL of the clip.

*   Example: `http://example-bucket-.oss-cn-hangzhou.aliyuncs.com/example-object.flv`.
*   The object must be URL-encoded by using the UTF-8 standard. For more information, see [URL encoding](~~423796~~).', example='http://example-bucket.oss-cn-hangzhou.aliyuncs.com/example-object.flv'),
            roleArn?: string(name='RoleArn', description='The Alibaba Cloud Resource Name (ARN) of the role used for proxy authorization.', example='acs:ram::<your uid>:role/<your role name>'),
            start?: string(name='Start', description='The start point in time of the clip.

*   Valid formats: hh:mm:ss\\[.SSS] and sssss\\[.SSS].
*   Examples: 01:59:59.999 and 32000.23.', example='01:59:59.999'),
          }
        ](name='Merge')
        }(name='MergeList', description='The merging configuration. A maximum of four URLs can be merged.'),
        multiSpeedInfo?: {
          code?: string(name='Code'),
          downgradePolicy?: string(name='DowngradePolicy'),
          duration?: double(name='Duration'),
          enable?: string(name='Enable'),
          message?: string(name='Message'),
          realSpeed?: double(name='RealSpeed'),
          settingSpeed?: int32(name='SettingSpeed'),
          timeCost?: double(name='TimeCost'),
        }(name='MultiSpeedInfo'),
        muxConfig?: {
          gif?: {
            ditherMode?: string(name='DitherMode', description='The color dithering algorithm of the palette. Valid values: sierra and bayer.', example='bayer'),
            finalDelay?: string(name='FinalDelay', description='The duration for which the final frame is paused. Unit: centiseconds.', example='0'),
            isCustomPalette?: string(name='IsCustomPalette', description='Indicates whether the custom palette was used. Valid values:

- **true**: custom.
- **false**: non-custom.', example='false'),
            loop?: string(name='Loop', description='The loop count.', example='0'),
          }(name='Gif', description='The transmuxing configuration for GIF.'),
          segment?: {
            duration?: string(name='Duration', description='The length of the segment. Unit: seconds.', example='20'),
          }(name='Segment', description='The segment configurations. The value is a JSON object.'),
          webp?: {
            loop?: string(name='Loop', description='The loop count.', example='0'),
          }(name='Webp', description='The transmuxing configuration for WebP.'),
        }(name='MuxConfig', description='The transmuxing configurations. If this value was set in the request parameter, the corresponding parameters in the specified transcoding template are overwritten.'),
        openingList?: {
          opening?: [ 
          {
            height?: string(name='Height', description='The height of the opening part.

*   Valid values: values in the range of (0, 4096), -1, and full.
*   A value of -1 indicates that the height of the source of the opening part is retained.
*   A value of full indicates that the height of the main part is used for the opening part.
*   Default value: **-1**.', example='-1'),
            start?: string(name='Start', description='The amount of time after which the opening part is played.

*   The amount of time after which the opening part is played. The value starts from 0.
*   Unit: seconds.
*   Default value: **0**.', example='0'),
            width?: string(name='Width', description='The width of the opening part.

*   Valid values: values in the range of (0, 4096), -1, and full.
*   A value of -1 indicates that the width of the source of the opening part is retained.
*   A value of full indicates that the width of the main part is used for the opening part.
*   Default value: **-1**.', example='-1'),
            openUrl?: string(name='openUrl', description='The OSS URL of the opening part of the video.', example='http://example.oss-cn-shanghai.aliyuncs.com/t5.mp4'),
          }
        ](name='Opening')
        }(name='OpeningList', description='The list of opening parts. The value must be a JSON object.'),
        outSubtitleList?: {
          outSubtitle?: [ 
          {
            map?: string(name='Map', description='The video track. Format: `0:{stream}:{stream sequence number}`, that is, `0:v:{video_index}`. The value of stream is v, which indicates an audio stream of a video. The sequence number is the index of the audio stream in the list and starts from 0.', example='0:v:0'),
            message?: string(name='Message', description='The error message returned if the job fails to be created. This parameter is not returned if the job is created.', example='The specified parameter “%s” cannot be null.'),
            outSubtitleFile?: {
              bucket?: string(name='Bucket', description='The OSS bucket in which the output file is stored.', example='exampleBucket'),
              location?: string(name='Location', description='The ID of the OSS region in which the output file resides.', example='oss-cn-hangzhou'),
              object?: string(name='Object', description='The name of the OSS object of the output file.', example='example.flv'),
              roleArn?: string(name='RoleArn', description='The ARN of the role used for proxy authorization.', example='acs:ram::<your uid>:role/<your role name>'),
            }(name='OutSubtitleFile', description='The details of the output file.'),
            success?: boolean(name='Success', description='Indicates whether the job succeeded. Valid values:

*   **true**: succeeded.
*   **fasle**: failed.', example='true'),
          }
        ](name='OutSubtitle')
        }(name='OutSubtitleList', description='The output subtitle list.'),
        outputFile?: {
          bucket?: string(name='Bucket', description='The OSS bucket in which the output file is stored.', example='example-bucket'),
          location?: string(name='Location', description='The ID of the OSS region in which the output file resides.', example='oss-cn-hangzhou'),
          object?: string(name='Object', description='The name of the OSS object of the output file.', example='example-output.flv'),
          roleArn?: string(name='RoleArn', description='The ARN of the role used for proxy authorization.', example='acs:ram::<your uid>:role/<your role name>'),
        }(name='OutputFile', description='The information about the output file.'),
        priority?: string(name='Priority', description='The priority of the job in the MPS queue to which the job is added. 

- A value of 10 indicates the highest priority.
- Default value: **6**.', example='6'),
        properties?: {
          bitrate?: string(name='Bitrate', description='The bitrate of the video.', example='490'),
          duration?: string(name='Duration', description='The duration of the video.', example='17'),
          fileFormat?: string(name='FileFormat', description='The format of the video.', example='mp4'),
          fileSize?: string(name='FileSize', description='The size of the file.', example='1057273'),
          format?: {
            bitrate?: string(name='Bitrate', description='The total bitrate.', example='490.784'),
            duration?: string(name='Duration', description='The total duration.', example='17.234000'),
            formatLongName?: string(name='FormatLongName', description='The full name of the container format.', example='QuickTime / MOV'),
            formatName?: string(name='FormatName', description='The short name of the container format. Valid values: mov, mp4, m4a, 3gp, 3g2, and mj2.', example='mov'),
            numPrograms?: string(name='NumPrograms', description='The total number of program streams.', example='0'),
            numStreams?: string(name='NumStreams', description='The total number of media streams.', example='2'),
            size?: string(name='Size', description='The size of the file.', example='1057273'),
            startTime?: string(name='StartTime', description='The start time.', example='-0.064000'),
          }(name='Format', description='The format information.'),
          fps?: string(name='Fps', description='The frame rate of the video.', example='30'),
          height?: string(name='Height', description='The length of the video.', example='1280'),
          sourceLogos?: {
            sourceLogo?: [ 
            {
              source?: string(name='Source', description='The keywords.', example='example'),
            }
          ](name='SourceLogo')
          }(name='SourceLogos', description='The non-engine layer keywords.'),
          streams?: {
            audioStreamList?: {
              audioStream?: [ 
              {
                bitrate?: string(name='Bitrate', description='The bitrate.', example='64.136'),
                channelLayout?: string(name='ChannelLayout', description='The output layout of the sound channels.', example='mono'),
                channels?: string(name='Channels', description='The number of sound channels.', example='1'),
                codecLongName?: string(name='CodecLongName', description='The full name of the encoding format.', example='AAC (Advanced Audio Coding)'),
                codecName?: string(name='CodecName', description='The short name of the encoding format.', example='aac'),
                codecTag?: string(name='CodecTag', description='The tag of the encoding format.', example='0x6134706d'),
                codecTagString?: string(name='CodecTagString', description='The tag string of the encoding format.', example='mp4'),
                codecTimeBase?: string(name='CodecTimeBase', description='The codec time base.', example='1/32000'),
                duration?: string(name='Duration', description='The duration.', example='17.223562'),
                index?: string(name='Index', description='The sequence number of the audio stream. The value indicates the position of the audio stream in all audio streams.', example='1'),
                lang?: string(name='Lang', description='The language. For more information, see [FFmpeg language definition](https://www.ffmpeg.org/ffmpeg-all.html#Metadata) and [ISO 639](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes).', example='und'),
                numFrames?: string(name='NumFrames', description='The total number of frames.', example='50'),
                sampleFmt?: string(name='SampleFmt', description='The sampling format.', example='fltp'),
                samplerate?: string(name='Samplerate', description='The sampling rate.', example='32000'),
                startTime?: string(name='StartTime', description='The start time.', example='0.064000'),
                timebase?: string(name='Timebase', description='The time base.', example='1/32000'),
              }
            ](name='AudioStream')
            }(name='AudioStreamList', description='The list of audio streams.'),
            subtitleStreamList?: {
              subtitleStream?: [ 
              {
                index?: string(name='Index', description='The sequence number of the subtitle stream. 

This parameter indicates the position of the subtitle stream in all subtitle streams.', example='1'),
                lang?: string(name='Lang', description='The language. For more information, see [FFmpeg language definition](https://www.ffmpeg.org/ffmpeg-all.html#Metadata) and [ISO 639](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes).', example='eng'),
              }
            ](name='SubtitleStream')
            }(name='SubtitleStreamList', description='The list of subtitle streams.'),
            videoStreamList?: {
              videoStream?: [ 
              {
                avgFPS?: string(name='AvgFPS', description='The average frame rate of the video stream.', example='30.0'),
                bitrate?: string(name='Bitrate', description='The bitrate.', example='421.117'),
                codecLongName?: string(name='CodecLongName', description='The full name of the encoding format.', example='H.264 / AVC / MPEG-4 AVC / MPEG-4 part 10'),
                codecName?: string(name='CodecName', description='The short name of the encoding format.', example='h264'),
                codecTag?: string(name='CodecTag', description='The tag of the encoding format.', example='0x31637661'),
                codecTagString?: string(name='CodecTagString', description='The tag string of the encoding format.', example='avc1'),
                codecTimeBase?: string(name='CodecTimeBase', description='The codec time base.', example='1/60'),
                dar?: string(name='Dar', description='The display aspect ratio (DAR).', example='9:16'),
                duration?: string(name='Duration', description='The duration.', example='17.233333'),
                fps?: string(name='Fps', description='The frame rate.', example='30.0'),
                hasBFrames?: string(name='HasBFrames', description='Indicates whether the video stream contains bidirectional frames (B-frames).', example='2'),
                height?: string(name='Height', description='The latter number in the video resolution. The number indicates the video height.', example='1280'),
                index?: string(name='Index', description='The sequence number of the video stream. The value indicates the position of the video stream in all video streams.', example='0'),
                lang?: string(name='Lang', description='The language. For more information, see [FFmpeg language definition](https://www.ffmpeg.org/ffmpeg-all.html#Metadata) and [ISO 639](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes).', example='eng'),
                level?: string(name='Level', description='The codec level.', example='31'),
                networkCost?: {
                  avgBitrate?: string(name='AvgBitrate', description='The average bitrate.', example='300'),
                  costBandwidth?: string(name='CostBandwidth', description='The maximum bandwidth that was consumed.', example='10'),
                  preloadTime?: string(name='PreloadTime', description='The amount of preload time.', example='8'),
                }(name='NetworkCost', description='The network bandwidth consumption.'),
                numFrames?: string(name='NumFrames', description='The total number of frames.', example='30'),
                pixFmt?: string(name='PixFmt', description='The pixel format of the video stream.', example='yuv420p'),
                profile?: string(name='Profile', description='The codec profile.', example='high'),
                sar?: string(name='Sar', description='The sample aspect ratio (SAR) of the video stream.', example='1:1'),
                startTime?: string(name='StartTime', description='The start time.', example='0.000000'),
                timebase?: string(name='Timebase', description='The time base of the video stream.', example='1/15360'),
                width?: string(name='Width', description='The former number in the video resolution. The number indicates the video width.', example='720'),
                bitsPerRawSample?: string(name='bitsPerRawSample', description='The number of binary bits used by each sample or pixel.', example='8'),
                colorPrimaries?: string(name='colorPrimaries', description='The primary colors.', example='bt709'),
                colorTransfer?: string(name='colorTransfer', description='The color transfer configuration.', example='bt709'),
              }
            ](name='VideoStream')
            }(name='VideoStreamList', description='The list of video streams.'),
          }(name='Streams', description='The stream information.'),
          width?: string(name='Width', description='The width of the video.', example='720'),
        }(name='Properties', description='The media properties.'),
        rotate?: string(name='Rotate', description='The video rotation angle.', example='90'),
        subtitleConfig?: {
          extSubtitleList?: {
            extSubtitle?: [ 
            {
              charEnc?: string(name='CharEnc', description='The character set used by the external subtitle. 

- Valid values: UTF-8, GBK, BIG5, and auto.
- Default value: **auto**.

>  If you set this parameter to auto, the detected character set may not be the actual character set. We recommend that you set this parameter to another value.', example='auto'),
              fontName?: string(name='FontName', description='The font of the hard subtitles converted from external subtitles. Default value: SimSum. For more information, see [Fonts](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/font-name).', example='"WenQuanYi Zen Hei", "Yuanti SC Regular", "SimSun"'),
              input?: {
                bucket?: string(name='Bucket', description='The OSS bucket in which the input file is stored.', example='example-bucket-****'),
                location?: string(name='Location', description='The ID of the OSS region in which the input file resides.', example='oss-cn-hangzhou'),
                object?: string(name='Object', description='The name of the OSS object that is used as the input file.', example='example-output.flv'),
              }(name='Input', description='The input subtitle file. 

- Files in the SRT or ASS format are supported. For more information, see the part about input parameters in [Parameter details](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/parameter-details).
- Example: `{“Bucket”:”example-bucket”,“Location”:”oss-cn-hangzhou”,“Object”:”example.srt”}`.'),
            }
          ](name='ExtSubtitle')
          }(name='ExtSubtitleList', description='The list of external subtitles.'),
          subtitleList?: {
            subtitle?: [ 
            {
              map?: string(name='Map', description='The audio track. Format: `0:{stream}:{stream sequence number}`, that is, `0:a:{audio_index}`. The value of stream is a, which indicates an audio stream. The sequence number is the index of the audio stream in the list and starts from 0.', example='0:a:0'),
            }
          ](name='Subtitle')
          }(name='SubtitleList', description='The list of subtitles.'),
        }(name='SubtitleConfig', description='The subtitle configuration.'),
        superReso?: {
          isHalfSample?: string(name='IsHalfSample', description='Indicates whether to obtain parameters related to the sampling rate. Valid values:

- **true**: The parameters are obtained.
- **false**: The parameters are not obtained.', example='true'),
        }(name='SuperReso', description='Indicates that the resolution of the source video is used.'),
        tailSlateList?: {
          tailSlate?: [ 
          {
            bgColor?: string(name='BgColor', description='The color of the bars that are added to the ending part if the size of the ending part is smaller than that of the main part. Default value: White. For more information, see [Background colors](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/parameter-details).', example='White'),
            blendDuration?: string(name='BlendDuration', description='The amount of time between the end of the main part and the beginning of the ending part. During the video part transition, the last frame of the main part fades out, and the first frame of the ending part fades in. Unit: seconds. Default value: 0.', example='0'),
            height?: string(name='Height', description='The height of the ending part. 

- Valid values: values in the range of (0, 4096), -1, and full.
- A value of -1 indicates that the height of the source of the ending part is retained.
- A value of full indicates that the height of the main part is used for the ending part.
- Default value: **-1**.', example='-1'),
            isMergeAudio?: boolean(name='IsMergeAudio', description='Indicates whether to merge the audio content of the ending part. Valid values:

- **true**: yes.
- **false**: no.', example='true'),
            start?: string(name='Start', description='The time at which the ending part is played.', example='00000.00'),
            tailUrl?: string(name='TailUrl', description='The OSS URL of the ending part.', example='http://example-bucket-****.oss-cn-hangzhou.aliyuncs.com/opening_01.flv'),
            width?: string(name='Width', description='The width of the ending part. Valid values: values in the range of (0, 4096), -1, and full.  

- A value of -1 indicates that the width of the source of the ending part is retained.
- A value of full indicates that the width of the main part is used for the ending part.
- Default value: **-1**.', example='-1'),
          }
        ](name='TailSlate')
        }(name='TailSlateList', description='The list of ending parts.'),
        templateId?: string(name='TemplateId', description='The ID of the template.', example='S00000001-200010'),
        transConfig?: {
          adjDarMethod?: string(name='AdjDarMethod', description='The method of resolution adjustment. Default value: **none**. Valid values: rescale, crop, pad, and none.', example='none'),
          isCheckAudioBitrate?: string(name='IsCheckAudioBitrate', description='Indicates whether the audio bitrate is checked.

*   If the output audio bitrate is higher than the input audio bitrate, the system considers that the output bitrate equals the input bitrate.
*   **true**: The audio bitrate is checked.
*   **false**: The audio bitrate is not checked.
*   Default value: **false**.', example='false'),
          isCheckAudioBitrateFail?: string(name='IsCheckAudioBitrateFail', description='Indicates whether the audio bitrate is checked. If the output audio bitrate is higher than the audio bitrate of the media source, a transcoding failure is returned without transcoding the media. This value has a higher priority than IsCheckAudioBitrate.

*   **true**: The audio bitrate is checked.
*   **false**: The audio bitrate is not checked.
*   Default value: **false**.', example='false'),
          isCheckReso?: string(name='IsCheckReso', description='Indicates whether the resolution is checked.

*   If the output resolution is higher than the input resolution based on the width or height, the system considers that the output resolution equals the input resolution.
*   **true**: The resolution is checked.
*   **false**: The resolution is not checked.
*   Default value: **false**.', example='false'),
          isCheckResoFail?: string(name='IsCheckResoFail', description='Indicates whether the resolution is checked.

*   If the output resolution is higher than the input resolution based on the width or height, a transcoding failure is returned.
*   **true**: The resolution is checked.
*   **false**: The resolution is not checked.
*   Default value: **false**.', example='false'),
          isCheckVideoBitrate?: string(name='IsCheckVideoBitrate', description='Indicates whether the video bitrate is checked.

*   If the output bitrate is higher than the input bitrate, the system considers that the output bitrate equals the input bitrate.
*   **true**: The video bitrate is checked.
*   **false**: The video bitrate is not checked.
*   Default value: **false**.', example='false'),
          isCheckVideoBitrateFail?: string(name='IsCheckVideoBitrateFail', description='Indicates whether the video bitrate is checked. If the bitrate of the output video is higher than that of the input video, a transcoding failure is returned without transcoding the video. This parameter takes precedence over the IsCheckVideoBitrate parameter.

*   **true**: The video bitrate is checked.
*   **false**: The video bitrate is not checked.
*   Default value: **false**.', example='false'),
          transMode?: string(name='TransMode', description='The transcoding mode.

*   Valid values: onepass, twopass, and CBR.
*   Default value: **onepass**.', example='onepass'),
        }(name='TransConfig', description='The general transcoding configurations.

>  If this value was set in the request parameter, the corresponding parameters in the specified transcoding template are overwritten.'),
        userData?: string(name='UserData', description='The custom data.', example='testid-001'),
        video?: {
          bitrate?: string(name='Bitrate', description='The average bitrate of the video. Unit: Kbit/s.', example='500'),
          bitrateBnd?: {
            max?: string(name='Max', description='The maximum bitrate.', example='1000'),
            min?: string(name='Min', description='The minimum bitrate.', example='300'),
          }(name='BitrateBnd', description='The average bitrate range of the video.'),
          bufsize?: string(name='Bufsize', description='The size of the buffer.

*   Unit: KB.
*   Default value: **6000**.', example='6000'),
          codec?: string(name='Codec', description='The codec.

*   Valid values: H.264 and H.265.
*   Default value: H.264.', example='H.264'),
          crf?: string(name='Crf', description='The constant rate factor.

*   Default value when the Codec parameter is set to H.264: **23**. Default value when the Codec parameter is set to H.265: **26**.
*   If this parameter is returned, the setting of the Bitrate parameter is invalid.', example='26'),
          crop?: string(name='Crop', description='The crop mode. The following two modes are supported:

*   **border**: automatically detects and removes black bars.
*   A value in the format of width:height:left:top: crops the video image based on the custom setting.', example='1280:800:0:140'),
          degrain?: string(name='Degrain', description='The level of the independent denoising algorithm.', example='5'),
          fps?: string(name='Fps', description='The frame rate.

*   Unit: frames per second.
*   The value is 60 if the frame rate of the input file exceeds 60.
*   By default, the frame rate of the input file is used.', example='25'),
          gop?: string(name='Gop', description='The maximum interval between keyframes or the maximum number of frames in a frame group. Unit: seconds.

*   Default value: **250**.
*   If you specified the maximum number of frames, the value does not contain a unit.', example='250'),
          height?: string(name='Height', description='The height of the video.

*   Unit: pixels.
*   By default, the original video height is used.', example='720'),
          maxFps?: string(name='MaxFps', description='The maximum frame rate.', example='60'),
          maxrate?: string(name='Maxrate', description='The maximum bitrate of the video. Unit: Kbit/s.', example='3000'),
          pad?: string(name='Pad', description='The black bars to be added to the video.

*   Unit: pixels.
*   Format: width:height:left:top.', example='1280:800:0:140'),
          pixFmt?: string(name='PixFmt', description='The pixel format. Standard pixel formats such as yuv420p and yuvj420p are supported.', example='yuv420p'),
          preset?: string(name='Preset', description='The preset video algorithm. Default value: **medium**. Valid values:

*   **veryfast**
*   **fast**
*   **medium**
*   **slow**
*   **slower**', example='medium'),
          profile?: string(name='Profile', description='The codec profile. Valid values: baseline, main, and high.

>  If multiple definitions exist, we recommend that you set this parameter to baseline for the lowest definition to ensure normal playback on low-end devices. Set this parameter to main or high for other definitions.

*   **baseline**: applicable to mobile devices.
*   **main**: applicable to standard-definition devices.
*   **high**: applicable to high-definition devices.
*   Default value: **high**.', example='high'),
          qscale?: string(name='Qscale', description='The level of quality control on the video.', example='15'),
          resoPriority?: string(name='ResoPriority', description='The priority of the resource.', example='1'),
          scanMode?: string(name='ScanMode', description='The scan mode. Valid values:

*   **interlaced**: An interlaced scan is performed.
*   **progressive**: A progressive scan is performed.
*   **auto**: A scan is performed based on the video source.', example='interlaced'),
          width?: string(name='Width', description='The width of the video.

*   Unit: pixels.
*   By default, the original width of the video is used.', example='1280'),
        }(name='Video', description='The video configuration.'),
        videoStreamMap?: string(name='VideoStreamMap', description='The sequence number of the video stream. The sequence number is the index of the video stream in the list and starts from 0. If you do not set the corresponding parameter in the request, the default video stream is selected.', example='0'),
        waterMarkConfigUrl?: string(name='WaterMarkConfigUrl', description='The URL of the watermark configuration file.', example='http://example.com/configure'),
        waterMarkList?: {
          waterMark?: [ 
          {
            dx?: string(name='Dx', description='The horizontal offset of the watermark image relative to the output video. If this parameter is set in the request, the value overwrites the corresponding parameter in the watermark template. Default value: 0. The value can be an integer or a decimal.

*   An integer indicates the pixel value of the horizontal offset.

    *   Valid values: **\\[8,4096]**.
    *   Unit: pixels.

*   A decimal indicates the ratio of the horizontal offset to the width in the output video resolution.

    *   Valid values: (0,1).
    *   The decimal number can be accurate to four decimal places, such as 0.9999. Excessive digits are automatically deleted.', example='100'),
            dy?: string(name='Dy', description='The vertical offset of the watermark image relative to the output video. If this parameter is set in the request, the value overwrites the vertical offset of the watermark image relative to the output video. The value can be an integer or a decimal.

*   An integer indicates the pixel value of the vertical offset.

    *   Valid values: **\\[8,4096]**.
    *   Unit: pixels.

*   A decimal indicates the ratio of the pixel value to the height in the output video resolution in pixels.

    *   Valid values: **(0,1)**.
    *   The decimal number can be accurate to four decimal places, such as 0.9999. Excessive digits are automatically deleted.', example='100'),
            height?: string(name='Height', description='The height of the watermark image. If this parameter is set in the request, the value overwrites the corresponding parameter in the watermark template. The value can be an integer or a decimal.

*   An integer indicates the pixel value of the watermark height.

    *   Valid values: **\\[8,4096]**.
    *   Unit: pixels.

*   A decimal indicates the ratio of the watermark height to the height in the output video resolution.

    *   Valid values: **(0,1)**.
    *   The decimal number can be accurate to four decimal places, such as 0.9999. Excessive digits are automatically deleted.', example='50'),
            inputFile?: {
              bucket?: string(name='Bucket', description='The name of the OSS bucket to which the input file is uploaded.', example='example-bucket'),
              location?: string(name='Location', description='The ID of the OSS region in which the input file resides.', example='oss-cn-hangzhou'),
              object?: string(name='Object', description='The name of the Object Storage Service (OSS) object that is used as the input file.', example='example-logo-****.png'),
            }(name='InputFile', description='The watermark input file. You can use an image in the PNG format or a file in the MOV format as the watermark input.'),
            referPos?: string(name='ReferPos', description='The position of the watermark. If this parameter is set in the request, the value overwrites the corresponding parameter in the watermark template. Valid values:

*   TopRight: the upper-right corner.
*   TopLeft: the upper-left corner.
*   BottomRight: the lower-right corner.
*   BottomLeft: the lower-left corner.', example='TopRight'),
            type?: string(name='Type', description='The type of the watermark. If this parameter is set in the request, the value overwrites the corresponding parameter in the watermark template. For more information, see [Parameter details](~~29253~~). Valid values:

*   Image: an image watermark.
*   Text: a text watermark.', example='Image'),
            waterMarkTemplateId?: string(name='WaterMarkTemplateId', description='The ID of the watermark template.', example='88c6ca184c0e47098a5b665e2a12****'),
            width?: string(name='Width', description='The width of the watermark image. If this parameter is set in the request, the value overwrites the corresponding parameter in the watermark template. The value can be an integer or a decimal.

*   An integer indicates the pixel value of the watermark width.

    *   Valid values: **\\[8,4096]**.
    *   Unit: pixels.

*   A decimal indicates the ratio of the watermark width to the width in the output video resolution.

    *   Valid values: **(0,1)**.
    *   The decimal number can be accurate to four decimal places, such as 0.9999. Excessive digits are automatically deleted.', example='50'),
          }
        ](name='WaterMark')
        }(name='WaterMarkList', description='The list of watermarks.'),
      }(name='Output', description='The output of the job.'),
      percent?: long(name='Percent', description='The transcoding progress.', example='100'),
      pipeline?: {
        extendConfig?: {
          isBoostNew?: boolean(name='IsBoostNew'),
          maxMultiSpeed?: int32(name='MaxMultiSpeed'),
          multiSpeedDowngradePolicy?: string(name='MultiSpeedDowngradePolicy'),
        }(name='ExtendConfig'),
        id?: string(name='Id'),
        name?: string(name='Name'),
        speed?: string(name='Speed'),
        state?: string(name='State'),
      }(name='Pipeline'),
      pipelineId?: string(name='PipelineId', description='The ID of the MPS queue.', example='88c6ca184c0e47b665e2a1267971****'),
      state?: string(name='State', description='The status of the job. Valid values:

*   **Submitted**: The job has been submitted.
*   **Transcoding**: The job is being processed.
*   **TranscodeSuccess**: The job succeeded.
*   **TranscodeFail**: Transcoding failed.
*   **TranscodeCancelled**: The job has been canceled.', example='TranscodeSuccess'),
    }
  ](name='Job')
  }(name='JobList', description='The list of transcoding jobs.'),
  nonExistJobIds?: {
    string?: [ string ](name='String')
  }(name='NonExistJobIds', description='The list of nonexistent job IDs. If all queried job IDs exist, the response does not contain this parameter.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='197ADF44-104C-514C-9F92-D8924CB34E2A'),
}

model QueryJobListResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: QueryJobListResponseBody(name='body'),
}

/**
  * *   By default, returned jobs are sorted in descending order by CreationTime.
  * *   You can call this operation to query up to 10 transcoding jobs at a time.
  * *   If you do not set the JobIds parameter, the `InvalidParameter` error code is returned.
  * ## QPS limit
  * You can call this API operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request QueryJobListRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return QueryJobListResponse
 */
async function queryJobListWithOptions(request: QueryJobListRequest, runtime: Util.RuntimeOptions): QueryJobListResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.includePipelineInfo)) {
    query['IncludePipelineInfo'] = request.includePipelineInfo;
  }
  if (!Util.isUnset(request.jobIds)) {
    query['JobIds'] = request.jobIds;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'QueryJobList',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * *   By default, returned jobs are sorted in descending order by CreationTime.
  * *   You can call this operation to query up to 10 transcoding jobs at a time.
  * *   If you do not set the JobIds parameter, the `InvalidParameter` error code is returned.
  * ## QPS limit
  * You can call this API operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request QueryJobListRequest
  * @return QueryJobListResponse
 */
async function queryJobList(request: QueryJobListRequest): QueryJobListResponse {
  var runtime = new Util.RuntimeOptions{};
  return queryJobListWithOptions(request, runtime);
}

model QueryMediaCensorJobDetailRequest {
  jobId?: string(name='JobId', description='The ID of the content moderation job. You can obtain the job ID from the response parameters of the [SubmitMediaCensorJob](~~91774~~) operation.', example='2288c6ca184c0e47098a5b665e2a12****'),
  maximumPageSize?: long(name='MaximumPageSize', description='The maximum number of entries to return on each page.

*   Default value: **30**.
*   Valid values: **1 to 300**.', example='30'),
  nextPageToken?: string(name='NextPageToken', description='The token that is used to retrieve the next page of the query results. You can leave this parameter empty when you call this operation to query the results of a content moderation job for the first time. The token of the next page is returned after you call this operation to query the results of a content moderation job for the first time.', example='ae0fd49c0840e14daf0d66a75b83****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model QueryMediaCensorJobDetailResponseBody = {
  mediaCensorJobDetail?: {
    barrageCensorResult?: {
      label?: string(name='Label', description='The labels of the moderation result. Multiple labels are separated with commas (,). Valid values:

*   **spam**: spam
*   **ad**: ads
*   **abuse**: abuse
*   **flood**: excessive junk content
*   **contraband**: prohibited content
*   **meaningless**: meaningless content
*   **normal**: normal content', example='normal'),
      rate?: string(name='Rate', description='The score.', example='99.91'),
      scene?: string(name='Scene', description='The moderation scenario. The value is **antispam**.', example='antispam'),
      suggestion?: string(name='Suggestion', description='The recommended subsequent operation. Valid values:

*   **pass**: The content passes the moderation.
*   **review**: The content needs to be manually reviewed again.
*   **block**: The content needs to be blocked.', example='pass'),
    }(name='BarrageCensorResult', description='The moderation result of live comments.'),
    code?: string(name='Code', description='The error code returned if the job fails. This parameter is not returned if the job is successful.', example='InvalidParameter.ResourceNotFound'),
    coverImageCensorResults?: {
      coverImageCensorResult?: [ 
      {
        bucket?: string(name='Bucket', description='The OSS bucket that stores the video thumbnail.', example='bucket-out-test-****'),
        location?: string(name='Location', description='The OSS region in which the video thumbnail resides.', example='oss-cn-shanghai'),
        object?: string(name='Object', description='The Object Storage Service (OSS) object that is used as the video thumbnail.', example='test/ai/censor/v2/vme-****.jpg'),
        results?: {
          result?: [ 
          {
            label?: string(name='Label', description='The labels of the moderation result. Multiple labels are separated with commas (,).

*   Valid values in the pornographic content moderation scenario:

    *   **normal**: normal content
    *   **sexy**: sexy content
    *   **porn**: pornographic content

*   Valid values in the terrorist content moderation scenario:

    *   **normal**: normal content
    *   **bloody**: bloody content
    *   **explosion**: explosion and smoke
    *   **outfit**: special costume
    *   **logo**: special logo
    *   **weapon**: weapon
    *   **politics**: political content
    *   **violence**: violence
    *   **crowd**: crowd
    *   **parade**: parade
    *   **carcrash**: car accident
    *   **flag**: flag
    *   **location**: landmark
    *   **others**: other content

*   Valid values in the ad moderation scenario:

    *   **normal**: normal content
    *   **ad**: other ads
    *   **politics**: political content in text
    *   **porn**: pornographic content in text
    *   **abuse**: abuse in text
    *   **terrorism**: terrorist content in text
    *   **contraband**: prohibited content in text
    *   **spam**: spam in text
    *   **npx**: illegal ads
    *   **qrcode**: QR code
    *   **programCode**: mini program code

*   Valid values in the live moderation scenario:

    *   **normal**: normal content
    *   **meaningless**: meaningless content, such as a black or white screen
    *   **PIP**: picture-in-picture
    *   **smoking**: smoking
    *   **drivelive**: live broadcasting in a running vehicle

*   Valid values in the logo moderation scenario:

    *   **normal**: normal content
    *   **TV**: controlled TV station logo
    *   **trademark**: trademark', example='normal'),
            rate?: string(name='Rate', description='The score. Valid values: **0 to 100**.', example='100'),
            scene?: string(name='Scene', description='The moderation scenario. Valid values:

*   **porn**: pornographic content moderation
*   **terrorism**: terrorist content moderation
*   **ad**: ad violation moderation
*   **live**: undesirable scene moderation
*   **logo**: logo moderation', example='porn'),
            suggestion?: string(name='Suggestion', description='The recommended subsequent operation. Valid values:

*   **pass**: The content passes the moderation.
*   **review**: The content needs to be manually reviewed again.
*   **block**: The content needs to be blocked.', example='pass'),
          }
        ](name='Result')
        }(name='Results', description='The moderation results.'),
      }
    ](name='CoverImageCensorResult')
    }(name='CoverImageCensorResults', description='The moderation results of thumbnails.'),
    creationTime?: string(name='CreationTime', description='The time when the job was created.', example='2018-09-13T16:32:24Z'),
    descCensorResult?: {
      label?: string(name='Label', description='The labels of the moderation result. Valid values:

*   **spam**: spam
*   **ad**: ads
*   **abuse**: abuse
*   **flood**: excessive junk content
*   **contraband**: prohibited content
*   **meaningless**: meaningless content
*   **normal**: normal content', example='normal'),
      rate?: string(name='Rate', description='The score.', example='100'),
      scene?: string(name='Scene', description='The moderation scenario. The value is **antispam**.', example='antispam'),
      suggestion?: string(name='Suggestion', description='The recommended subsequent operation. Valid values:

*   **pass**: The content passes the moderation.
*   **review**: The content needs to be manually reviewed again.
*   **block**: The content needs to be blocked.', example='review'),
    }(name='DescCensorResult', description='The moderation result of the description.'),
    finishTime?: string(name='FinishTime', description='The time when the job was completed.', example='2018-09-21'),
    input?: {
      bucket?: string(name='Bucket', description='The OSS bucket that stores the input file.', example='bucket-test-in-****'),
      location?: string(name='Location', description='The OSS region in which the input file resides.', example='oss-cn-shanghai'),
      object?: string(name='Object', description='The OSS object that is used as the input file.', example='test/ai/censor/test-****.mp4'),
    }(name='Input', description='The information about the job input.'),
    jobId?: string(name='JobId', description='The ID of the content moderation job.', example='f8f166eea7a44e9bb0a4aecf9543****'),
    message?: string(name='Message', description='The error message returned if the job fails. This parameter is not returned if the job is successful.', example='The resource operated cannot be found'),
    pipelineId?: string(name='PipelineId', description='The ID of the ApsaraVideo Media Processing (MPS) queue that is used to run the job.', example='c5b30b7c0d0e4a0abde1d5f9e751****'),
    state?: string(name='State', description='The status of the job.', example='Success'),
    suggestion?: string(name='Suggestion', description='The overall result of the job. Valid values:

*   **pass**: The content passes the moderation.
*   **review**: The content needs to be manually reviewed again.
*   **block**: The content needs to be blocked.

If the moderation result of one type of the moderated content is review, the overall result is review. If the moderation result of one type of the moderated content is block, the overall result is block.', example='block'),
    titleCensorResult?: {
      label?: string(name='Label', description='The labels of the moderation result. Valid values: 

- **normal**: normal content
- **spam**: spam
- **ad**: ads
- **abuse**: abuse
- **flood**: excessive junk content
- **contraband**: prohibited content
- **meaningless**: meaningless content', example='meaningless'),
      rate?: string(name='Rate', description='The score.', example='99.91'),
      scene?: string(name='Scene', description='The moderation scenario. The value is **antispam**.', example='antispam'),
      suggestion?: string(name='Suggestion', description='The recommended subsequent operation. Valid values:

- **pass**: The content passes the moderation.
- **review**: The content needs to be manually reviewed again.
- **block**: The content needs to be blocked.', example='block'),
    }(name='TitleCensorResult', description='The moderation result of the title.'),
    userData?: string(name='UserData', description='The custom data.', example='example userdata ****'),
    vensorCensorResult?: {
      censorResults?: {
        censorResult?: [ 
        {
          label?: string(name='Label', description='The labels of the moderation result. Multiple labels are separated with commas (,). 

- Valid values in the pornographic content moderation scenario:   - **porn**: pornographic content
  - **sexy**: sexy content
  - **normal**: normal content
- Valid values in the terrorist content moderation scenario:   - **normal**: normal content
  - **bloody**: bloody content
  - **explosion**: explosion and smoke
  - **outfit**: special costume
  - **logo**: special logo
  - **weapon**: weapon
  - **politics**: political content
  - **violence**: violence
  - **crowd**: crowd
  - **parade**: parade
  - **carcrash**: car accident
  - **flag**: flag
  - **location**: landmark
  - **others**: other content
- Valid values in the ad moderation scenario:   - **normal**: normal content
  - **ad**: other ads
  - **politics**: political content in text
  - **porn**: pornographic content in text
  - **abuse**: abuse in text
  - **terrorism**: terrorist content in text
  - **contraband**: prohibited content in text
  - **spam**: spam in text
  - **npx**: illegal ads
  - **qrcode**: QR code
  - **programCode**: mini program code
- Valid values in the live moderation scenario:   - **normal**: normal content
  - **meaningless**: meaningless content, such as a black or white screen
  - **PIP**: picture-in-picture
  - **smoking**: smoking
  - **drivelive**: live broadcasting in a running vehicle
- Valid values in the logo moderation scenario:   - **normal**: normal content
  - **TV**: controlled TV station logo
  - **trademark**: trademark', example='meaningless'),
          rate?: string(name='Rate', description='The score.', example='100'),
          scene?: string(name='Scene', description='The moderation scenario. Valid values: 

- **porn**: pornographic content moderation
- **terrorism**: terrorist content moderation
- **ad**: ad violation moderation
- **live**: undesirable scene moderation
- **logo**: logo moderation', example='terrorism'),
          suggestion?: string(name='Suggestion', description='The recommended subsequent operation. Valid values:

- **pass**: The content passes the moderation.
- **review**: The content needs to be manually reviewed again.
- **block**: The content needs to be blocked.', example='review'),
        }
      ](name='CensorResult')
      }(name='CensorResults', description='A collection of the moderation results. The information includes the summary about various scenarios such as pornographic content and terrorist content.'),
      nextPageToken?: string(name='NextPageToken', description='The token that is used to retrieve the next page of the query results.', example='ea04afcca7cd4e80b9ece8fbb251****'),
      videoTimelines?: {
        videoTimeline?: [ 
        {
          censorResults?: {
            censorResult?: [ 
            {
              label?: string(name='Label', description='The labels of the moderation result. Multiple labels are separated with commas (,).

*   Valid values in the pornographic content moderation scenario:

    *   **porn**: pornographic content
    *   **sexy**: sexy content
    *   **normal**: normal content

*   Valid values in the terrorist content moderation scenario:

    *   **normal**: normal content
    *   **bloody**: bloody content
    *   **explosion**: explosion and smoke
    *   **outfit**: special costume
    *   **logo**: special logo
    *   **weapon**: weapon
    *   **politics**: political content
    *   **violence**: violence
    *   **crowd**: crowd
    *   **parade**: parade
    *   **carcrash**: car accident
    *   **flag**: flag
    *   **location**: landmark
    *   **others**: other content

*   Valid values in the ad moderation scenario:

    *   **normal**: normal content
    *   **ad**: other ads
    *   **politics**: political content in text
    *   **porn**: pornographic content in text
    *   **abuse**: abuse in text
    *   **terrorism**: terrorist content in text
    *   **contraband**: prohibited content in text
    *   **spam**: spam in text
    *   **npx**: illegal ads
    *   **qrcode**: QR code
    *   **programCode**: mini program code

*   Valid values in the live moderation scenario:

    *   **normal**: normal content
    *   **meaningless**: meaningless content, such as a black or white screen
    *   **PIP**: picture-in-picture
    *   **smoking**: smoking
    *   **drivelive**: live broadcasting in a running vehicle

*   Valid values in the logo moderation scenario:

    *   **normal**: normal content
    *   **TV**: controlled TV station logo
    *   **trademark**: trademark', example='porn,ad'),
              rate?: string(name='Rate', description='The score.', example='99.99'),
              scene?: string(name='Scene', description='The moderation scenario. Valid values:

*   **porn**: pornographic content moderation
*   **terrorism**: terrorist content moderation
*   **ad**: ad violation moderation
*   **live**: undesirable scene moderation
*   **logo**: logo moderation', example='porn'),
              suggestion?: string(name='Suggestion', description='The recommended subsequent operation. Valid values:

*   **pass**: The content passes the moderation.
*   **review**: The content needs to be manually reviewed again.
*   **block**: The content needs to be blocked.', example='block'),
            }
          ](name='CensorResult')
          }(name='CensorResults', description='A collection of the moderation results. The information includes the summary about various scenarios such as pornographic content and terrorist content.'),
          object?: string(name='Object', description='The one or more OSS objects that are generated as the output snapshots.

>  In the example, {Count} is a placeholder. The OSS objects that are generated as output snapshots are named `output00001-****.jpg`, `output00002-****.jpg`, and so on.', example='output{Count}.jpg'),
          timestamp?: string(name='Timestamp', description='The position in the video.

Format: `hh:mm:ss[.SSS]`.', example='00:02:59.999'),
        }
      ](name='VideoTimeline')
      }(name='VideoTimelines', description='The moderation results that are sorted in ascending order by time.'),
    }(name='VensorCensorResult', description='The results of video review.'),
    videoCensorConfig?: {
      bizType?: string(name='BizType', description='The custom business type. Default value: common.', example='common'),
      outputFile?: {
        bucket?: string(name='Bucket', description='The OSS bucket that stores the output snapshot.', example='test-bucket-****'),
        location?: string(name='Location', description='The OSS region in which the OSS bucket for storing the output snapshot resides.', example='oss-cn-shanghai'),
        object?: string(name='Object', description='The one or more OSS objects that are generated as the output snapshots.

>  In the example, {Count} is a placeholder. The OSS objects that are generated as output snapshots are named `output00001-****.jpg`, `output00002-****.jpg`, and so on.', example='output{Count}.jpg'),
      }(name='OutputFile', description='The information about output snapshots.'),
      videoCensor?: string(name='VideoCensor', description='Indicates whether the video content needs to be moderated. Default value: **true**. Valid values:

*   **true**: The video content needs to be moderated.
*   **false**: The video content does not need to be moderated.', example='true'),
    }(name='VideoCensorConfig', description='The video moderation configurations.'),
  }(name='MediaCensorJobDetail', description='The results of the content moderation job.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='B42299E6-F71F-465F-8FE9-4FC2E3D3C2CA'),
}

model QueryMediaCensorJobDetailResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: QueryMediaCensorJobDetailResponseBody(name='body'),
}

/**
  * In the content moderation results, the moderation results of the video are sorted in ascending order by time into a timeline. If the video is long, the content moderation results are paginated, and the first page is returned. You can call this operation to query the full moderation results of the video.
  * ## QPS limit
  * You can call this API operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request QueryMediaCensorJobDetailRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return QueryMediaCensorJobDetailResponse
 */
async function queryMediaCensorJobDetailWithOptions(request: QueryMediaCensorJobDetailRequest, runtime: Util.RuntimeOptions): QueryMediaCensorJobDetailResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.jobId)) {
    query['JobId'] = request.jobId;
  }
  if (!Util.isUnset(request.maximumPageSize)) {
    query['MaximumPageSize'] = request.maximumPageSize;
  }
  if (!Util.isUnset(request.nextPageToken)) {
    query['NextPageToken'] = request.nextPageToken;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'QueryMediaCensorJobDetail',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * In the content moderation results, the moderation results of the video are sorted in ascending order by time into a timeline. If the video is long, the content moderation results are paginated, and the first page is returned. You can call this operation to query the full moderation results of the video.
  * ## QPS limit
  * You can call this API operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request QueryMediaCensorJobDetailRequest
  * @return QueryMediaCensorJobDetailResponse
 */
async function queryMediaCensorJobDetail(request: QueryMediaCensorJobDetailRequest): QueryMediaCensorJobDetailResponse {
  var runtime = new Util.RuntimeOptions{};
  return queryMediaCensorJobDetailWithOptions(request, runtime);
}

model QueryMediaCensorJobListRequest {
  endOfJobCreatedTimeRange?: string(name='EndOfJobCreatedTimeRange', example='2022-02-14T02:16:07Z'),
  jobIds?: string(name='JobIds'),
  maximumPageSize?: long(name='MaximumPageSize', example='20'),
  nextPageToken?: string(name='NextPageToken', example='79aff3eee82242e092899db5f669****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  pipelineId?: string(name='PipelineId', example='c5b30b7c0d0e4a0abde1d5f9e751****'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  startOfJobCreatedTimeRange?: string(name='StartOfJobCreatedTimeRange', example='2021-12-22T03:48:05Z'),
  state?: string(name='State', example='All'),
}

model QueryMediaCensorJobListResponseBody = {
  mediaCensorJobList?: {
    mediaCensorJob?: [ 
    {
      barrageCensorResult?: {
        label?: string(name='Label', example='normal'),
        rate?: string(name='Rate', example='99.91'),
        scene?: string(name='Scene', example='antispam'),
        suggestion?: string(name='Suggestion', example='Pass'),
      }(name='BarrageCensorResult'),
      code?: string(name='Code', example='InvalidParameter.ResourceNotFound'),
      coverImageCensorResults?: {
        coverImageCensorResult?: [ 
        {
          bucket?: string(name='Bucket', example='example-Bucket-****'),
          location?: string(name='Location', example='oss-cn-shanghai'),
          object?: string(name='Object', example='test/ai/censor/v2/vme-****.jpg'),
          results?: {
            result?: [ 
            {
              label?: string(name='Label', example='normal'),
              rate?: string(name='Rate', example='100'),
              scene?: string(name='Scene', example='live'),
              suggestion?: string(name='Suggestion', example='pass'),
            }
          ](name='Result')
          }(name='Results'),
        }
      ](name='CoverImageCensorResult')
      }(name='CoverImageCensorResults'),
      creationTime?: string(name='CreationTime', example='2021-11-04T07:25:48Z'),
      descCensorResult?: {
        label?: string(name='Label', example='ad'),
        rate?: string(name='Rate', example='100'),
        scene?: string(name='Scene', example='antispam'),
        suggestion?: string(name='Suggestion', example='Pass'),
      }(name='DescCensorResult'),
      finishTime?: string(name='FinishTime', example='2021-11-04T07:25:48Z'),
      input?: {
        bucket?: string(name='Bucket', example='bucket-test-in-****'),
        location?: string(name='Location', example='oss-cn-shanghai'),
        object?: string(name='Object', example='test/ai/censor/test-****.mp4'),
      }(name='Input'),
      jobId?: string(name='JobId', example='f8f166eea7a44e9bb0a4aecf9543****'),
      message?: string(name='Message', example='The resource operated cannot be found'),
      pipelineId?: string(name='PipelineId', example='c5b30b7c0d0e4a0abde1d5f9e751****'),
      state?: string(name='State', example='Success'),
      suggestion?: string(name='Suggestion', example='Pass'),
      titleCensorResult?: {
        label?: string(name='Label', example='meaningless'),
        rate?: string(name='Rate', example='99.91'),
        scene?: string(name='Scene', example='Antispam'),
        suggestion?: string(name='Suggestion', example='Block'),
      }(name='TitleCensorResult'),
      userData?: string(name='UserData', example='example userdata ****'),
      vensorCensorResult?: {
        censorResults?: {
          censorResult?: [ 
          {
            label?: string(name='Label', example='meaningless'),
            rate?: string(name='Rate', example='100'),
            scene?: string(name='Scene', example='Terrorism'),
            suggestion?: string(name='Suggestion', example='Review'),
          }
        ](name='CensorResult')
        }(name='CensorResults'),
        nextPageToken?: string(name='NextPageToken', example='ea04afcca7cd4e80b9ece8fbb251****'),
        videoTimelines?: {
          videoTimeline?: [ 
          {
            censorResults?: {
              censorResult?: [ 
              {
                label?: string(name='Label', example='normal'),
                rate?: string(name='Rate', example='99.99'),
                scene?: string(name='Scene', example='pron'),
                suggestion?: string(name='Suggestion', example='Block'),
              }
            ](name='CensorResult')
            }(name='CensorResults'),
            object?: string(name='Object', example='output{Count}.jpg'),
            timestamp?: string(name='Timestamp', example='00:02:59.999'),
          }
        ](name='VideoTimeline')
        }(name='VideoTimelines'),
      }(name='VensorCensorResult'),
      videoCensorConfig?: {
        bizType?: string(name='BizType', example='common'),
        outputFile?: {
          bucket?: string(name='Bucket', example='test-bucket-****'),
          location?: string(name='Location', example='oss-cn-shanghai'),
          object?: string(name='Object', example='output{Count}.jpg'),
        }(name='OutputFile'),
        videoCensor?: string(name='VideoCensor', example='true'),
      }(name='VideoCensorConfig'),
    }
  ](name='MediaCensorJob')
  }(name='MediaCensorJobList'),
  nextPageToken?: string(name='NextPageToken', example='9b1a42bc6e8d46e6a1383b7e7f01****'),
  nonExistIds?: {
    string?: [ string ](name='String')
  }(name='NonExistIds'),
  requestId?: string(name='RequestId', example='D1D5C080-8E2F-5030-8AB4-13092F17631B'),
}

model QueryMediaCensorJobListResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: QueryMediaCensorJobListResponseBody(name='body'),
}

async function queryMediaCensorJobListWithOptions(request: QueryMediaCensorJobListRequest, runtime: Util.RuntimeOptions): QueryMediaCensorJobListResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.endOfJobCreatedTimeRange)) {
    query['EndOfJobCreatedTimeRange'] = request.endOfJobCreatedTimeRange;
  }
  if (!Util.isUnset(request.jobIds)) {
    query['JobIds'] = request.jobIds;
  }
  if (!Util.isUnset(request.maximumPageSize)) {
    query['MaximumPageSize'] = request.maximumPageSize;
  }
  if (!Util.isUnset(request.nextPageToken)) {
    query['NextPageToken'] = request.nextPageToken;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.pipelineId)) {
    query['PipelineId'] = request.pipelineId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.startOfJobCreatedTimeRange)) {
    query['StartOfJobCreatedTimeRange'] = request.startOfJobCreatedTimeRange;
  }
  if (!Util.isUnset(request.state)) {
    query['State'] = request.state;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'QueryMediaCensorJobList',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

async function queryMediaCensorJobList(request: QueryMediaCensorJobListRequest): QueryMediaCensorJobListResponse {
  var runtime = new Util.RuntimeOptions{};
  return queryMediaCensorJobListWithOptions(request, runtime);
}

model QueryMediaInfoJobListRequest {
  mediaInfoJobIds?: string(name='MediaInfoJobIds', description='The IDs of the media information analysis jobs. Separate multiple jobs with commas (,). You can query up to 10 jobs at a time.', example='23ca1d184c0e4341e5b665e2a12****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model QueryMediaInfoJobListResponseBody = {
  mediaInfoJobList?: {
    mediaInfoJob?: [ 
    {
      async?: boolean(name='Async', description='Indicates whether the job is in asynchronous mode.', example='true'),
      code?: string(name='Code', description='The error code returned when the job fails.', example='InvalidParameter.JsonObjectFormatInvalid'),
      creationTime?: string(name='CreationTime', description='The time when the job was created.', example='2014-01-10T12:00:00Z'),
      input?: {
        bucket?: string(name='Bucket', description='The name of the OSS bucket that stores the object.', example='example-bucket'),
        location?: string(name='Location', description='The ID of the OSS region.', example='oss-cn-hangzhou'),
        object?: string(name='Object', description='The name of the Object Storage Service (OSS) object used as the input file.', example='example.mp4'),
      }(name='Input', description='The information about the job input.'),
      jobId?: string(name='JobId', description='The ID of the job.', example='23ca1d184c0e4341e5b665e2a12****'),
      MNSMessageResult?: {
        errorCode?: string(name='ErrorCode', description='The error code returned when the job fails. This parameter is not returned if the job is successful.', example='InvalidParameter.JsonObjectFormatInvalid'),
        errorMessage?: string(name='ErrorMessage', description='The error message returned when the job fails. This parameter is not returned if the job is successful.', example='The parameter \\"Input\\" does not conform to the JSON Object specification'),
        messageId?: string(name='MessageId', description='The ID of the success message returned when the job is successful. This parameter is not returned if the job fails.', example='123'),
      }(name='MNSMessageResult', description='The message sent by Message Service (MNS) to notify users of the job result.'),
      message?: string(name='Message', description='The error message returned when the job fails.', example='The parameter ”*” does not conform to the JSON Object specification'),
      pipelineId?: string(name='PipelineId', description='The ID of the MPS queue to which the job is added.', example='88c6ca184c0e432bbf5b665e2a15****'),
      properties?: {
        bitrate?: string(name='Bitrate', description='The bitrate.', example='1630.045'),
        duration?: string(name='Duration', description='The duration.', example='17.226000'),
        fileFormat?: string(name='FileFormat', description='The file format.', example='QuickTime/MOV'),
        fileSize?: string(name='FileSize', description='The size of the file.', example='3509895'),
        format?: {
          bitrate?: string(name='Bitrate', description='The total bitrate.', example='1630.045'),
          duration?: string(name='Duration', description='The total duration.', example='17.226000'),
          formatLongName?: string(name='FormatLongName', description='The full name of the container format.', example='QuickTime/MOV'),
          formatName?: string(name='FormatName', description='The short name of the container format.', example='mov'),
          numPrograms?: string(name='NumPrograms', description='The total number of program streams.', example='2'),
          numStreams?: string(name='NumStreams', description='The total number of media streams.', example='1'),
          size?: string(name='Size', description='The size of the file.', example='3509895'),
          startTime?: string(name='StartTime', description='The start time.', example='0.042000'),
        }(name='Format', description='The format information.'),
        fps?: string(name='Fps', description='The frame rate.', example='25'),
        height?: string(name='Height', description='The height of the video. Unit: pixel.', example='720'),
        streams?: {
          audioStreamList?: {
            audioStream?: [ 
            {
              bitrate?: string(name='Bitrate', description='The bitrate.', example='1536000'),
              channelLayout?: string(name='ChannelLayout', description='The number of sound channels.', example='5.1(side)'),
              channels?: string(name='Channels', description='The output layout of the sound channels.', example='2'),
              codecLongName?: string(name='CodecLongName', description='The full name of the encoding format.', example='DCA (DTS Coherent Acoustics)'),
              codecName?: string(name='CodecName', description='The short name of the encoding format. Valid values:

- **acc**
- **mp3**
- **mp4**
- **ogg**
- **flac**', example='acc'),
              codecTag?: string(name='CodecTag', description='The tag of the encoding format.', example='0x0000'),
              codecTagString?: string(name='CodecTagString', description='The tag string of the encoding format.', example='[0][0][0][0]'),
              codecTimeBase?: string(name='CodecTimeBase', description='The codec time base.', example='1/48000'),
              duration?: string(name='Duration', description='The duration.', example='123'),
              index?: string(name='Index', description='The sequence number of the audio stream. The value indicates the position of the audio stream in all audio streams.', example='1'),
              lang?: string(name='Lang', description='The language. For more information, see [21.20.1 Metadata](https://www.ffmpeg.org/ffmpeg-all.html?spm=a2c4g.11186623.2.66.243851cd2SntfN#Metadata) in FFmpeg documentation.', example='eng'),
              numFrames?: string(name='NumFrames', description='The total number of frames.', example='123'),
              sampleFmt?: string(name='SampleFmt', description='The sampling format.', example='fltp'),
              samplerate?: string(name='Samplerate', description='The sampling rate.', example='48000'),
              startTime?: string(name='StartTime', description='The start time.', example='0.042000'),
              timebase?: string(name='Timebase', description='The time base.', example='1/1000'),
            }
          ](name='AudioStream')
          }(name='AudioStreamList', description='The information about each audio stream.'),
          subtitleStreamList?: {
            subtitleStream?: [ 
            {
              codecLongName?: string(name='CodecLongName', description='The full name of the encoding format.', example='ASS (Advanced SSA) subtitle'),
              codecName?: string(name='CodecName', description='The short name of the encoding format. Valid values:

- **srt**
- **ass**', example='ass'),
              codecTag?: string(name='CodecTag', description='The tag of the encoding format.', example='0x0000'),
              codecTagString?: string(name='CodecTagString', description='The tag string of the encoding format.', example='[0][0][0][0]'),
              codecTimeBase?: string(name='CodecTimeBase', description='The codec time base.', example='0/1'),
              duration?: string(name='Duration', description='The duration. Unit: seconds.', example='1370.116000'),
              index?: string(name='Index', description='The sequence number of the subtitle stream. The value indicates the position of the subtitle stream in all subtitle streams.', example='3'),
              lang?: string(name='Lang', description='The language.', example='eng'),
              startTime?: string(name='StartTime', description='The start time.', example='0.000000'),
              timebase?: string(name='Timebase', description='The time base.', example='1/1000'),
            }
          ](name='SubtitleStream')
          }(name='SubtitleStreamList', description='The information about each subtitle stream.'),
          videoStreamList?: {
            videoStream?: [ 
            {
              avgFPS?: string(name='AvgFPS', description='The average frame rate.', example='24000/1001'),
              bitrate?: string(name='Bitrate', description='The bitrate.', example='30541090'),
              codecLongName?: string(name='CodecLongName', description='The full name of the encoding format.', example='H.264/AVC/MPEG-4 AVC/MPEG-4 part 10'),
              codecName?: string(name='CodecName', description='The short name of the encoding format. Valid values:

*   **h264**
*   **h265**
*   **gif**
*   **webp**', example='h264'),
              codecTag?: string(name='CodecTag', description='The tag of the encoding format.', example='0x0000'),
              codecTagString?: string(name='CodecTagString', description='The tag string of the encoding format.', example='[0][0][0][0]'),
              codecTimeBase?: string(name='CodecTimeBase', description='The codec time base.', example='1001/48000'),
              colorPrimaries?: string(name='ColorPrimaries', description='The level of color reconstruction.', example='700'),
              colorRange?: string(name='ColorRange', description='The color range.', example='700'),
              colorTransfer?: string(name='ColorTransfer', description='The color channel.', example='R255 G83 B170'),
              dar?: string(name='Dar', description='The display aspect ratio (DAR).', example='16:9'),
              duration?: string(name='Duration', description='The duration.', example='100'),
              fps?: string(name='Fps', description='The frame rate.', example='25'),
              hasBFrames?: string(name='HasBFrames', description='Indicates whether the video stream contains bidirectional frames (B-frames). A value of 1 indicates that the video stream contains B-frames. A value of 0 indicates that the video stream does not contain B-frames.', example='0'),
              height?: string(name='Height', description='The latter number in the video resolution. The number indicates the video height.', example='1080'),
              index?: string(name='Index', description='The sequence number of the video stream. The value indicates the position of the video stream in all video streams.', example='1'),
              lang?: string(name='Lang', description='The language.', example='eng'),
              level?: string(name='Level', description='The codec level.', example='41'),
              networkCost?: {
                avgBitrate?: string(name='AvgBitrate', description='The average bitrate.', example='300.34'),
                costBandwidth?: string(name='CostBandwidth', description='The maximum bandwidth consumption.', example='10'),
                preloadTime?: string(name='PreloadTime', description='The amount of preload time.', example='8'),
              }(name='NetworkCost', description='The network bandwidth consumption.'),
              numFrames?: string(name='NumFrames', description='The total number of frames.', example='100'),
              pixFmt?: string(name='PixFmt', description='The pixel format.', example='yuv420p'),
              profile?: string(name='Profile', description='The codec profile.', example='High'),
              rotate?: string(name='Rotate', description='The video rotation angle.', example='180'),
              sar?: string(name='Sar', description='The sample aspect ratio (SAR).', example='1:1'),
              startTime?: string(name='StartTime', description='The start time.', example='0.042000'),
              timebase?: string(name='Timebase', description='The time base.', example='1/1000'),
              width?: string(name='Width', description='The former number in the video resolution. The number indicates the video width.', example='1920'),
            }
          ](name='VideoStream')
          }(name='VideoStreamList', description='The information about each video stream.'),
        }(name='Streams', description='The stream information.'),
        width?: string(name='Width', description='The width of the video. Unit: pixel.', example='1280'),
      }(name='Properties', description='The information about the input file. For more information, see [AliyunProperties](~~29251~~).'),
      state?: string(name='State', description='The status of the job. Valid values:

*   **Analyzing**: The job is being run.
*   **Success**: The job is successful.
*   **Fail**: The job fails.', example='Success'),
      userData?: string(name='UserData', description='The custom data.', example='testid-001'),
    }
  ](name='MediaInfoJob')
  }(name='MediaInfoJobList', description='The details of each returned media information analysis job.'),
  nonExistMediaInfoJobIds?: {
    string?: [ string ](name='String')
  }(name='NonExistMediaInfoJobIds', description='The IDs of the media information analysis jobs that do not exist. If all specified jobs exist, the response does not contain this parameter.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='46A04AA5-B119-41BB-B750-7C5327AC3E7A'),
}

model QueryMediaInfoJobListResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: QueryMediaInfoJobListResponseBody(name='body'),
}

/**
  * *   You can call this operation to query up to 10 media information analysis jobs at a time.
  * *   After you upload a media file, the media information can be retrieved only after a callback is returned, indicating that the media file has been analyzed. If you have not received a callback for a long period, or if you have not configured callback settings but you cannot retrieve the media information long after a media information analysis job is submitted, the job may fail. In this case, [submit a ticket](https://workorder-intl.console.aliyun.com/?spm=5176.12246746.top-nav.dticket.68797bbcm8H408#/ticket/add/?productId=1232) and provide your Alibaba Cloud account ID, the region in which you use ApsaraVideo Media Processing (MPS), and the video ID for troubleshooting.
  * ## QPS limit
  * You can call this API operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request QueryMediaInfoJobListRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return QueryMediaInfoJobListResponse
 */
async function queryMediaInfoJobListWithOptions(request: QueryMediaInfoJobListRequest, runtime: Util.RuntimeOptions): QueryMediaInfoJobListResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.mediaInfoJobIds)) {
    query['MediaInfoJobIds'] = request.mediaInfoJobIds;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'QueryMediaInfoJobList',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * *   You can call this operation to query up to 10 media information analysis jobs at a time.
  * *   After you upload a media file, the media information can be retrieved only after a callback is returned, indicating that the media file has been analyzed. If you have not received a callback for a long period, or if you have not configured callback settings but you cannot retrieve the media information long after a media information analysis job is submitted, the job may fail. In this case, [submit a ticket](https://workorder-intl.console.aliyun.com/?spm=5176.12246746.top-nav.dticket.68797bbcm8H408#/ticket/add/?productId=1232) and provide your Alibaba Cloud account ID, the region in which you use ApsaraVideo Media Processing (MPS), and the video ID for troubleshooting.
  * ## QPS limit
  * You can call this API operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request QueryMediaInfoJobListRequest
  * @return QueryMediaInfoJobListResponse
 */
async function queryMediaInfoJobList(request: QueryMediaInfoJobListRequest): QueryMediaInfoJobListResponse {
  var runtime = new Util.RuntimeOptions{};
  return queryMediaInfoJobListWithOptions(request, runtime);
}

model QueryMediaListRequest {
  includeMediaInfo?: boolean(name='IncludeMediaInfo', description='Specifies whether to include media information in the returned result.

*   Valid values: true and false.
*   Default value: **false**.', example='true'),
  includePlayList?: boolean(name='IncludePlayList', description='Specifies whether to include playback information in the returned result.

*   Valid values: true and false.
*   Default value: **false**.', example='true'),
  includeSnapshotList?: boolean(name='IncludeSnapshotList', description='Specifies whether to include snapshot information in the returned result.

*   Valid values: true and false.
*   Default value: **false**.', example='true'),
  includeSummaryList?: boolean(name='IncludeSummaryList', description='Specifies whether to include summaries in the returned result.

*   Valid values: true and false.
*   Default value: **false**.', example='true'),
  mediaIds?: string(name='MediaIds', description='The IDs of the media files. To obtain the ID of a media file, you can perform the following operations in the ApsaraVideo Media Processing (MPS) console: In the left-side navigation pane, choose **Media Management** > **Media List**. Find the required video and click Manage. The ID of the video is displayed on the Basics tab. Separate multiple IDs with commas (,). You can query up to 10 media files at a time.', example='3e1cd21131a94525be55acf65888****,e26cfa29e784402388463f61dbec****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model QueryMediaListResponseBody = {
  mediaList?: {
    media?: [ 
    {
      bitrate?: string(name='Bitrate', description='The bitrate.', example='2659.326'),
      cateId?: long(name='CateId', description='The ID of the category to which the media file belongs.', example='1'),
      censorState?: string(name='CensorState', description='The review status of the media file. Valid values:

*   **Initiated**: The media file is uploaded but not reviewed.
*   **Pass**: The media file is uploaded and passes the review.', example='Initiated'),
      coverURL?: string(name='CoverURL', description='The OSS URL of the thumbnail.', example='http://example-bucket1-****.oss-cn-hangzhou.aliyuncs.com//example-****.png'),
      creationTime?: string(name='CreationTime', description='The time when the media file was created.', example='2016-09-14T08:30:33Z'),
      description?: string(name='Description', description='The description.', example='This is description ****'),
      duration?: string(name='Duration', description='The duration.', example='7.965000'),
      file?: {
        state?: string(name='State', description='The status of the input file. Valid values:

*   **Normal**: normal
*   **Deleted**: deleted', example='Normal'),
        URL?: string(name='URL', description='The OSS URL of the input file.', example='http://example-bucket-****.oss-cn-hangzhou.aliyuncs.com//example-****.mp4'),
      }(name='File', description='The details of the input file.'),
      format?: string(name='Format', description='The encoding format. Valid values: mov, mp4, m4a, 3gp, 3g2, and mj2.', example='mov'),
      fps?: string(name='Fps', description='The frame rate.', example='25.0'),
      height?: string(name='Height', description='The height of the media file.', example='1080'),
      mediaId?: string(name='MediaId', description='The ID of the media file.', example='3e1cd21131a94525be55acf65888****'),
      mediaInfo?: {
        format?: {
          bitrate?: string(name='Bitrate', description='The bitrate.', example='2659.326'),
          duration?: string(name='Duration', description='The total duration.', example='7.965000'),
          formatLongName?: string(name='FormatLongName', description='The full name of the container format.', example='QuickTime/MOV'),
          formatName?: string(name='FormatName', description='The short name of the container format. Valid values: mov, mp4, m4a, 3gp, 3g2, and mj2.', example='mov'),
          numPrograms?: string(name='NumPrograms', description='The total number of program streams.', example='2'),
          numStreams?: string(name='NumStreams', description='The total number of media streams.', example='2'),
          size?: string(name='Size', description='The size of the file.', example='2647692'),
          startTime?: string(name='StartTime', description='The start time.', example='0.000000'),
        }(name='Format', description='The format information.'),
        streams?: {
          audioStreamList?: {
            audioStream?: [ 
            {
              bitrate?: string(name='Bitrate', description='The bitrate.', example='160.008'),
              channelLayout?: string(name='ChannelLayout', description='The output layout of the sound channels.', example='stereo'),
              channels?: string(name='Channels', description='The number of sound channels.', example='2'),
              codecLongName?: string(name='CodecLongName', description='The full name of the encoding format.', example='AAC(Advanced Audio Coding)'),
              codecName?: string(name='CodecName', description='The short name of the encoding format. Valid values: H264, mov, aac, avc, and mpeg.', example='mov'),
              codecTag?: string(name='CodecTag', description='The tag of the encoding format.', example='0x6134706d'),
              codecTagString?: string(name='CodecTagString', description='The tag string of the encoding format.', example='mp4a'),
              codecTimeBase?: string(name='CodecTimeBase', description='The codec time base.', example='1/44100'),
              duration?: string(name='Duration', description='The duration.', example='182.591995'),
              index?: string(name='Index', description='The sequence number of the audio stream. The value indicates the position of the audio stream in all audio streams.', example='1'),
              lang?: string(name='Lang', description='The language. For more information, see [FFmpeg documentation](https://www.ffmpeg.org/ffmpeg-all.html#Metadata).', example='und'),
              numFrames?: string(name='NumFrames', description='The total number of frames.', example='100'),
              sampleFmt?: string(name='SampleFmt', description='The sampling format.', example='fltp'),
              samplerate?: string(name='Samplerate', description='The sampling rate.', example='44100'),
              startTime?: string(name='StartTime', description='The start time.', example='0.000000'),
              timebase?: string(name='Timebase', description='The time base.', example='1/44100'),
            }
          ](name='AudioStream')
          }(name='AudioStreamList', description='The list of audio streams.'),
          subtitleStreamList?: {
            subtitleStream?: [ 
            {
              index?: string(name='Index', description='The sequence number of the subtitle stream. The value indicates the position of the subtitle stream in all subtitle streams.', example='3'),
              lang?: string(name='Lang', description='The language. For more information, see [FFmpeg documentation](https://www.ffmpeg.org/ffmpeg-all.html#Metadata).', example='und'),
            }
          ](name='SubtitleStream')
          }(name='SubtitleStreamList', description='The list of subtitle streams.'),
          videoStreamList?: {
            videoStream?: [ 
            {
              avgFPS?: string(name='AvgFPS', description='The average frame rate.', example='29.97003'),
              bitrate?: string(name='Bitrate', description='The bitrate.', example='2659.326'),
              codecLongName?: string(name='CodecLongName', description='The full name of the encoding format.', example='QuickTime/MOV'),
              codecName?: string(name='CodecName', description='The short name of the encoding format. Valid values: mov, mp4, m4a, 3gp, 3g2, and mj2.', example='mov'),
              codecTag?: string(name='CodecTag', description='The tag of the encoding format.', example='0x31637661'),
              codecTagString?: string(name='CodecTagString', description='The tag string of the encoding format.', example='avc1'),
              codecTimeBase?: string(name='CodecTimeBase', description='The codec time base.', example='1001/60000'),
              dar?: string(name='Dar', description='The display aspect ratio (DAR).', example='16:9'),
              duration?: string(name='Duration', description='The duration.', example='182.683000'),
              fps?: string(name='Fps', description='The frame rate.', example='29.97003'),
              hasBFrames?: string(name='HasBFrames', description='Indicates whether the video stream contains bidirectional frames (B-frames). A value of **1** indicates that the video stream contains B-frames. A value of **2** indicates that the video stream does not contain B-frames.', example='2'),
              height?: string(name='Height', description='The latter number in the video resolution. The number indicates the video height.', example='1080'),
              index?: string(name='Index', description='The sequence number of the video stream. The value indicates the position of the video stream in all video streams.', example='5'),
              lang?: string(name='Lang', description='The language. For more information, see [FFmpeg documentation](https://www.ffmpeg.org/ffmpeg-all.html#Metadata).', example='und'),
              level?: string(name='Level', description='The codec level.', example='40'),
              networkCost?: {
                avgBitrate?: string(name='AvgBitrate', description='The average bitrate.', example='2659.326'),
                costBandwidth?: string(name='CostBandwidth', description='The maximum bandwidth that was consumed.', example='100'),
                preloadTime?: string(name='PreloadTime', description='The amount of preload time.', example='0.01'),
              }(name='NetworkCost', description='The network bandwidth consumption.'),
              numFrames?: string(name='NumFrames', description='The total number of frames.', example='12'),
              pixFmt?: string(name='PixFmt', description='The pixel format.', example='yuv420p'),
              profile?: string(name='Profile', description='The codec profile.', example='High'),
              rotate?: string(name='Rotate', description='The video rotation angle.', example='90'),
              sar?: string(name='Sar', description='The sample aspect ratio (SAR).', example='1:1'),
              startTime?: string(name='StartTime', description='The start time.', example='0.000000'),
              timebase?: string(name='Timebase', description='The time base.', example='1/30000'),
              width?: string(name='Width', description='The former number in the video resolution. The number indicates the video width.', example='100'),
            }
          ](name='VideoStream')
          }(name='VideoStreamList', description='The list of video streams.'),
        }(name='Streams', description='The stream information.'),
      }(name='MediaInfo', description='The media information.'),
      playList?: {
        play?: [ 
        {
          activityName?: string(name='ActivityName', description='The name of the workflow activity.', example='example-activity-****'),
          bitrate?: string(name='Bitrate', description='The bitrate of the media file.', example='2659.326'),
          duration?: string(name='Duration', description='The duration of the media file.', example='7.965000'),
          encryption?: string(name='Encryption', description='Indicates whether the media file is encrypted. Valid values:

*   **0**: The media file is not encrypted.
*   **1**: The media file is encrypted.', example='0'),
          file?: {
            state?: string(name='State', description='The status of the file. Valid values:

*   **Normal**: normal
*   **Deleted**: deleted', example='Normal'),
            URL?: string(name='URL', description='The Object Storage Service (OSS) URL of the output file.', example='http://example-bucket-****.oss-cn-hangzhou.aliyuncs.com//example-****.mp4'),
          }(name='File', description='The playback file.'),
          format?: string(name='Format', description='The encoding format of the media file. Valid values: mov, mp4, m4a, 3gp, 3g2, and mj2.', example='mp4'),
          fps?: string(name='Fps', description='The frame rate of the media file.', example='25.0'),
          height?: string(name='Height', description='The height.', example='1080'),
          mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the workflow that generates the playback file.', example='93ab850b4f6f44eab54b6e91d24d****'),
          mediaWorkflowName?: string(name='MediaWorkflowName', description='The name of the workflow that generates the playback file.', example='example-mediaworkflow-****'),
          size?: string(name='Size', description='The size of the media file.', example='2647692'),
          width?: string(name='Width', description='The width of the media file.', example='760'),
        }
      ](name='Play')
      }(name='PlayList', description='The playlist.'),
      publishState?: string(name='PublishState', description='The publishing status of the media file. Valid values:

- **Initiated**: The media file is in the initial state.
- **UnPublish**: The media file has not been published, and the playback permission on the OSS object is Private.
- **Published**: The media file has been published, and the playback permission on the OSS object is Default.
- **Deleted**: The media file has been deleted.', example='Published'),
      runIdList?: {
        runId?: [ string ](name='RunId')
      }(name='RunIdList', description='The ID of the instance.'),
      size?: string(name='Size', description='The size of the file.', example='2647692'),
      snapshotList?: {
        snapshot?: [ 
        {
          activityName?: string(name='ActivityName', description='The name of the workflow activity that generates the snapshot.', example='example-activity1-****'),
          count?: string(name='Count', description='The number of snapshots. This parameter is valid only when the value of the **Type** parameter is **Sequence**.', example='5'),
          file?: {
            state?: string(name='State', description='The status of the file. Valid values:

- **Normal**: normal
- **Deleted**: deleted', example='Normal'),
            URL?: string(name='URL', description='The OSS URL of the snapshot.', example='http://example1-bucket1-****.oss-cn-hangzhou.aliyuncs.com//example111-****.png'),
          }(name='File', description='The snapshot.'),
          mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the workflow that generates the snapshot.', example='6cc3aa66d1cb4bb2adf14e726c0a****'),
          mediaWorkflowName?: string(name='MediaWorkflowName', description='The name of the workflow that generates the snapshot.', example='example-workflow-****'),
          type?: string(name='Type', description='The type of the snapshot. Valid values:

- **Single**
- **Sequence**', example='Sequence'),
        }
      ](name='Snapshot')
      }(name='SnapshotList', description='The list of snapshots.'),
      summaryList?: {
        summary?: [ 
        {
          activityName?: string(name='ActivityName', description='The name of the workflow activity.', example='example-activity-****'),
          file?: {
            state?: string(name='State', description='The status of the file. Valid values:

*   **Normal**: normal
*   **Deleted**: deleted', example='Normal'),
            URL?: string(name='URL', description='The OSS URL of the input file.', example='http://example-bucket-****.oss-cn-hangzhou.aliyuncs.com//example-****.mp4'),
          }(name='File', description='The information about the input file.'),
          mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the workflow that generates the summary.', example='93ab850b4f6f44eab54b6e91d24d****'),
          mediaWorkflowName?: string(name='MediaWorkflowName', description='The name of the workflow that generates the summary.', example='example-mediaworkflow-****'),
          type?: string(name='Type', description='The type of the summary. Valid values:

*   **Video**: video
*   **Gif**: dynamic image', example='Video'),
        }
      ](name='Summary')
      }(name='SummaryList', description='The list of video summaries.'),
      tags?: {
        tag?: [ string ](name='Tag')
      }(name='Tags', description='The tags of the media file.'),
      title?: string(name='Title', description='The title.', example='example-title-****'),
      width?: string(name='Width', description='The width.', example='1920'),
    }
  ](name='Media')
  }(name='MediaList', description='The list of media files.'),
  nonExistMediaIds?: {
    mediaId?: [ string ](name='MediaId')
  }(name='NonExistMediaIds', description='The IDs of the media files that do not exist. This parameter is not returned when all specified media files exist.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='283DC68C-146F-4489-A2A1-2F88F1472A56'),
}

model QueryMediaListResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: QueryMediaListResponseBody(name='body'),
}

/**
  * You can call this operation to query up to 10 media files at a time.
  * ## QPS limit
  * You can call this operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request QueryMediaListRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return QueryMediaListResponse
 */
async function queryMediaListWithOptions(request: QueryMediaListRequest, runtime: Util.RuntimeOptions): QueryMediaListResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.includeMediaInfo)) {
    query['IncludeMediaInfo'] = request.includeMediaInfo;
  }
  if (!Util.isUnset(request.includePlayList)) {
    query['IncludePlayList'] = request.includePlayList;
  }
  if (!Util.isUnset(request.includeSnapshotList)) {
    query['IncludeSnapshotList'] = request.includeSnapshotList;
  }
  if (!Util.isUnset(request.includeSummaryList)) {
    query['IncludeSummaryList'] = request.includeSummaryList;
  }
  if (!Util.isUnset(request.mediaIds)) {
    query['MediaIds'] = request.mediaIds;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'QueryMediaList',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * You can call this operation to query up to 10 media files at a time.
  * ## QPS limit
  * You can call this operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request QueryMediaListRequest
  * @return QueryMediaListResponse
 */
async function queryMediaList(request: QueryMediaListRequest): QueryMediaListResponse {
  var runtime = new Util.RuntimeOptions{};
  return queryMediaListWithOptions(request, runtime);
}

model QueryMediaListByURLRequest {
  fileURLs?: string(name='FileURLs', description='The OSS URLs of the media files. To obtain the OSS URL of a media file, you can perform the following operations in the ApsaraVideo Media Processing (MPS) console: In the left-side navigation pane, choose **Media Management** > **Media List**. Find the media file whose OSS URL you want to view and click **Manage** in the **Actions** column. The OSS URL of the media file is displayed on the **Obtain Encoding URL** tab. Separate multiple URLs with commas (,). You can query up to 10 media files at a time.

*   The URL complies with RFC 3986 and is encoded in UTF-8, with reserved characters being percent-encoded. The value can be up to 3,200 bytes in size. For more information, see [URL encoding](~~423796~~).
*   Only OSS HTTP URLs are supported. Alibaba Cloud CDN URLs and HTTPS URLs are not supported.', example='http://example-bucket-****.oss-cn-shanghai.aliyuncs.com/example.mp4'),
  includeMediaInfo?: boolean(name='IncludeMediaInfo', description='Specifies whether to include media information in the returned result.

*   Valid values: true and false.

*   Default value: **false**.

> To obtain detailed information about the media files, set this parameter to true.', example='true'),
  includePlayList?: boolean(name='IncludePlayList', description='Specifies whether to include playback information in the returned result.

*   Valid values: true and false.
*   Default value: **false**.', example='true'),
  includeSnapshotList?: boolean(name='IncludeSnapshotList', description='Specifies whether to include snapshot information in the returned result.

*   Valid values: true and false.
*   Default value: **false**.', example='true'),
  includeSummaryList?: boolean(name='IncludeSummaryList', description='Specifies whether to include summaries in the returned result.

*   Valid values: true and false.
*   Default value: **false**.', example='true'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model QueryMediaListByURLResponseBody = {
  mediaList?: {
    media?: [ 
    {
      bitrate?: string(name='Bitrate', description='The bitrate.', example='593.192'),
      cateId?: long(name='CateId', description='The ID of the category to which the media file belongs.', example='123'),
      censorState?: string(name='CensorState', description='The review status of the media file. Valid values:

*   **Initiated**: The media file is uploaded but not reviewed.
*   **Pass**: The media file is uploaded and passes the review.', example='Initiated'),
      coverURL?: string(name='CoverURL', description='The OSS URL of the thumbnail.', example='http://example-bucket1-****.oss-cn-hangzhou.aliyuncs.com//example-****.png'),
      creationTime?: string(name='CreationTime', description='The time when the media file was created.', example='2021-07-14T13:05:00Z'),
      description?: string(name='Description', description='The description.', example='This is description ****'),
      duration?: string(name='Duration', description='The duration.', example='79.204000'),
      file?: {
        state?: string(name='State', description='The status of the media file. Valid values:

*   **Normal**: The file is normal.
*   **Deleted**: The file is deleted.', example='Normal'),
        URL?: string(name='URL', description='The OSS URL of the input file.', example='http://example-bucket-****.oss-cn-hangzhou.aliyuncs.com//example-****.mp4'),
      }(name='File', description='The details of the input file.'),
      format?: string(name='Format', description='The encoding format. Valid values: mov, mp4, m4a, 3gp, 3g2, and mj2.', example='mov'),
      fps?: string(name='Fps', description='The frame rate.', example='15.0'),
      height?: string(name='Height', description='The height of the queried media file.', example='360'),
      mediaId?: string(name='MediaId', description='The ID of the media file.', example='52d7e98b05e648199612290bb819****'),
      mediaInfo?: {
        format?: {
          bitrate?: string(name='Bitrate', description='The bitrate.', example='593.192'),
          duration?: string(name='Duration', description='The duration.', example='79.204000'),
          formatLongName?: string(name='FormatLongName', description='The full name of the encoding format.', example='QuickTime/MOV'),
          formatName?: string(name='FormatName', description='The short name of the container format. Valid values: mov, mp4, m4a, 3gp, 3g2, and mj2.', example='mov'),
          numPrograms?: string(name='NumPrograms', description='The total number of program streams.', example='0'),
          numStreams?: string(name='NumStreams', description='The total number of media streams.', example='2'),
          size?: string(name='Size', description='The size.', example='5872904'),
          startTime?: string(name='StartTime', description='The start time.', example='0.000000'),
        }(name='Format', description='The format information.'),
        streams?: {
          audioStreamList?: {
            audioStream?: [ 
            {
              bitrate?: string(name='Bitrate', description='The bitrate.', example='76.356'),
              channelLayout?: string(name='ChannelLayout', description='The output layout of the sound channels.', example='stereo'),
              channels?: string(name='Channels', description='The number of sound channels.', example='2'),
              codecLongName?: string(name='CodecLongName', description='The full name of the encoding format.', example='AAC (Advanced Audio Coding)'),
              codecName?: string(name='CodecName', description='The short name of the encoding format. Valid values: H264, mov, aac, avc, and mpeg.', example='aac'),
              codecTag?: string(name='CodecTag', description='The tag of the encoding format.', example='0x6134706d'),
              codecTagString?: string(name='CodecTagString', description='The tag string of the encoding format.', example='mp4a'),
              codecTimeBase?: string(name='CodecTimeBase', description='The codec time base.', example='1/44100'),
              duration?: string(name='Duration', description='The duration of the media file.', example='79.203265'),
              index?: string(name='Index', description='The sequence number of the audio stream. The value indicates the position of the audio stream in all audio streams.', example='1'),
              lang?: string(name='Lang', description='The language. For more information, see [FFmpeg language definition](https://www.ffmpeg.org/ffmpeg-all.html#Metadata).', example='und'),
              numFrames?: string(name='NumFrames', description='The total number of frames.', example='100'),
              sampleFmt?: string(name='SampleFmt', description='The sampling format.', example='fltp'),
              samplerate?: string(name='Samplerate', description='The sampling rate.', example='44100'),
              startTime?: string(name='StartTime', description='The start time.', example='0.000000'),
              timebase?: string(name='Timebase', description='The time base.', example='1/44100'),
            }
          ](name='AudioStream')
          }(name='AudioStreamList', description='The list of audio streams.'),
          subtitleStreamList?: {
            subtitleStream?: [ 
            {
              index?: string(name='Index', description='The sequence number of the subtitle stream. The value indicates the position of the subtitle stream in all subtitle streams.', example='1'),
              lang?: string(name='Lang', description='The language. For more information, see [FFmpeg language definition](https://www.ffmpeg.org/ffmpeg-all.html#Metadata).', example='und'),
            }
          ](name='SubtitleStream')
          }(name='SubtitleStreamList', description='The list of subtitle streams.'),
          videoStreamList?: {
            videoStream?: [ 
            {
              avgFPS?: string(name='AvgFPS', description='The average frame rate.', example='15.0'),
              bitrate?: string(name='Bitrate', description='The bitrate.', example='512.701'),
              codecLongName?: string(name='CodecLongName', description='The full name of the encoding format.', example='H.264/AVC/MPEG-4 AVC/MPEG-4 part 10'),
              codecName?: string(name='CodecName', description='The short name of the encoding format. Valid values: H264, mov, aac, avc, and mpeg.', example='H264'),
              codecTag?: string(name='CodecTag', description='The tag of the encoding format.', example='0x31637661'),
              codecTagString?: string(name='CodecTagString', description='The tag string of the encoding format.', example='avc1'),
              codecTimeBase?: string(name='CodecTimeBase', description='The codec time base.', example='1/30'),
              dar?: string(name='Dar', description='The display aspect ratio (DAR) of the video stream.', example='16:9'),
              duration?: string(name='Duration', description='The duration.', example='79.200000'),
              fps?: string(name='Fps', description='The frame rate.', example='15.0'),
              hasBFrames?: string(name='HasBFrames', description='Indicates whether the video stream contains bidirectional frames (B-frames). A value of **1** indicates that the video stream contains B-frames. A value of **2** indicates that the video stream does not contain B-frames.', example='2'),
              height?: string(name='Height', description='The latter number in the video resolution. The number indicates the video height.', example='360'),
              index?: string(name='Index', description='The sequence number of the video stream. The value indicates the position of the video stream in all video streams.', example='5'),
              lang?: string(name='Lang', description='The language. For more information, see [FFmpeg documentation](https://www.ffmpeg.org/ffmpeg-all.html#Metadata).', example='und'),
              level?: string(name='Level', description='The codec level.', example='31'),
              networkCost?: {
                avgBitrate?: string(name='AvgBitrate', description='The average bitrate of the video stream.', example='2659.326'),
                costBandwidth?: string(name='CostBandwidth', description='The maximum bandwidth that was consumed.', example='100'),
                preloadTime?: string(name='PreloadTime', description='The amount of preload time.', example='0.01'),
              }(name='NetworkCost', description='The network bandwidth consumption.'),
              numFrames?: string(name='NumFrames', description='The total number of frames.', example='12'),
              pixFmt?: string(name='PixFmt', description='The pixel format of the video stream.', example='yuv420p'),
              profile?: string(name='Profile', description='The codec profile.', example='High'),
              rotate?: string(name='Rotate', description='The rotation angle of the video.', example='90'),
              sar?: string(name='Sar', description='The sample aspect ratio (SAR).', example='1:1'),
              startTime?: string(name='StartTime', description='The start time.', example='0.046029'),
              timebase?: string(name='Timebase', description='The time base.', example='1/15360'),
              width?: string(name='Width', description='The former number in the video resolution. The number indicates the video width and cannot be negative.', example='640'),
            }
          ](name='VideoStream')
          }(name='VideoStreamList', description='The list of video streams.'),
        }(name='Streams', description='The stream information.'),
      }(name='MediaInfo', description='The media information.'),
      playList?: {
        play?: [ 
        {
          activityName?: string(name='ActivityName', description='The name of the workflow activity.', example='test name'),
          bitrate?: string(name='Bitrate', description='The bitrate of the media file.', example='25.067'),
          duration?: string(name='Duration', description='The duration.', example='7.965000'),
          encryption?: string(name='Encryption', description='Indicates whether the media file is encrypted. Valid values:

*   **0**: The media file is not encrypted.
*   **1**: The media file is encrypted.', example='1'),
          file?: {
            state?: string(name='State', description='The status of the media file. Valid values:

*   **Normal**: The file is normal.
*   **Deleted**: The file is deleted.', example='Normal'),
            URL?: string(name='URL', description='The OSS URL of the playback file.', example='http://example-bucket-****.oss-cn-hangzhou.aliyuncs.com//example-****.mp4l-test/in/1.mp4'),
          }(name='File', description='The playback file.'),
          format?: string(name='Format', description='The encoding format of the media file. Valid values: mov, mp4, m4a, 3gp, 3g2, and mj2.', example='mov'),
          fps?: string(name='Fps', description='The frame rate.', example='25.0'),
          height?: string(name='Height', description='The height of the media file.', example='10'),
          mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the workflow that generates the playback file.', example='6cc3aa66d1cb4bb2adf14e726c0a****'),
          mediaWorkflowName?: string(name='MediaWorkflowName', description='The name of the workflow that generates the playback file.', example='example-mediaworkflow-****'),
          size?: string(name='Size', description='The size.', example='100'),
          width?: string(name='Width', description='The width.', example='11'),
        }
      ](name='Play')
      }(name='PlayList', description='The playlist.'),
      publishState?: string(name='PublishState', description='The publishing status of the media file. Valid values:

- **Initiated**: The media file is in the initial state.
- **UnPublish**: The media file has not been published, and the playback permission on the OSS object is Private.
- **Published**: The media file has been published, and the playback permission on the OSS object is Default.
- **Deleted**: The file is deleted.', example='Published'),
      runIdList?: {
        runId?: [ string ](name='RunId')
      }(name='RunIdList', description='The IDs of the media workflow execution instances.'),
      size?: string(name='Size', description='The size of the file.', example='5872904'),
      snapshotList?: {
        snapshot?: [ 
        {
          activityName?: string(name='ActivityName', description='The name of the workflow activity that generates the snapshot.', example='example-activity1-****'),
          count?: string(name='Count', description='The number of snapshots. This parameter is valid only when the value of the **Type** parameter is **Sequence**.', example='3'),
          file?: {
            state?: string(name='State', description='The status of the file. Valid values:

- **Normal**: The file is normal.
- **Deleted**: The file is deleted.', example='Normal'),
            URL?: string(name='URL', description='The OSS URL of the snapshot.', example='http://example1-bucket1-****.oss-cn-hangzhou.aliyuncs.com//example111-****.png'),
          }(name='File', description='The snapshot.'),
          mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the workflow that generates the snapshot.', example='6cc3aa66d1cb4bb2adf14e726c0a****'),
          mediaWorkflowName?: string(name='MediaWorkflowName', description='The name of the workflow that generates the snapshot.', example='example-workflow-****'),
          type?: string(name='Type', description='The type of the snapshot. Valid values:

- **Single**: a single snapshot
- **Sequence**: snapshots in sequence', example='Single'),
        }
      ](name='Snapshot')
      }(name='SnapshotList', description='The list of snapshots.'),
      summaryList?: {
        summary?: [ 
        {
          activityName?: string(name='ActivityName', description='The name of the workflow activity.', example='example-activity-****'),
          file?: {
            state?: string(name='State', description='The status of the media file. Valid values:

*   **Normal**: The file is normal.
*   **Deleted**: The file is deleted.', example='Normal'),
            URL?: string(name='URL', description='The OSS URL of the input file.', example='http://example-bucket-****.o'),
          }(name='File', description='The information about the input file.'),
          mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the workflow that generates the summary.', example='93ab850b4f6f44eab54b6e91d24d****'),
          mediaWorkflowName?: string(name='MediaWorkflowName', description='The name of the workflow that generates the summary.', example='example-mediaworkflow-****'),
          type?: string(name='Type', description='The type of the summary. Valid values:

*   **Video**: video
*   **Gif**: dynamic image', example='Gif'),
        }
      ](name='Summary')
      }(name='SummaryList', description='The list of video summaries.'),
      tags?: {
        tag?: [ string ](name='Tag')
      }(name='Tags', description='The tags of the media file.'),
      title?: string(name='Title', description='The title.', example='kled.mp4'),
      width?: string(name='Width', description='The width.', example='640'),
    }
  ](name='Media')
  }(name='MediaList', description='The list of media files.'),
  nonExistFileURLs?: {
    fileURL?: [ string ](name='FileURL')
  }(name='NonExistFileURLs', description='The IDs of the media files that do not exist. This parameter is not returned if all specified media files exist.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='1C8A0AEB-4321-485B-B4CB-DA4E9E6C9B42'),
}

model QueryMediaListByURLResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: QueryMediaListByURLResponseBody(name='body'),
}

/**
  * *   You can call this operation to query up to 10 media files at a time.
  * *   Before you call this operation, you must call the [AddMedia](~~44458~~) operation to add media files.
  * *   You can call this operation to query only media files that are processed in a workflow. To obtain comprehensive information about a media file that is newly uploaded to OSS, you can call this operation after the corresponding workflow is complete. To query media files that are not processed in a workflow, you must call the [SubmitMediaInfoJob](~~29220~~) operation to submit a media information analysis job. After the job is complete, you can query the information about the media files.
  * ## QPS limit
  * You can call this API operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request QueryMediaListByURLRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return QueryMediaListByURLResponse
 */
async function queryMediaListByURLWithOptions(request: QueryMediaListByURLRequest, runtime: Util.RuntimeOptions): QueryMediaListByURLResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.fileURLs)) {
    query['FileURLs'] = request.fileURLs;
  }
  if (!Util.isUnset(request.includeMediaInfo)) {
    query['IncludeMediaInfo'] = request.includeMediaInfo;
  }
  if (!Util.isUnset(request.includePlayList)) {
    query['IncludePlayList'] = request.includePlayList;
  }
  if (!Util.isUnset(request.includeSnapshotList)) {
    query['IncludeSnapshotList'] = request.includeSnapshotList;
  }
  if (!Util.isUnset(request.includeSummaryList)) {
    query['IncludeSummaryList'] = request.includeSummaryList;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'QueryMediaListByURL',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * *   You can call this operation to query up to 10 media files at a time.
  * *   Before you call this operation, you must call the [AddMedia](~~44458~~) operation to add media files.
  * *   You can call this operation to query only media files that are processed in a workflow. To obtain comprehensive information about a media file that is newly uploaded to OSS, you can call this operation after the corresponding workflow is complete. To query media files that are not processed in a workflow, you must call the [SubmitMediaInfoJob](~~29220~~) operation to submit a media information analysis job. After the job is complete, you can query the information about the media files.
  * ## QPS limit
  * You can call this API operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request QueryMediaListByURLRequest
  * @return QueryMediaListByURLResponse
 */
async function queryMediaListByURL(request: QueryMediaListByURLRequest): QueryMediaListByURLResponse {
  var runtime = new Util.RuntimeOptions{};
  return queryMediaListByURLWithOptions(request, runtime);
}

model QueryMediaWorkflowExecutionListRequest {
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  runIds?: string(name='RunIds', description='The IDs of the media workflow execution instances. To obtain the instance ID, log on to the **ApsaraVideo Media Processing (MPS) console** and choose **Workflows** > **Execution Instances** in the left-side navigation pane. Separate multiple IDs with commas (,). You can query a maximum of 10 media workflow execution instances at a time.', example='48e33690ac19445488c706924321****'),
}

model QueryMediaWorkflowExecutionListResponseBody = {
  mediaWorkflowExecutionList?: {
    mediaWorkflowExecution?: [ 
    {
      activityList?: {
        activity?: [ 
        {
          code?: string(name='Code', description='The error code returned when the method fails to be called.

*   This parameter is returned only when **Fail** is returned for the State parameter.
*   ****', example='InvalidParameter.ResourceContentBad'),
          endTime?: string(name='EndTime', description='The time when the method ends.', example='2016-04-01T06:53:44Z'),
          jobId?: string(name='JobId', description='The IDs of the jobs that are generated when the methods are called. For example, job IDs for the analysis, transcode, and snapshot methods.', example='2376030d9d0849399cd20e20f4f3****'),
          MNSMessageResult?: {
            errorCode?: string(name='ErrorCode', description='The error code returned when the MNS message fails to be sent. This parameter is not returned if the MNS message is sent.', example='The Topic/Queue config is empty, not send message'),
            errorMessage?: string(name='ErrorMessage', description='The error message returned when the MNS message fails to be sent. This parameter is not returned if the MNS message is sent.', example='MessageConfigEmpty'),
            messageId?: string(name='MessageId', description='The ID of the message that indicates the MNS message is sent. This parameter is not returned if the MNS message fails to be sent.', example='4f3bc83233de4e2f81c7dade443e****'),
          }(name='MNSMessageResult', description='The message sent by Message Service (MNS) to notify the user of the job result.'),
          message?: string(name='Message', description='The error message returned when the method fails to be called.

*   This parameter is returned only when **Fail** is returned for the State parameter.
*   ****', example='The resource operated InputFile is bad'),
          name?: string(name='Name', description='The name of the method. 

>  The name of each method in a media workflow is unique.', example='Start'),
          startTime?: string(name='StartTime', description='The time when the method is called.', example='2016-04-01T06:53:44Z'),
          state?: string(name='State', description='The status of the method. Valid values:

*   Running: The method is being called.
*   Success: The method is called.
*   Fail: The method failed to be called.
*   Skipped: The method is skipped.

>  For example, after the analysis is complete, the transcode method is called and high-definition and standard-definition transcoding jobs are created. The system determines whether to run the jobs based on the analysis result. If the resolution of the input video is low, the high-definition transcoding job may be skipped.', example='Running'),
          type?: string(name='Type', description='The methods that are supported in the media workflow. Valid values: Start, Snapshot, Transcode, Analysis, and Report. For more information, see [Methods supported for media workflows](~~68494~~).', example='Start'),
        }
      ](name='Activity')
      }(name='ActivityList', description='The methods that are called in the media workflow.'),
      creationTime?: string(name='CreationTime', description='The time when the media workflow was created.', example='016-04-01T06:53:43Z'),
      input?: {
        inputFile?: {
          bucket?: string(name='Bucket', description='The name of the OSS bucket in which the input file is stored.', example='example-bucket-****'),
          location?: string(name='Location', description='The ID of the region in which the input file resides.', example='mps-cn-shanghai'),
          object?: string(name='Object', description='The name of the Object Storage Service (OSS) object that is used as the input file.', example='example-mediaWorkflow-****/example-object-****/example.mp4'),
        }(name='InputFile', description='The input file of the media workflow.'),
        userData?: string(name='UserData', description='The custom data.', example='example data ****'),
      }(name='Input', description='The input data of the media workflow.'),
      mediaId?: string(name='MediaId', description='The ID of the media file. A media file contains all the information about a media workflow.', example='512046582a924698a41e0f8b0d2b****'),
      mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the media workflow.', example='93ab850b4f6f44eab54b6e91****81d4'),
      name?: string(name='Name', description='The name of the media workflow.', example='example-mediaworkflow-****'),
      runId?: string(name='RunId', description='The ID of the execution instance.', example='48e33690ac19445488c706924321****'),
      state?: string(name='State', description='The status of the media workflow. Valid values:

*   Running: The media workflow is running.
*   Completed: The media workflow is complete.

>  Completed only indicates that the media workflow is complete. View the status of each method in the workflow, such as the transcode and snapshot methods, to check whether the method is called.

*   Fail: The media workflow fails.', example='Completed'),
    }
  ](name='MediaWorkflowExecution')
  }(name='MediaWorkflowExecutionList', description='The details of the media workflows.'),
  nonExistRunIds?: {
    runId?: [ string ](name='RunId')
  }(name='NonExistRunIds', description='The IDs of the execution instances that do not exist. null is returned if all specified execution instances exist.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='D1D5C080-8E2F-5030-8AB4-13092F17631B'),
}

model QueryMediaWorkflowExecutionListResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: QueryMediaWorkflowExecutionListResponseBody(name='body'),
}

/**
  * *   You can call this operation to query a maximum of 10 media workflow execution instances at a time.
  * *   Before you call this operation, make sure that the workflow pipeline is enabled. Otherwise, the workflow may not run as expected. For example, the following exceptions may occur: the workflow node is invalid and jobs created in the workflow cannot be executed.
  * ## QPS limit
  * You can call this operation up to 100 times per second. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request QueryMediaWorkflowExecutionListRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return QueryMediaWorkflowExecutionListResponse
 */
async function queryMediaWorkflowExecutionListWithOptions(request: QueryMediaWorkflowExecutionListRequest, runtime: Util.RuntimeOptions): QueryMediaWorkflowExecutionListResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.runIds)) {
    query['RunIds'] = request.runIds;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'QueryMediaWorkflowExecutionList',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * *   You can call this operation to query a maximum of 10 media workflow execution instances at a time.
  * *   Before you call this operation, make sure that the workflow pipeline is enabled. Otherwise, the workflow may not run as expected. For example, the following exceptions may occur: the workflow node is invalid and jobs created in the workflow cannot be executed.
  * ## QPS limit
  * You can call this operation up to 100 times per second. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request QueryMediaWorkflowExecutionListRequest
  * @return QueryMediaWorkflowExecutionListResponse
 */
async function queryMediaWorkflowExecutionList(request: QueryMediaWorkflowExecutionListRequest): QueryMediaWorkflowExecutionListResponse {
  var runtime = new Util.RuntimeOptions{};
  return queryMediaWorkflowExecutionListWithOptions(request, runtime);
}

model QueryMediaWorkflowListRequest {
  mediaWorkflowIds?: string(name='MediaWorkflowIds', description='The IDs of the media workflows that you want to query. To obtain the IDs of the media workflows, you can log on to the **ApsaraVideo Media Processing (MPS) console** and choose **Workflows** > **Workflow Settings** in the left-side navigation pane. You can query up to 10 media workflows at a time. Separate multiple IDs of media workflows with commas (,).', example='93ab850b4f6f44eab54b6e9181d4****,72dfa5e679ab4be9a3ed9974c736****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model QueryMediaWorkflowListResponseBody = {
  mediaWorkflowList?: {
    mediaWorkflow?: [ 
    {
      creationTime?: string(name='CreationTime', description='The time when the media workflow was created.', example='2016-04-01T05:29:38Z'),
      mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the media workflow.', example='93ab850b4f6f44eab54b6e9181d4****'),
      name?: string(name='Name', description='The name of the media workflow.', example='example-mediaworkflow-****'),
      state?: string(name='State', description='The status of the media workflow. Valid values:

*   **Inactive**: The media workflow is deactivated.
*   **Active**: The media workflow is activated.
*   **Deleted**: The media workflow is deleted.', example='Active'),
      topology?: string(name='Topology', description='The topology of the media workflow.', example='{mediaworkflow","State":"Active","Topology":"{\\"Activities\\":{\\"Act-Start\\":{\\"Parameters\\":{\\"PipelineId\\":\\"130266f58161436a80bf07cb12c8****\\",\\"InputFile\\":\\"{\\\\\\"Bucket\\\\\\": \\\\\\"example-bucket-****\\\\\\",\\\\\\"Location\\\\\\": \\\\\\"cn-shanghai\\\\\\"}\\"},\\"Type\\":\\"Start\\"},\\"Act-Report\\":{\\"Parameters\\":{},\\"Type\\":\\"Report\\"},\\"Act-Transcode-M3U8\\":{\\"Parameters\\":{\\"Outputs\\":\\"[{\\\\\\"Object\\\\\\":\\\\\\"transcode/{ObjectPrefix}{FileName}\\\\\\",\\\\\\"TemplateId\\\\\\": \\\\\\"957d1719ee85ed6527b90cf62726****\\\\\\"}]\\",\\"OutputBucket\\":\\"example-bucket-****\\",\\"OutputLocation\\":\\"cn-shanghai\\"},\\"Type\\":\\"Transcode\\"}},\\"Dependencies\\":{\\"Act-Start\\":[\\"Act-Transcode-M3U8\\"],\\"Act-Report\\":[],\\"Act-Transcode-M3U8\\":[\\"Act-Report\\"]}}","MediaWorkflowId":"93ab850b4f6f44eab54b6e91d24d****"}]},"RequestId":"16CD0CDD-457E-420D-9755-8385075A1234"}'),
      triggerMode?: string(name='TriggerMode', description='The trigger mode of the media workflow. Valid values:

*   **OssAutoTrigger**: The media workflow is automatically triggered.
*   **NotInAuto**: The media workflow is not automatically triggered.', example='OssAutoTrigger'),
    }
  ](name='MediaWorkflow')
  }(name='MediaWorkflowList', description='The details of the media workflows.'),
  nonExistMediaWorkflowIds?: {
    mediaWorkflowId?: [ string ](name='MediaWorkflowId')
  }(name='NonExistMediaWorkflowIds', description='The media workflow IDs that do not exist. This parameter is not returned if all specified media workflows are found.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='16CD0CDD-457E-420D-1234-8385075A618B'),
}

model QueryMediaWorkflowListResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: QueryMediaWorkflowListResponseBody(name='body'),
}

/**
  * You can call this operation to query up to 10 media workflows at a time.
  * ## Limits on QPS
  * You can call this operation up to 100 times per second. If the number of the calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request QueryMediaWorkflowListRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return QueryMediaWorkflowListResponse
 */
async function queryMediaWorkflowListWithOptions(request: QueryMediaWorkflowListRequest, runtime: Util.RuntimeOptions): QueryMediaWorkflowListResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.mediaWorkflowIds)) {
    query['MediaWorkflowIds'] = request.mediaWorkflowIds;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'QueryMediaWorkflowList',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * You can call this operation to query up to 10 media workflows at a time.
  * ## Limits on QPS
  * You can call this operation up to 100 times per second. If the number of the calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request QueryMediaWorkflowListRequest
  * @return QueryMediaWorkflowListResponse
 */
async function queryMediaWorkflowList(request: QueryMediaWorkflowListRequest): QueryMediaWorkflowListResponse {
  var runtime = new Util.RuntimeOptions{};
  return queryMediaWorkflowListWithOptions(request, runtime);
}

model QueryPipelineListRequest {
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  pipelineIds?: string(name='PipelineIds', description='The IDs of the MPS queues that you want to query. To view the IDs, you can log on to the **MPS console** and choose **Global Settings** > **Pipelines** in the left-side navigation pane. You can query up to 10 MPS queues at a time. Separate multiple IDs of MPS queues with commas (,).', example='d1ce4d3efcb549419193f50f1fcd****,72dfa5e679ab4be9a3ed9974c736****'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model QueryPipelineListResponseBody = {
  nonExistPids?: {
    string?: [ string ](name='String')
  }(name='NonExistPids', description='The MPS queue IDs that do not exist. This parameter is not returned if all specified MPS queues are found.  

>  This parameter is returned if the specified MPS queue IDs that do not exist are valid. Check whether the user ID (UID) that is used to submit the request is the same as the UID to which the queues belong.'),
  pipelineList?: {
    pipeline?: [ 
    {
      extendConfig?: {
        isBoostNew?: boolean(name='IsBoostNew'),
        maxMultiSpeed?: int32(name='MaxMultiSpeed'),
        multiSpeedDowngradePolicy?: string(name='MultiSpeedDowngradePolicy'),
      }(name='ExtendConfig'),
      id?: string(name='Id', description='The ID of the MPS queue.', example='d1ce4d3efcb549419193f50f1fcd****'),
      name?: string(name='Name', description='The name of the MPS queue.', example='example-pipeline-****'),
      notifyConfig?: {
        mqTag?: string(name='MqTag', description='The tags of the messages.', example='mts-test'),
        mqTopic?: string(name='MqTopic', description='The queue of messages that are received.', example='example1,example2'),
        queueName?: string(name='QueueName', description='The name of the queue that is created in MNS.', example='example-queue-****'),
        topic?: string(name='Topic', description='The name of the topic that is created in MNS.', example='example-topic-****'),
      }(name='NotifyConfig', description='The Message Service (MNS) configuration.'),
      quotaAllocate?: long(name='QuotaAllocate', description='The quota that is allocated to the MPS queue.', example='10'),
      role?: string(name='Role', description='The role that is assigned to the current RAM user.', example='AliyunMTSDefaultRole'),
      speed?: string(name='Speed', description='The type of the MPS queue. Default value: **Standard**. Valid values:

*   **Boost**: MPS queue with transcoding speed boosted
*   **Standard**: standard MPS queue
*   **NarrowBandHDV2**: MPS queue that supports Narrowband HD 2.0
*   **AIVideoCover**: MPS queue for intelligent snapshot capture
*   **AIVideoFPShot**: MPS queue for media fingerprinting
*   **AIVideoCensor**: MPS queue for automated review
*   **AIVideoMCU**: MPS queue for smart tagging
*   **AIVideoSummary**: MPS queue for video synopsis
*   **AIVideoPorn**: MPS queue for pornography detection in videos
*   **AIAudioKWS**: MPS queue for keyword recognition in audio
*   **AIAudioASR**: MPS queue for speech-to-text conversion', example='Standard'),
      speedLevel?: long(name='SpeedLevel', description='The level of the MPS queue.', example='2'),
      state?: string(name='State', description='The state of the MPS queue. Valid values:

*   **Active**: The MPS queue is active.
*   **Paused**: The MPS queue is paused.', example='Paused'),
    }
  ](name='Pipeline')
  }(name='PipelineList', description='The details of the MPS queues.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='1C538EAA-ACAF-5AD8-B091-A72C63007149'),
}

model QueryPipelineListResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: QueryPipelineListResponseBody(name='body'),
}

/**
  * *   You can call this operation to query up to 10 MPS queues at a time.
  * *   If `"Code": "InvalidIdentity.ServiceDisabled","Message": "The request identity was not allowed operated.","Recommend"` is returned after you call this operation, check whether the RAM user that you use is assigned the AliyunMTSDefaultRole role to obtain the permissions on MPS and whether your Alibaba Cloud account has overdue payments.
  * ## QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request QueryPipelineListRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return QueryPipelineListResponse
 */
async function queryPipelineListWithOptions(request: QueryPipelineListRequest, runtime: Util.RuntimeOptions): QueryPipelineListResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.pipelineIds)) {
    query['PipelineIds'] = request.pipelineIds;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'QueryPipelineList',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * *   You can call this operation to query up to 10 MPS queues at a time.
  * *   If `"Code": "InvalidIdentity.ServiceDisabled","Message": "The request identity was not allowed operated.","Recommend"` is returned after you call this operation, check whether the RAM user that you use is assigned the AliyunMTSDefaultRole role to obtain the permissions on MPS and whether your Alibaba Cloud account has overdue payments.
  * ## QPS limit
  * You can call this operation up to 100 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request QueryPipelineListRequest
  * @return QueryPipelineListResponse
 */
async function queryPipelineList(request: QueryPipelineListRequest): QueryPipelineListResponse {
  var runtime = new Util.RuntimeOptions{};
  return queryPipelineListWithOptions(request, runtime);
}

model QuerySmarttagJobRequest {
  jobId?: string(name='JobId', example='39f8e0bc005e4f309379701645f4****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  params?: string(name='Params', example='{"labelResultType":"auto"}'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model QuerySmarttagJobResponseBody = {
  jobStatus?: string(name='JobStatus', example='Success'),
  requestId?: string(name='RequestId', example='7B117AF5-2A16-412C-B127-FA6175ED1AD0'),
  results?: {
    result?: [ 
    {
      data?: string(name='Data', example='{"title":"example-title-****"}'),
      type?: string(name='Type', example='Meta'),
    }
  ](name='Result')
  }(name='Results'),
  userData?: string(name='UserData', example='example UserData ****'),
}

model QuerySmarttagJobResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: QuerySmarttagJobResponseBody(name='body'),
}

async function querySmarttagJobWithOptions(request: QuerySmarttagJobRequest, runtime: Util.RuntimeOptions): QuerySmarttagJobResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.jobId)) {
    query['JobId'] = request.jobId;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.params)) {
    query['Params'] = request.params;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'QuerySmarttagJob',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

async function querySmarttagJob(request: QuerySmarttagJobRequest): QuerySmarttagJobResponse {
  var runtime = new Util.RuntimeOptions{};
  return querySmarttagJobWithOptions(request, runtime);
}

model QuerySmarttagTemplateListRequest {
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  templateId?: string(name='TemplateId', example='05de22f255284c7a8d2aab535dde****'),
}

model QuerySmarttagTemplateListResponseBody = {
  requestId?: string(name='RequestId', example='5210DBB0-E327-4D45-ADBC-0B83C8796E26'),
  templates?: {
    template?: [ 
    {
      analyseTypes?: string(name='AnalyseTypes', example='ocr,asr,classification,shows,face,role,object,tvstation,action,emotion,landmark,scene'),
      faceCategoryIds?: string(name='FaceCategoryIds', example='politician,sensitive,celebrity'),
      faceCustomParamsConfig?: string(name='FaceCustomParamsConfig', example='{ "faceDetThreshold":0.999, "faceRegThreshold":0.9 }'),
      industry?: string(name='Industry', example='common'),
      isDefault?: boolean(name='IsDefault', example='false'),
      keywordConfig?: string(name='KeywordConfig', example='{ "type": "name,location,organization,other" }'),
      knowledgeConfig?: string(name='KnowledgeConfig', example='{ "movie":"name,alias,chnl,genre", "music":"songName,artistName", "person":"name,gender" }'),
      labelType?: string(name='LabelType', example='hmi'),
      labelVersion?: string(name='LabelVersion', example='1.0'),
      landmarkGroupIds?: string(name='LandmarkGroupIds', example='common'),
      objectGroupIds?: string(name='ObjectGroupIds', example='general,item,weapon,animal'),
      scene?: string(name='Scene', example='search'),
      templateId?: string(name='TemplateId', example='05de22f255284c7a8d2aab535dde****'),
      templateName?: string(name='TemplateName', example='example-****'),
    }
  ](name='Template')
  }(name='Templates'),
}

model QuerySmarttagTemplateListResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: QuerySmarttagTemplateListResponseBody(name='body'),
}

async function querySmarttagTemplateListWithOptions(request: QuerySmarttagTemplateListRequest, runtime: Util.RuntimeOptions): QuerySmarttagTemplateListResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.templateId)) {
    query['TemplateId'] = request.templateId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'QuerySmarttagTemplateList',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

async function querySmarttagTemplateList(request: QuerySmarttagTemplateListRequest): QuerySmarttagTemplateListResponse {
  var runtime = new Util.RuntimeOptions{};
  return querySmarttagTemplateListWithOptions(request, runtime);
}

model QuerySnapshotJobListRequest {
  endOfJobCreatedTimeRange?: string(name='EndOfJobCreatedTimeRange', description='The end of the time range within which the creation time of snapshot jobs to be queried is.

*   Specify the time in the ISO 8601 standard in the
*   YYYY-MM-DDThh:mm:ssZ format. The time must be in UTC.', example='2014-01-12T12:00:00Z'),
  maximumPageSize?: long(name='MaximumPageSize', description='The maximum number of entries to return on each page.

*   Default value: **30**.
*   Valid values: **\\[1,300]**.', example='30'),
  nextPageToken?: string(name='NextPageToken', description='The token that is used to retrieve the next page of the query results. The value is a 32-bit UUID. If the returned query results cannot be displayed within one page, this parameter is returned. The value of this parameter is updated for each query.', example='cc6cbef8e8d5481ca536f5d2a466****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  pipelineId?: string(name='PipelineId', description='The ID of the MPS queue to which the snapshot jobs that you want to query are submitted. To obtain the ID, you can log on to the **MPS console** and choose **Global Settings** > **Pipelines** in the left-side navigation pane.', example='b11c171cced04565b1f38f1ecc39****'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  snapshotJobIds?: string(name='SnapshotJobIds', description='The IDs of the snapshot jobs that you want to query. To obtain the IDs, you can log on to the **ApsaraVideo Media Processing (MPS) console**, click **Tasks** in the left-side navigation pane, and then view the IDs on the **Snapshot** tab. You can query up to **10** snapshot jobs at a time. Separate the IDs with commas (,).', example='72dfa5e679ab4be9a3ed9974c736****'),
  startOfJobCreatedTimeRange?: string(name='StartOfJobCreatedTimeRange', description='The beginning of the time range within which the creation time of snapshot jobs to be queried is.

*   Specify the time in the ISO 8601 standard in the
*   YYYY-MM-DDThh:mm:ssZ format. The time must be in UTC.', example='2014-01-10T12:00:00Z'),
  state?: string(name='State', description='The status of the snapshot jobs that you want to query.

*   **Submitted**: The job was submitted.
*   **Snapshoting**: The job is being processed.
*   **Success**: The job was successfully processed.
*   **Fail**: The job failed.', example='Snapshoting'),
}

model QuerySnapshotJobListResponseBody = {
  nextPageToken?: string(name='NextPageToken', description='The token that is used to retrieve the next page of the query results. The value is a 32-bit UUID. If the returned query results cannot be displayed within one page, this parameter is returned. The value of this parameter is updated for each query.', example='b11c171cced04565b1f38f1ecc39****'),
  nonExistSnapshotJobIds?: {
    string?: [ string ](name='String')
  }(name='NonExistSnapshotJobIds', description='The snapshot job IDs that do not exist. This parameter is not returned if all specified snapshot jobs are found.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='34BCAB31-2833-43A7-9FBD-B34302AB23EQ'),
  snapshotJobList?: {
    snapshotJob?: [ 
    {
      code?: string(name='Code', description='The error code returned when the job fails. This parameter is not returned if the job is successfully processed.', example='InvalidParameter'),
      count?: string(name='Count', description='The number of snapshots that were taken.', example='1'),
      creationTime?: string(name='CreationTime', description='The time when the job was created.', example='2021-06-30T12:34:29Z'),
      id?: string(name='Id', description='The ID of the snapshot job.', example='cc6cbef8e8d5481ca536f5d2a466****'),
      input?: {
        bucket?: string(name='Bucket', description='The OSS bucket that stores the input file.', example='example'),
        location?: string(name='Location', description='The ID of the region in which the input OSS bucket is located.', example='example-location'),
        object?: string(name='Object', description='The OSS object that is used as the input file.', example='example.flv'),
        roleArn?: string(name='RoleArn', description='The ARN of the specified RAM role. Format: acs:ram::$accountID:role/$roleName.', example='acs:ram::1:role/testrole'),
      }(name='Input', description='The information about the job input.'),
      MNSMessageResult?: {
        errorCode?: string(name='ErrorCode', description='The error code returned when the job fails. This parameter is not returned if the job is successfully processed.', example='InvalidParameter'),
        errorMessage?: string(name='ErrorMessage', description='The error message returned when the job fails. This parameter is not returned if the job is successfully processed.', example='The resource operated InputFile is bad'),
        messageId?: string(name='MessageId', description='The ID of the message. This parameter is not returned if the job fails.', example='799454621135656C7F815F198A76****'),
      }(name='MNSMessageResult', description='The message sent by MNS to notify the user of the job result.'),
      message?: string(name='Message', description='The error message returned when the job fails. This parameter is not returned if the job is successfully processed.', example='The resource operated InputFile is bad'),
      pipelineId?: string(name='PipelineId', description='The ID of the MPS queue to which the snapshot job was submitted.', example='b11c171cced04565b1f38f1ecc39****'),
      snapshotConfig?: {
        frameType?: string(name='FrameType', description='The type of the snapshot. Valid values:

*   **normal**: normal frames.
*   **intra**: I-frames.
*   Default value: **intra**.', example='intra'),
        height?: string(name='Height', description='The height of the output snapshot.', example='8'),
        interval?: string(name='Interval', description='The interval for taking snapshots.

*   If this Interval parameter is specified in the request, snapshots are taken at intervals. The value must be greater than 0.
*   Unit: seconds.
*   Default value: **10**.', example='10'),
        num?: string(name='Num', description='The number of snapshots to take. If the Num parameter is set in the request, snapshots are taken at intervals.', example='10'),
        outputFile?: {
          bucket?: string(name='Bucket', description='The OSS bucket that stores the output file.', example='example'),
          location?: string(name='Location', description='The ID of the region in which the output OSS bucket is located.', example='example-location'),
          object?: string(name='Object', description='The OSS object that is generated as the output file of the snapshot job.', example='example.png'),
          roleArn?: string(name='RoleArn', description='The Alibaba Cloud Resource Name (ARN) of the specified RAM role. Format: acs:ram::$accountID:role/$roleName.', example='acs:ram::1:role/testrole'),
        }(name='OutputFile', description='The Object Storage Service (OSS) output file of the snapshot job.'),
        tileOut?: {
          cellHeight?: string(name='CellHeight', description='The height of a single image. The default value is the height of the output snapshot.', example='8'),
          cellSelStep?: string(name='CellSelStep', description='The stride of a single image.', example='3'),
          cellWidth?: string(name='CellWidth', description='The width of a single image. The default value is the width of the output snapshot.', example='8'),
          color?: string(name='Color', description='The background color.

*   Default value: **black**.
*   You can set the Color parameter to a **color keyword** or **random** in the request.

>  If you want to set the background color to black, you can specify the color keyword in one of the following three formats: Black, black, and #000000.', example='black'),
          columns?: string(name='Columns', description='The number of columns that the tiled image can contain. Default value: **10**.', example='10'),
          isKeepCellPic?: string(name='IsKeepCellPic', description='Indicates whether the single images are retained. Default value: **true**.', example='false'),
          lines?: string(name='Lines', description='The number of rows that the tiled image can contain. Default value: **10**.', example='10'),
          margin?: string(name='Margin', description='The margin width of the tiled image.

*   Default value: **0**.
*   Unit: pixel.', example='0'),
          padding?: string(name='Padding', description='The distance between images.

*   Default value: **0**.
*   Unit: pixel.', example='0'),
        }(name='TileOut', description='The tiling configuration.'),
        tileOutputFile?: {
          bucket?: string(name='Bucket', description='The OSS bucket that stores the output file.', example='example'),
          location?: string(name='Location', description='The ID of the region in which the output OSS bucket is located.', example='example-location'),
          object?: string(name='Object', description='The OSS object that is generated as the output file of the tiling job.', example='example.png'),
          roleArn?: string(name='RoleArn', description='The ARN of the specified RAM role. Format: acs:ram::$accountID:role/$roleName.', example='acs:ram::1:role/testrole'),
        }(name='TileOutputFile', description='The OSS output file of the tiling job.'),
        time?: string(name='Time', description='The start time for taking snapshots. Unit: milliseconds.', example='4'),
        width?: string(name='Width', description='The width of the output snapshot.', example='8'),
      }(name='SnapshotConfig', description='The snapshot configuration.'),
      state?: string(name='State', description='The status of the snapshot job. 

- **Submitted**: The job was submitted.
- **Snapshoting**: The job is being processed.
- **Success**: The job was successfully processed.
- **Fail**: The job failed.', example='Snapshoting'),
      tileCount?: string(name='TileCount', description='The number of snapshots that are contained in the tiled image.', example='7'),
      userData?: string(name='UserData', description='The custom data.', example='testid-001'),
    }
  ](name='SnapshotJob')
  }(name='SnapshotJobList', description='The information about the snapshot jobs.'),
}

model QuerySnapshotJobListResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: QuerySnapshotJobListResponseBody(name='body'),
}

/**
  * You can call this operation to query up to 10 snapshot jobs at a time.
  * ## Limits on QPS
  * You can call this operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request QuerySnapshotJobListRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return QuerySnapshotJobListResponse
 */
async function querySnapshotJobListWithOptions(request: QuerySnapshotJobListRequest, runtime: Util.RuntimeOptions): QuerySnapshotJobListResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.endOfJobCreatedTimeRange)) {
    query['EndOfJobCreatedTimeRange'] = request.endOfJobCreatedTimeRange;
  }
  if (!Util.isUnset(request.maximumPageSize)) {
    query['MaximumPageSize'] = request.maximumPageSize;
  }
  if (!Util.isUnset(request.nextPageToken)) {
    query['NextPageToken'] = request.nextPageToken;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.pipelineId)) {
    query['PipelineId'] = request.pipelineId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.snapshotJobIds)) {
    query['SnapshotJobIds'] = request.snapshotJobIds;
  }
  if (!Util.isUnset(request.startOfJobCreatedTimeRange)) {
    query['StartOfJobCreatedTimeRange'] = request.startOfJobCreatedTimeRange;
  }
  if (!Util.isUnset(request.state)) {
    query['State'] = request.state;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'QuerySnapshotJobList',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * You can call this operation to query up to 10 snapshot jobs at a time.
  * ## Limits on QPS
  * You can call this operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request QuerySnapshotJobListRequest
  * @return QuerySnapshotJobListResponse
 */
async function querySnapshotJobList(request: QuerySnapshotJobListRequest): QuerySnapshotJobListResponse {
  var runtime = new Util.RuntimeOptions{};
  return querySnapshotJobListWithOptions(request, runtime);
}

model QuerySnapshotJobListV2Request {
  endOfJobCreatedTimeRange?: string(name='EndOfJobCreatedTimeRange'),
  maximumPageSize?: string(name='MaximumPageSize'),
  nextPageToken?: string(name='NextPageToken'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  pipelineId?: string(name='PipelineId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  snapshotJobIds?: string(name='SnapshotJobIds'),
  startOfJobCreatedTimeRange?: string(name='StartOfJobCreatedTimeRange'),
  state?: string(name='State'),
}

model QuerySnapshotJobListV2ResponseBody = {
  nextPageToken?: string(name='NextPageToken'),
  nonExistSnapshotJobIds?: [ string ](name='NonExistSnapshotJobIds'),
  requestId?: string(name='RequestId'),
  snapshotJobList?: [ 
    {
      code?: string(name='Code'),
      count?: string(name='Count'),
      creationTime?: string(name='CreationTime'),
      id?: string(name='Id'),
      input?: {
        bucket?: string(name='Bucket'),
        location?: string(name='Location'),
        object?: string(name='Object'),
        roleArn?: string(name='RoleArn'),
      }(name='Input'),
      MNSMessageResult?: {
        errorCode?: string(name='ErrorCode'),
        errorMessage?: string(name='ErrorMessage'),
        messageId?: string(name='MessageId'),
      }(name='MNSMessageResult'),
      message?: string(name='Message'),
      pipelineId?: string(name='PipelineId'),
      snapshotConfig?: {
        frameType?: string(name='FrameType'),
        height?: string(name='Height'),
        interval?: string(name='Interval'),
        num?: string(name='Num'),
        outputFile?: {
          bucket?: string(name='Bucket'),
          location?: string(name='Location'),
          object?: string(name='Object'),
          roleArn?: string(name='RoleArn'),
        }(name='OutputFile'),
        tileOut?: {
          cellHeight?: string(name='CellHeight'),
          cellSelStep?: string(name='CellSelStep'),
          cellWidth?: string(name='CellWidth'),
          color?: string(name='Color'),
          columns?: string(name='Columns'),
          isKeepCellPic?: string(name='IsKeepCellPic'),
          lines?: string(name='Lines'),
          margin?: string(name='Margin'),
          padding?: string(name='Padding'),
        }(name='TileOut'),
        tileOutputFile?: {
          bucket?: string(name='Bucket'),
          location?: string(name='Location'),
          object?: string(name='Object'),
          roleArn?: string(name='RoleArn'),
        }(name='TileOutputFile'),
        time?: string(name='Time'),
        width?: string(name='Width'),
      }(name='SnapshotConfig'),
      state?: string(name='State'),
      tileCount?: string(name='TileCount'),
      userData?: string(name='UserData'),
    }
  ](name='SnapshotJobList'),
}

model QuerySnapshotJobListV2Response = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: QuerySnapshotJobListV2ResponseBody(name='body'),
}

async function querySnapshotJobListV2WithOptions(request: QuerySnapshotJobListV2Request, runtime: Util.RuntimeOptions): QuerySnapshotJobListV2Response {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.endOfJobCreatedTimeRange)) {
    query['EndOfJobCreatedTimeRange'] = request.endOfJobCreatedTimeRange;
  }
  if (!Util.isUnset(request.maximumPageSize)) {
    query['MaximumPageSize'] = request.maximumPageSize;
  }
  if (!Util.isUnset(request.nextPageToken)) {
    query['NextPageToken'] = request.nextPageToken;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.pipelineId)) {
    query['PipelineId'] = request.pipelineId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.snapshotJobIds)) {
    query['SnapshotJobIds'] = request.snapshotJobIds;
  }
  if (!Util.isUnset(request.startOfJobCreatedTimeRange)) {
    query['StartOfJobCreatedTimeRange'] = request.startOfJobCreatedTimeRange;
  }
  if (!Util.isUnset(request.state)) {
    query['State'] = request.state;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'QuerySnapshotJobListV2',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

async function querySnapshotJobListV2(request: QuerySnapshotJobListV2Request): QuerySnapshotJobListV2Response {
  var runtime = new Util.RuntimeOptions{};
  return querySnapshotJobListV2WithOptions(request, runtime);
}

model QueryTemplateListRequest {
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  templateIds?: string(name='TemplateIds', description='The IDs of the custom transcoding templates that you want to query. You can query up to 10 custom transcoding templates at a time. Separate multiple IDs of custom transcoding templates with commas (,).', example='16f01ad6175e4230ac42bb5182cd****,88c6ca184c0e424d5w5b665e2a12****'),
}

model QueryTemplateListResponseBody = {
  nonExistTids?: {
    string?: [ string ](name='String')
  }(name='NonExistTids', description='The template IDs that do not exist. This parameter is not returned if all specified custom transcoding templates are found.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='BC860F04-778A-472F-AB39-E1BF329C1EA8'),
  templateList?: {
    template?: [ 
    {
      audio?: {
        bitrate?: string(name='Bitrate', description='The audio bitrate of the output file.

*   Valid values: **\\[8, 1000]**.
*   Unit: Kbit/s.
*   Default value: **128**.', example='500'),
        channels?: string(name='Channels', description='The number of sound channels. Default value: **2**.', example='2'),
        codec?: string(name='Codec', description='The audio codec. Default value: **aac**. Valid values:

*   **aac**
*   **mp3**
*   **vorbis**
*   **flac**', example='aac'),
        profile?: string(name='Profile', description='The codec profile of the audio. Valid values when the Codec parameter is set to aac:

*   **aac_low**
*   **aac_he**
*   **aac_he_v2**
*   **aac_ld**
*   **aac_eld**', example='aac_low'),
        qscale?: string(name='Qscale', description='The strength of the independent noise reduction algorithm.', example='1'),
        remove?: string(name='Remove', description='Indicates whether to delete the audio stream.

*   **true**: The audio stream is deleted.
*   **false**: The audio stream is retained.
*   Default value: **false**.', example='false'),
        samplerate?: string(name='Samplerate', description='The sampling rate.

*   Unit: Hz.
*   Default value: **44100**.', example='44100'),
      }(name='Audio', description='The audio codec configuration.'),
      container?: {
        format?: string(name='Format', description='The container format. Valid values: flv, mp4, ts, m3u8, gif, mp3, ogg, and flac.', example='flv'),
      }(name='Container', description='The container configuration.'),
      id?: string(name='Id', description='The ID of the transcoding template.', example='16f01ad6175e4230ac42bb5182cd****'),
      muxConfig?: {
        gif?: {
          ditherMode?: string(name='DitherMode', description='The color dithering algorithm of the palette. Valid values: sierra and bayer.', example='sierra'),
          finalDelay?: string(name='FinalDelay', description='The duration for which the final frame is paused.', example='0'),
          isCustomPalette?: string(name='IsCustomPalette', description='Indicates whether the custom palette is used.', example='false'),
          loop?: string(name='Loop', description='The loop count.', example='0'),
        }(name='Gif', description='The transmuxing configuration for GIF.'),
        segment?: {
          duration?: string(name='Duration', description='The length of the segment. Unit: seconds.', example='10'),
        }(name='Segment', description='The segment configuration.'),
        webp?: {
          loop?: string(name='Loop', description='The loop count.', example='0'),
        }(name='Webp', description='The transmuxing configuration for WebP.'),
      }(name='MuxConfig', description='The transmuxing configuration.'),
      name?: string(name='Name', description='The name of the template.', example='MPS-example'),
      state?: string(name='State', description='The status of the template.

*   **Normal**: The template is normal.
*   **Deleted**: The template is deleted.', example='Normal'),
      transConfig?: {
        adjDarMethod?: string(name='AdjDarMethod', description='The method of resolution adjustment. Default value: **none**. Valid values:

*   rescale: The input video is rescaled.
*   crop: The input video is cropped.
*   none: No change is made.', example='none'),
        isCheckAudioBitrate?: string(name='IsCheckAudioBitrate', description='Indicates whether to check the audio bitrate.

*   If this feature is enabled and the system detects that the audio bitrate of the output file is greater than that of the input file, the audio bitrate of the input file is retained after transcoding.
*   **true**: The audio bitrate is checked.
*   **false**: The audio bitrate is not checked.
*   Default value: **false**.', example='false'),
        isCheckAudioBitrateFail?: string(name='IsCheckAudioBitrateFail', description='Indicates whether to allow audio bitrate check errors.

*   **true**: If the audio bitrate check fails, the input file is not transcoded.
*   **false**: The audio bitrate is not checked.
*   Default value: **false**.
*   This parameter takes precedence over the IsCheckAudioBitrate parameter.', example='false'),
        isCheckReso?: string(name='IsCheckReso', description='Indicates whether to check the resolution.

*   If this feature is enabled and the system detects that the resolution of the output file is higher than that of the input file based on the width or height, the resolution of the input file is retained after transcoding.
*   **true**: The resolution is checked.
*   **false**: The resolution is not checked.
*   Default value: **false**.', example='false'),
        isCheckResoFail?: string(name='IsCheckResoFail', description='Indicates whether to check the resolution.

*   If this feature is enabled and the system detects that the resolution of the output file is higher than that of the input file based on the width or height, an error that indicates a transcoding failure is returned.
*   **true**: The resolution is checked.
*   **false**: The resolution is not checked.
*   Default value: **false**.', example='false'),
        isCheckVideoBitrate?: string(name='IsCheckVideoBitrate', description='Indicates whether to check the video bitrate.

*   If this feature is enabled and the system detects that the video bitrate of the output file is greater than that of the input file, the video bitrate of the input file is retained after transcoding.
*   **true**: The video bitrate is checked.
*   **false**: The video bitrate is not checked.
*   Default value: **false**.', example='false'),
        isCheckVideoBitrateFail?: string(name='IsCheckVideoBitrateFail', description='Indicates whether to allow video bitrate check errors.

*   **true**: If the video bitrate check fails, the input file is not transcoded.
*   **false**: The video bitrate is not checked.
*   Default value: **false**.
*   This parameter takes precedence over the IsCheckVideoBitrate parameter.', example='false'),
        transMode?: string(name='TransMode', description='The transcoding mode. Default value: **onepass**. Valid values:

*   **onepass**
*   **twopass**
*   **CBR**', example='onepass'),
      }(name='TransConfig', description='The general transcoding configuration.'),
      video?: {
        bitrate?: string(name='Bitrate', description='The average bitrate of the video. Unit: Kbit/s.', example='200'),
        bitrateBnd?: {
          max?: string(name='Max', description='The upper limit of the total bitrate. Unit: Kbit/s.', example='100'),
          min?: string(name='Min', description='The lower limit of the total bitrate. Unit: Kbit/s.', example='500'),
        }(name='BitrateBnd', description='The average bitrate range of the video.'),
        bufsize?: string(name='Bufsize', description='The size of the buffer.

*   Unit: KB.
*   Default value: **6000**.', example='6000'),
        codec?: string(name='Codec', description='The video codec. Default value: **H.264**.', example='H.264'),
        crf?: string(name='Crf', description='The constant rate factor.

*   Default value when the Codec parameter is set to H.264: **23**. Default value when the Codec parameter is set to H.265: **26**.
*   If this parameter is specified, the setting of the Bitrate parameter is invalid.', example='15'),
        crop?: string(name='Crop', description='The method of video cropping. Valid values:

*   **border**: automatically detects and removes borders.
*   **Other values**: the custom cropping modes. Format: width:height:left:top. Example: 1280:800:0:140.', example='border'),
        degrain?: string(name='Degrain', description='The level of video quality control.', example='10'),
        fps?: string(name='Fps', description='The frame rate of the video.

*   The value is 60 if the frame rate of the input video exceeds 60.
*   Default value: the frame rate of the input video.', example='25'),
        gop?: string(name='Gop', description='The maximum number of frames between two keyframes. Default value: **250**.', example='10'),
        height?: string(name='Height', description='The height of the video.

*   Unit: pixel.
*   Default value: the height of the input video.', example='800'),
        longShortMode?: string(name='LongShortMode', description='Indicates whether to enable the auto-rotate screen feature.

*   If this feature is enabled, the width of the output video corresponds to the long side of the input video, which is the height of the input video in portrait mode. The height of the output video corresponds to the short side of the input video, which is the width of the input video in portrait mode.
*   **true**: The auto-rotate screen feature is enabled.
*   **false**: The auto-rotate screen feature is disabled.
*   Default value: **false**.', example='false'),
        maxFps?: string(name='MaxFps', description='The maximum frame rate.', example='60'),
        maxrate?: string(name='Maxrate', description='The maximum video bitrate. Unit: Kbit/s.', example='500'),
        pad?: string(name='Pad', description='The black borders to be added to the video. Format: width:height:left:top. Example: 1280:800:0:140.', example='1280:800:0:140'),
        pixFmt?: string(name='PixFmt', description='The pixel format for video color encoding. Valid values: standard pixel formats such as yuv420p and yuvj420p.', example='yuv420p'),
        preset?: string(name='Preset', description='The preset video algorithm. Valid values: veryfast, fast, medium, slow, and slower. Default value: **medium**.', example='medium'),
        profile?: string(name='Profile', description='The codec profile.

*   **baseline**: applicable to mobile devices.
*   **main**: applicable to standard-definition devices.
*   **high**: applicable to high-definition devices.
*   Default value: **high**.', example='high'),
        qscale?: string(name='Qscale', description='The strength of the independent noise reduction algorithm.', example='1'),
        remove?: string(name='Remove', description='Indicates whether to delete the video stream.

*   **true**: The video stream is deleted.
*   **false**: The video stream is retained.
*   Default value: **false**.', example='false'),
        resoPriority?: string(name='ResoPriority', description='The policy of resolution adjustment. Valid values: CropFirst, widthFirst, and heightFirst.', example='heightFirst'),
        scanMode?: string(name='ScanMode', description='The scan mode. Valid values:

*   **interlaced**
*   **progressive**', example='interlaced'),
        width?: string(name='Width', description='The width of the video.

*   Unit: pixel.
*   Default value: the width of the input video.', example='256'),
      }(name='Video', description='The video codec configuration.'),
    }
  ](name='Template')
  }(name='TemplateList', description='The details of the custom transcoding templates.'),
}

model QueryTemplateListResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: QueryTemplateListResponseBody(name='body'),
}

/**
  * You can call this operation to query up to 10 custom transcoding templates at a time.
  * ## Limits on QPS
  * You can call this operation up to 100 times per second. If the number of the calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request QueryTemplateListRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return QueryTemplateListResponse
 */
async function queryTemplateListWithOptions(request: QueryTemplateListRequest, runtime: Util.RuntimeOptions): QueryTemplateListResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.templateIds)) {
    query['TemplateIds'] = request.templateIds;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'QueryTemplateList',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * You can call this operation to query up to 10 custom transcoding templates at a time.
  * ## Limits on QPS
  * You can call this operation up to 100 times per second. If the number of the calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request QueryTemplateListRequest
  * @return QueryTemplateListResponse
 */
async function queryTemplateList(request: QueryTemplateListRequest): QueryTemplateListResponse {
  var runtime = new Util.RuntimeOptions{};
  return queryTemplateListWithOptions(request, runtime);
}

model QueryVideoQualityJobRequest {
  jobId?: string(name='JobId', example='7c2abbb270474c98823dac56cc06****'),
  userId?: long(name='UserId', example='125340688170****'),
}

model QueryVideoQualityJobResponseBody = {
  code?: string(name='Code', example='Success'),
  job?: {
    jobId?: string(name='JobId', example='0c8f04aa60bd4377a906bd6c91ec****'),
    message?: string(name='Message', example='The job is completed successfully'),
    output?: string(name='Output', example='oss://example-bucket-****.oss-cn-shanghai.aliyuncs.com/example1.txt'),
    status?: string(name='Status', example='Success'),
    userId?: long(name='UserId', example='125340688170****'),
    videoQualityResults?: string(name='VideoQualityResults', example='{"LessExpose":[{"Ratio":"0.917673658134****","TimeStamp":"00:00:10.0"}],"Noise":[{"Ratio":"0.0419343847352","TimeStamp":"00:00:05.0"}],"ImageDefinition":[{"TimeStamp":"00:00:05.0"},{"TimeStamp":"00:00:10.0"}],"Border":[{"TimeStamp":"00:00:05.0"},{"TimeStamp":"00:00:10.0"}],"Freeze":[{"Score":"1","TimeStamp":"00:00:00.233"},{"Score":"1","TimeStamp":"00:00:02.600"},{"Score":"1","TimeStamp":"00:00:02.900"},{"Score":"1","TimeStamp":"00:00:02.933"},{"Score":"1","TimeStamp":"00:00:03.400"},{"Score":"1","TimeStamp":"00:00:05.166"},{"Score":"1","TimeStamp":"00:00:05.800"},{"Score":"1","TimeStamp":"00:00:07.700"},{"Score":"1","TimeStamp":"00:00:07.800"},{"Score":"1","TimeStamp":"00:00:08.633"},{"Score":"1","TimeStamp":"00:00:08.833"},{"Score":"1","TimeStamp":"00:00:09.0"}]}'),
  }(name='Job'),
  message?: string(name='Message', example='The job is completed successfully'),
  requestId?: string(name='RequestId', example='5210DBB0-E327-4D45-ADBC-0B83C8796E26'),
}

model QueryVideoQualityJobResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: QueryVideoQualityJobResponseBody(name='body'),
}

async function queryVideoQualityJobWithOptions(request: QueryVideoQualityJobRequest, runtime: Util.RuntimeOptions): QueryVideoQualityJobResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.jobId)) {
    query['JobId'] = request.jobId;
  }
  if (!Util.isUnset(request.userId)) {
    query['UserId'] = request.userId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'QueryVideoQualityJob',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

async function queryVideoQualityJob(request: QueryVideoQualityJobRequest): QueryVideoQualityJobResponse {
  var runtime = new Util.RuntimeOptions{};
  return queryVideoQualityJobWithOptions(request, runtime);
}

model QueryWaterMarkTemplateListRequest {
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  waterMarkTemplateIds?: string(name='WaterMarkTemplateIds', description='The IDs of the watermark templates that you want to query. To obtain the IDs of the watermark templates, you can log on to the **ApsaraVideo Media Processing (MPS) console** and choose **Global Settings** > **Watermark Templates** in the left-side navigation pane. You can query up to 10 watermark templates at a time. Separate multiple IDs of watermark templates with commas (,).', example='3780bd69b2b74540bc7b1096f564****'),
}

model QueryWaterMarkTemplateListResponseBody = {
  nonExistWids?: {
    string?: [ string ](name='String')
  }(name='NonExistWids', description='The watermark template IDs that do not exist. This parameter is not returned if all specified watermark templates are found.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='17079AF5-6276-51A9-B755-D26594C93F3C'),
  waterMarkTemplateList?: {
    waterMarkTemplate?: [ 
    {
      dx?: string(name='Dx', description='The horizontal offset. Unit: pixel.', example='100'),
      dy?: string(name='Dy', description='The vertical offset. Unit: pixel.', example='100'),
      height?: string(name='Height', description='The height of the watermark image. Unit: pixel.', example='8'),
      id?: string(name='Id', description='The ID of the watermark template.', example='3780bd69b2b74540bc7b1096f564****'),
      name?: string(name='Name', description='The name of the watermark template.', example='example-watermark'),
      ratioRefer?: {
        dx?: string(name='Dx', description='The horizontal offset of the watermark relative to the output video image. Default value: **0**. The default value indicates no offset. The value can be an integer or a decimal.

*   **Integer**: the vertical offset. This indicates the absolute position. Unit: pixel.
*   **Decimal**: the ratio of the horizontal offset to the width of the output video. The ratio varies based on the size of the video. Four decimal places are supported, such as 0.9999. More decimal places are discarded.', example='0.51'),
        dy?: string(name='Dy', description='The vertical offset of the watermark relative to the output video image. Default value: **0**. The default value indicates no offset. The value can be an integer or a decimal.

*   **Integer**: the vertical offset. This indicates the absolute position. Unit: pixel.
*   **Decimal**: the ratio of the vertical offset to the height of the output video. The ratio varies based on the size of the video. Four decimal places are supported, such as 0.9999. More decimal places are discarded.', example='0.4'),
        height?: string(name='Height', description='The height of the watermark image in the output video. The value can be an integer or a decimal.

*   **Integer**: the height of the watermark image. This indicates the absolute position. Unit: pixel.
*   **Decimal**: the ratio of the height of the watermark image to the height of the output video. The ratio varies based on the size of the video. Four decimal places are supported, such as 0.9999. More decimal places are discarded.', example='0.33'),
        width?: string(name='Width', description='The width of the watermark image in the output video. The value can be an integer or a decimal.

*   **Integer**: the width of the watermark image. This indicates the absolute position. Unit: pixel.
*   **Decimal**: the ratio of the width of the watermark image to the width of the output video. The ratio varies based on the size of the video. Four decimal places are supported, such as 0.9999. More decimal places are discarded.', example='0.36'),
      }(name='RatioRefer', description='The values of the Height, Width, Dx, and Dy parameters relative to the reference edges. If the values of the Height, Width, Dx, and Dy parameters are decimals between 0 and 1, the values are calculated by referring to the following edges in sequence:

*   **Width**: the width edge.
*   **Height**: the height edge.
*   **Long**: the long edge.
*   **Short**: the short edge.'),
      referPos?: string(name='ReferPos', description='The position of the watermark. Valid values:

*   **TopRight**: the upper-right corner.
*   **TopLeft**: the upper-left corner.
*   **BottomRight**: the lower-right corner.
*   **BottomLeft**: the lower-left corner.', example='TopRight'),
      state?: string(name='State', description='The status of the watermark template. Valid values: Valid values:

*   **Normal**: The watermark template is normal.
*   **Deleted**: The watermark template is deleted.', example='Normal'),
      timeline?: {
        duration?: string(name='Duration', description='The display duration of the watermark. Default value: **ToEND**. The default value indicates that the watermark is displayed until the video ends.', example='10'),
        start?: string(name='Start', description='The beginning of the time range during which the watermark is displayed.

*   Unit: seconds.
*   Default value: **0**.', example='0'),
      }(name='Timeline', description='The timeline of the watermark.'),
      type?: string(name='Type', description='The type of the watermark. Valid values:

*   Image, and
*   Text.

>  Only watermarks of the **Image** type are supported.', example='Image'),
      width?: string(name='Width', description='The width of the watermark image. Unit: pixel.', example='8'),
    }
  ](name='WaterMarkTemplate')
  }(name='WaterMarkTemplateList', description='The details of the watermark templates.'),
}

model QueryWaterMarkTemplateListResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: QueryWaterMarkTemplateListResponseBody(name='body'),
}

/**
  * You can call this operation to query up to 10 watermark templates at a time.
  * ## QPS limit
  * You can call this API operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request QueryWaterMarkTemplateListRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return QueryWaterMarkTemplateListResponse
 */
async function queryWaterMarkTemplateListWithOptions(request: QueryWaterMarkTemplateListRequest, runtime: Util.RuntimeOptions): QueryWaterMarkTemplateListResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.waterMarkTemplateIds)) {
    query['WaterMarkTemplateIds'] = request.waterMarkTemplateIds;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'QueryWaterMarkTemplateList',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * You can call this operation to query up to 10 watermark templates at a time.
  * ## QPS limit
  * You can call this API operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request QueryWaterMarkTemplateListRequest
  * @return QueryWaterMarkTemplateListResponse
 */
async function queryWaterMarkTemplateList(request: QueryWaterMarkTemplateListRequest): QueryWaterMarkTemplateListResponse {
  var runtime = new Util.RuntimeOptions{};
  return queryWaterMarkTemplateListWithOptions(request, runtime);
}

model RegisterCustomFaceRequest {
  categoryId?: string(name='CategoryId', example='CategoryId001-****'),
  imageUrl?: string(name='ImageUrl', example='http://example-****.jpeg'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  personId?: string(name='PersonId', example='PersonId001-****'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model RegisterCustomFaceResponseBody = {
  faceId?: string(name='FaceId', example='c6cc71cb44a9491093818faf9d60****'),
  requestId?: string(name='RequestId', example='91AEA76D-25B5-50DF-9126-AA6BB10FDAF4'),
}

model RegisterCustomFaceResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: RegisterCustomFaceResponseBody(name='body'),
}

async function registerCustomFaceWithOptions(request: RegisterCustomFaceRequest, runtime: Util.RuntimeOptions): RegisterCustomFaceResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.categoryId)) {
    query['CategoryId'] = request.categoryId;
  }
  if (!Util.isUnset(request.imageUrl)) {
    query['ImageUrl'] = request.imageUrl;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.personId)) {
    query['PersonId'] = request.personId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'RegisterCustomFace',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

async function registerCustomFace(request: RegisterCustomFaceRequest): RegisterCustomFaceResponse {
  var runtime = new Util.RuntimeOptions{};
  return registerCustomFaceWithOptions(request, runtime);
}

model RegisterCustomViewRequest {
  algorithm?: string(name='Algorithm', example='landmark'),
  customEntityId?: string(name='CustomEntityId', example='2'),
  customGroupId?: string(name='CustomGroupId', example='1'),
  imageUrl?: string(name='ImageUrl', example='http://127.66.**.**/image.jpeg'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model RegisterCustomViewResponseBody = {
  customViewId?: string(name='CustomViewId', example='1'),
  requestId?: string(name='RequestId', example='580e8ce3-3b80-44c5-9f3f-36ac3cc5bdd5'),
}

model RegisterCustomViewResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: RegisterCustomViewResponseBody(name='body'),
}

async function registerCustomViewWithOptions(request: RegisterCustomViewRequest, runtime: Util.RuntimeOptions): RegisterCustomViewResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.algorithm)) {
    query['Algorithm'] = request.algorithm;
  }
  if (!Util.isUnset(request.customEntityId)) {
    query['CustomEntityId'] = request.customEntityId;
  }
  if (!Util.isUnset(request.customGroupId)) {
    query['CustomGroupId'] = request.customGroupId;
  }
  if (!Util.isUnset(request.imageUrl)) {
    query['ImageUrl'] = request.imageUrl;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'RegisterCustomView',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

async function registerCustomView(request: RegisterCustomViewRequest): RegisterCustomViewResponse {
  var runtime = new Util.RuntimeOptions{};
  return registerCustomViewWithOptions(request, runtime);
}

model ReportFpShotJobResultRequest {
  details?: string(name='Details', description='The details of the job results. You can obtain the details from the response parameters of the [QueryFpShotJobList](~~93557~~) operation.', example='example details ****'),
  jobId?: string(name='JobId', description='The ID of the media fingerprint analysis job on which you want to provide feedback. To view the job ID, log on to the **MPS console**, click **Tasks** in the left-side navigation pane, and then click the **Video DNA** tab on the Tasks page.', example='88c6ca184c0e47098a5b665e2a12****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  result?: string(name='Result', description='The results of the media fingerprint analysis job. You can obtain the results of the media fingerprint analysis job from the response parameters of the [QueryFpShotJobList](~~93557~~) operation.', example='example result ****'),
}

model ReportFpShotJobResultResponseBody = {
  jobId?: string(name='JobId', description='The ID of the media fingerprint analysis job on which you provide feedback. We recommend that you keep this ID for subsequent operation calls.', example='88c6ca184c0e47098a5b665e2a12****'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='25818875-5F78-4A13-BEF6-D7393642CA58'),
}

model ReportFpShotJobResultResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: ReportFpShotJobResultResponseBody(name='body'),
}

/**
  * You can call this operation to provide feedback only on the results of failed media fingerprint analysis jobs.
  * ## QPS limit
  * You can call this operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request ReportFpShotJobResultRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return ReportFpShotJobResultResponse
 */
async function reportFpShotJobResultWithOptions(request: ReportFpShotJobResultRequest, runtime: Util.RuntimeOptions): ReportFpShotJobResultResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.details)) {
    query['Details'] = request.details;
  }
  if (!Util.isUnset(request.jobId)) {
    query['JobId'] = request.jobId;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.result)) {
    query['Result'] = request.result;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'ReportFpShotJobResult',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * You can call this operation to provide feedback only on the results of failed media fingerprint analysis jobs.
  * ## QPS limit
  * You can call this operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request ReportFpShotJobResultRequest
  * @return ReportFpShotJobResultResponse
 */
async function reportFpShotJobResult(request: ReportFpShotJobResultRequest): ReportFpShotJobResultResponse {
  var runtime = new Util.RuntimeOptions{};
  return reportFpShotJobResultWithOptions(request, runtime);
}

model SearchMediaWorkflowRequest {
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  pageNumber?: long(name='PageNumber', description='The number of the page to return. Default value: **1**.', example='1'),
  pageSize?: long(name='PageSize', description='The number of entries to return on each page.

*   A maximum of **100** entries can be returned on each page.
*   Default value: **10**.', example='10'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  stateList?: string(name='StateList', description='The status of the media workflows that you want to query. You can specify multiple states. Separate multiple states with commas (,). Default value: **Inactive,Active,Deleted**. Valid values:

*   **Inactive**: Deactivated media workflows are queried.
*   **Active**: Activated media workflows are queried.
*   **Deleted**: Deleted media workflows are queried.', example='Inactive,Active,Deleted'),
}

model SearchMediaWorkflowResponseBody = {
  mediaWorkflowList?: {
    mediaWorkflow?: [ 
    {
      creationTime?: string(name='CreationTime', description='The time when the media workflow was created.', example='2016-04-01T05:38:41Z'),
      mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the media workflow.', example='88c6ca184c0e4578645b665e2a12****'),
      name?: string(name='Name', description='The name of the media workflow.', example='example-mediaworkflow-****'),
      state?: string(name='State', description='The status of the media workflow. Valid values:

*   **Inactive**: The media workflow is deactivated.
*   **Active**: The media workflow is activated.
*   **Deleted**: The media workflow is deleted.', example='Active'),
      topology?: string(name='Topology', description='The topology of the media workflow.', example='{"MediaWorkflowList":{"MediaWorkflow":[{"CreationTime":"2016-04-01T05:29:38Z","Name":"example-mediaworkflow-****","State":"Active","Topology":"{\\"Activities\\":{\\"Act-Start\\":{\\"Parameters\\":{\\"PipelineId\\":\\"130266f58161436a80bf07cb12c8****\\",\\"InputFile\\":\\"{\\\\\\"Bucket\\\\\\": \\\\\\"example-bucket-****\\\\\\",\\\\\\"Location\\\\\\": \\\\\\"example-location\\\\\\"}\\"},\\"Type\\":\\"Start\\"},\\"Act-Report\\":{\\"Parameters\\":{},\\"Type\\":\\"Report\\"},\\"Act-Transcode-M3U8\\":{\\"Parameters\\":{\\"Outputs\\":\\"[{\\\\\\"Object\\\\\\":\\\\\\"transcode/{ObjectPrefix}{FileName}\\\\\\",\\\\\\"TemplateId\\\\\\": \\\\\\"957d1719ee85ed6527b90cf62726****\\\\\\"}]\\",\\"OutputBucket\\":\\"example-bucket-****\\",\\"OutputLocation\\":\\"example-location\\"},\\"Type\\":\\"Transcode\\"}},\\"Dependencies\\":{\\"Act-Start\\":[\\"Act-Transcode-M3U8\\"],\\"Act-Report\\":[],\\"Act-Transcode-M3U8\\":[\\"Act-Report\\"]}}","MediaWorkflowId":"93ab850b4f6f44eab54b6e91d24d****"}]},"RequestId":"16CD0CDD-457E-420D-9755-8385075A1234"}'),
      triggerMode?: string(name='TriggerMode', description='The trigger mode of the media workflow. Valid values:

*   **OssAutoTrigger**: The media workflow is automatically triggered.
*   **NotInAuto**: The media workflow is not automatically triggered.', example='OssAutoTrigger'),
    }
  ](name='MediaWorkflow')
  }(name='MediaWorkflowList', description='The details of the media workflows.'),
  pageNumber?: long(name='PageNumber', description='The page number of the returned page.', example='1'),
  pageSize?: long(name='PageSize', description='The number of entries returned per page.', example='10'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='16CD0CDD-457E-420D-9755-8385075A1234'),
  totalCount?: long(name='TotalCount', description='The total number of entries returned.', example='1'),
}

model SearchMediaWorkflowResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: SearchMediaWorkflowResponseBody(name='body'),
}

/**
  * You can call this operation to query media workflows in the specified state. If you do not specify the state, all media workflows are queried by default.
  * ## Limits on QPS
  * You can call this operation up to 100 times per second. If the number of the calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request SearchMediaWorkflowRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return SearchMediaWorkflowResponse
 */
async function searchMediaWorkflowWithOptions(request: SearchMediaWorkflowRequest, runtime: Util.RuntimeOptions): SearchMediaWorkflowResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.pageNumber)) {
    query['PageNumber'] = request.pageNumber;
  }
  if (!Util.isUnset(request.pageSize)) {
    query['PageSize'] = request.pageSize;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.stateList)) {
    query['StateList'] = request.stateList;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'SearchMediaWorkflow',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * You can call this operation to query media workflows in the specified state. If you do not specify the state, all media workflows are queried by default.
  * ## Limits on QPS
  * You can call this operation up to 100 times per second. If the number of the calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request SearchMediaWorkflowRequest
  * @return SearchMediaWorkflowResponse
 */
async function searchMediaWorkflow(request: SearchMediaWorkflowRequest): SearchMediaWorkflowResponse {
  var runtime = new Util.RuntimeOptions{};
  return searchMediaWorkflowWithOptions(request, runtime);
}

model SearchPipelineRequest {
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  pageNumber?: long(name='PageNumber', description='The number of the page to return. Default value: **1**.', example='1'),
  pageSize?: long(name='PageSize', description='The number of entries to return on each page.

*   A maximum of **100** entries can be returned on each page.
*   Default value: **10**.', example='10'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  state?: string(name='State', description='The status of the MPS queues that you want to query. If you leave this parameter empty, all MPS queues are queried.

*   **All**: All MPS queues are queried.
*   **Active**: Active MPS queues are queried.
*   **Paused**: Paused MPS queues are queried.
*   Default value: **All**.', example='Paused'),
}

model SearchPipelineResponseBody = {
  pageNumber?: long(name='PageNumber', description='The page number of the returned page.', example='1'),
  pageSize?: long(name='PageSize', description='The number of entries returned per page.', example='10'),
  pipelineList?: {
    pipeline?: [ 
    {
      extendConfig?: {
        isBoostNew?: boolean(name='IsBoostNew'),
        maxMultiSpeed?: int32(name='MaxMultiSpeed'),
        multiSpeedDowngradePolicy?: string(name='MultiSpeedDowngradePolicy'),
      }(name='ExtendConfig'),
      id?: string(name='Id', description='The ID of the MPS queue.', example='d1ce4d3efcb549419193f50f1fcd****'),
      name?: string(name='Name', description='The name of the MPS queue.', example='example-pipeline-****'),
      notifyConfig?: {
        mqTag?: string(name='MqTag', example='mts-test'),
        mqTopic?: string(name='MqTopic', example='example1,example2'),
        queueName?: string(name='QueueName', description='The name of the queue that is created in MNS.', example='example-queue-****'),
        topic?: string(name='Topic', description='The name of the topic that is created in MNS.', example='example-topic-****'),
      }(name='NotifyConfig', description='The MNS notification configuration.'),
      quotaAllocate?: long(name='QuotaAllocate', example='10'),
      role?: string(name='Role', description='The role that is assigned to the current RAM user.', example='AliyunMTSDefaultRole'),
      speed?: string(name='Speed', description='The type of the MPS queue. Default value: **Standard**. Valid values:

*   **Boost**: MPS queue with transcoding speed boosted.
*   **Standard**: standard MPS queue.
*   **NarrowBandHDV2**: MPS queue that supports Narrowband HD™ 2.0.
*   **AIVideoCover**: MPS queue for intelligent snapshot capture.
*   **AIVideoFPShot**: MPS queue for media fingerprinting.
*   **AIVideoCensor**: MPS queue for automated review.
*   **AIVideoMCU**: MPS queue for smart tagging.
*   **AIVideoSummary**: MPS queue for video synopsis.
*   **AIVideoPorn**: MPS queue for pornography detection in videos.
*   **AIAudioKWS**: MPS queue for keyword recognition in audio.
*   **AIAudioASR**: MPS queue for speech-to-text conversion.', example='Standard'),
      speedLevel?: long(name='SpeedLevel', example='1'),
      state?: string(name='State', description='The status of the pipeline. Valid values:

*   **Active**: The MPS queue is active.
*   **Paused**: The MPS queue is paused.', example='Paused'),
    }
  ](name='Pipeline')
  }(name='PipelineList', description='The details of the MPS queues.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='338CA33A-AE83-5DF4-B6F2-C6D3ED8143F5'),
  totalCount?: long(name='TotalCount', description='The total number of entries returned.', example='1'),
}

model SearchPipelineResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: SearchPipelineResponseBody(name='body'),
}

/**
  * You can call this operation to query MPS queues in the specified state. If you do not specify the state, all MPS queues are queried by default.
  * ## Limits on QPS
  * You can call this operation up to 100 times per second. If the number of the calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request SearchPipelineRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return SearchPipelineResponse
 */
async function searchPipelineWithOptions(request: SearchPipelineRequest, runtime: Util.RuntimeOptions): SearchPipelineResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.pageNumber)) {
    query['PageNumber'] = request.pageNumber;
  }
  if (!Util.isUnset(request.pageSize)) {
    query['PageSize'] = request.pageSize;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.state)) {
    query['State'] = request.state;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'SearchPipeline',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * You can call this operation to query MPS queues in the specified state. If you do not specify the state, all MPS queues are queried by default.
  * ## Limits on QPS
  * You can call this operation up to 100 times per second. If the number of the calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request SearchPipelineRequest
  * @return SearchPipelineResponse
 */
async function searchPipeline(request: SearchPipelineRequest): SearchPipelineResponse {
  var runtime = new Util.RuntimeOptions{};
  return searchPipelineWithOptions(request, runtime);
}

model SearchTemplateRequest {
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  pageNumber?: long(name='PageNumber', description='The number of the page to return. Default value: **1**.', example='1'),
  pageSize?: long(name='PageSize', description='The number of entries to return on each page.

*   A maximum of **100** entries can be returned on each page.
*   Default value: **10**.', example='10'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  state?: string(name='State', description='The status of the custom transcoding templates that you want to query.

*   **All**: All custom transcoding templates are queried.
*   **normal**: Normal custom transcoding templates are queried.
*   **Deleted**: Deleted custom transcoding templates are queried.
*   Default value: **All**.', example='Normal'),
}

model SearchTemplateResponseBody = {
  pageNumber?: long(name='PageNumber', description='The page number of the returned page.', example='1'),
  pageSize?: long(name='PageSize', description='The number of entries returned per page.', example='10'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='BC860F04-778A-472F-AB39-E1BF329C****'),
  templateList?: {
    template?: [ 
    {
      audio?: {
        bitrate?: string(name='Bitrate', description='The audio bitrate of the output file.

*   Unit: Kbit/s.
*   Default value: **128**.', example='500'),
        channels?: string(name='Channels', description='The number of sound channels. Default value: **2**.', example='2'),
        codec?: string(name='Codec', description='The audio codec. Default value: **aac**. Valid values:

*   **aac**
*   **mp3**
*   **vorbis**
*   **flac**', example='aac'),
        profile?: string(name='Profile', description='The codec profile of the audio. Valid values when the Codec parameter is set to aac:

*   **aac_low**
*   **aac_he**
*   **aac_he_v2**
*   **aac_ld**
*   **aac_eld**', example='aac_low'),
        qscale?: string(name='Qscale', description='The strength of the independent noise reduction algorithm. Valid values: **\\[1, 9]**.', example='1'),
        remove?: string(name='Remove', description='Indicates whether to delete the audio stream.

*   **true**: The audio stream is deleted.
*   **false**: The audio stream is retained.
*   Default value: **false**.', example='false'),
        samplerate?: string(name='Samplerate', description='The sampling rate.

*   Unit: Hz.
*   Default value: **44100**.', example='44100'),
      }(name='Audio', description='The audio codec configuration.'),
      container?: {
        format?: string(name='Format', description='The container format. Valid values:

*   **flv**
*   **mp4**
*   **ts**
*   **m3u8**
*   **gif**
*   **mp3**
*   **ogg**
*   **flac**', example='mp4'),
      }(name='Container', description='The container configuration.'),
      id?: string(name='Id', description='The ID of the transcoding template.', example='16f01ad6175e4230ac42bb5182cd****'),
      muxConfig?: {
        gif?: {
          ditherMode?: string(name='DitherMode', description='The color dithering algorithm of the palette. Valid values: sierra and bayer.', example='sierra'),
          finalDelay?: string(name='FinalDelay', description='The duration for which the final frame is paused. Unit: centiseconds.', example='0'),
          isCustomPalette?: string(name='IsCustomPalette', description='Indicates whether the custom palette is used.', example='false'),
          loop?: string(name='Loop', description='The loop count.', example='0'),
        }(name='Gif', description='The transmuxing configuration for GIF.'),
        segment?: {
          duration?: string(name='Duration', description='The length of the segment. Unit: seconds.', example='10'),
        }(name='Segment', description='The segment configuration.'),
      }(name='MuxConfig', description='The transmuxing configuration.'),
      name?: string(name='Name', description='The name of the template.', example='MPS-example'),
      state?: string(name='State', description='The status of the template.

*   **Normal**: The template is normal.
*   **Deleted**: The template is deleted.', example='Normal'),
      transConfig?: {
        adjDarMethod?: string(name='AdjDarMethod', description='The method of resolution adjustment. Default value: **none**. Valid values:

*   rescale: The input video is rescaled.
*   crop: The input video is cropped.
*   none: No change is made.', example='none'),
        isCheckAudioBitrate?: string(name='IsCheckAudioBitrate', description='Indicates whether to check the audio bitrate.

*   If this feature is enabled and the system detects that the audio bitrate of the output file is greater than that of the input file, the audio bitrate of the input file is retained after transcoding.
*   **true**: The audio bitrate is checked.
*   **false**: The audio bitrate is not checked.
*   Default value: **false**.', example='false'),
        isCheckAudioBitrateFail?: string(name='IsCheckAudioBitrateFail', description='Indicates whether to allow audio bitrate check errors.

*   **true**: If the audio bitrate check fails, the input file is not transcoded.
*   **false**: The audio bitrate is not checked.
*   Default value: **false**.
*   This parameter takes precedence over the IsCheckAudioBitrate parameter.', example='false'),
        isCheckReso?: string(name='IsCheckReso', description='Indicates whether to check the resolution.

*   If this feature is enabled and the system detects that the resolution of the output file is higher than that of the input file based on the width or height, the resolution of the input file is retained after transcoding.
*   **true**: The resolution is checked.
*   **false**: The resolution is not checked.
*   Default value: **false**.', example='false'),
        isCheckResoFail?: string(name='IsCheckResoFail', description='Indicates whether to check the resolution.

*   If this feature is enabled and the system detects that the resolution of the output file is higher than that of the input file based on the width or height, an error that indicates a transcoding failure is returned.
*   **true**: The resolution is checked.
*   **false**: The resolution is not checked.
*   Default value: **false**.', example='false'),
        isCheckVideoBitrate?: string(name='IsCheckVideoBitrate', description='Indicates whether to check the video bitrate.

*   If this feature is enabled and the system detects that the video bitrate of the output file is greater than that of the input file, the video bitrate of the input file is retained after transcoding.
*   **true**: The video bitrate is checked.
*   **false**: The video bitrate is not checked.
*   Default value: **false**.', example='false'),
        isCheckVideoBitrateFail?: string(name='IsCheckVideoBitrateFail', description='Indicates whether to allow video bitrate check errors.

*   **true**: If the video bitrate check fails, the input file is not transcoded.
*   **false**: The video bitrate is not checked.
*   Default value: **false**.
*   This parameter takes precedence over the IsCheckVideoBitrate parameter.', example='false'),
        transMode?: string(name='TransMode', description='The transcoding mode. Default value: **onepass**. Valid values:

*   **onepass**
*   **twopass**
*   **CBR**', example='onepass'),
      }(name='TransConfig', description='The general transcoding configuration.'),
      video?: {
        bitrate?: string(name='Bitrate', description='The average bitrate of the video. Unit: Kbit/s.', example='200'),
        bitrateBnd?: {
          max?: string(name='Max', description='The upper limit of the total bitrate. Unit: Kbit/s.', example='500'),
          min?: string(name='Min', description='The lower limit of the total bitrate. Unit: Kbit/s.', example='100'),
        }(name='BitrateBnd', description='The average bitrate range of the video.'),
        bufsize?: string(name='Bufsize', description='The size of the buffer.

*   Unit: KB.
*   Default value: **6000**.', example='6000'),
        codec?: string(name='Codec', description='The video codec.

*   Valid values: H.264 and H.265.
*   Default value: **H.264**.', example='H.264'),
        crf?: string(name='Crf', description='The constant rate factor.

*   Default value when the Codec parameter is set to H.264: **23**. Default value when the Codec parameter is set to H.265: **26**.
*   If this parameter is specified, the setting of Bitrate becomes invalid.', example='15'),
        crop?: string(name='Crop', description='The method of video cropping. Valid values:

*   **border**: automatically detects and removes borders.
*   **Other values**: the custom cropping modes. Format: width:height:left:top. Example: 1280:800:0:140.', example='border'),
        degrain?: string(name='Degrain', description='The level of video quality control.', example='10'),
        fps?: string(name='Fps', description='The frame rate of the video.

*   The value is 60 if the frame rate of the input video exceeds 60.
*   Default value: the frame rate of the input video.', example='25'),
        gop?: string(name='Gop', description='The maximum number of frames between two keyframes. Default value: **250**.', example='10'),
        height?: string(name='Height', description='The height of the video.

*   Unit: pixel.
*   Default value: the height of the input video.', example='800'),
        longShortMode?: string(name='LongShortMode', description='Indicates whether to enable the auto-rotate screen feature.

*   If this feature is enabled, the width of the output video corresponds to the long side of the input video, which is the height of the input video in portrait mode. The height of the output video corresponds to the short side of the input video, which is the width of the input video in portrait mode.
*   **true**: The auto-rotate screen feature is enabled.
*   **false**: The auto-rotate screen feature is disabled.
*   Default value: **false**.', example='false'),
        maxFps?: string(name='MaxFps', description='The maximum frame rate.', example='60'),
        maxrate?: string(name='Maxrate', description='The maximum video bitrate. Unit: Kbit/s.', example='500'),
        pad?: string(name='Pad', description='The black borders to be added to the video.

*   Format: width:height:left:top.
*   Example: 1280:800:0:140.', example='1280:800:0:140'),
        pixFmt?: string(name='PixFmt', description='The pixel format for video color encoding. Valid values: standard pixel formats such as yuv420p and yuvj420p.', example='yuv420p'),
        preset?: string(name='Preset', description='The preset video algorithm. Default value: **medium**. Valid values:

*   **veryfast**
*   **fast**
*   **medium**
*   **slow**
*   **slower**', example='medium'),
        profile?: string(name='Profile', description='The codec profile.

*   **baseline**: applicable to mobile devices.
*   **main**: applicable to standard-definition devices.
*   **high**: applicable to high-definition devices.
*   Default value: **high**.', example='high'),
        qscale?: string(name='Qscale', description='The strength of the independent noise reduction algorithm.', example='1'),
        remove?: string(name='Remove', description='Indicates whether to delete the video stream.

*   **true**: The video stream is deleted.
*   **false**: The video stream is retained.
*   Default value: **false**.', example='false'),
        scanMode?: string(name='ScanMode', description='The scan mode. Valid values:

*   **interlaced**
*   **progressive**', example='interlaced'),
        width?: string(name='Width', description='The width of the video.

*   Valid values: **\\[128, 4096]**.

<!---->

*   Unit: pixel.
*   Default value: the width of the input video.', example='256'),
      }(name='Video', description='The video codec configuration.'),
    }
  ](name='Template')
  }(name='TemplateList', description='The details of the custom transcoding templates.'),
  totalCount?: long(name='TotalCount', description='The total number of entries returned.', example='1'),
}

model SearchTemplateResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: SearchTemplateResponseBody(name='body'),
}

/**
  * You can call this operation to query custom transcoding templates in the specified state.
  * ## Limits on QPS
  * You can call this operation up to 100 times per second. If the number of the calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request SearchTemplateRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return SearchTemplateResponse
 */
async function searchTemplateWithOptions(request: SearchTemplateRequest, runtime: Util.RuntimeOptions): SearchTemplateResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.pageNumber)) {
    query['PageNumber'] = request.pageNumber;
  }
  if (!Util.isUnset(request.pageSize)) {
    query['PageSize'] = request.pageSize;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.state)) {
    query['State'] = request.state;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'SearchTemplate',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * You can call this operation to query custom transcoding templates in the specified state.
  * ## Limits on QPS
  * You can call this operation up to 100 times per second. If the number of the calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request SearchTemplateRequest
  * @return SearchTemplateResponse
 */
async function searchTemplate(request: SearchTemplateRequest): SearchTemplateResponse {
  var runtime = new Util.RuntimeOptions{};
  return searchTemplateWithOptions(request, runtime);
}

model SearchWaterMarkTemplateRequest {
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  pageNumber?: long(name='PageNumber', description='The number of the page to return. Default value: **1**.', example='1'),
  pageSize?: long(name='PageSize', description='The number of entries to return on each page.

*   A maximum of **100** entries can be returned on each page.
*   Default value: **10**.', example='10'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  state?: string(name='State', description='The status of the watermark templates that you want to query. Valid values:

*   **All**: All watermark templates are queried. This is the default value.
*   **normal**: Normal watermark templates are queried.
*   **Deleted**: Deleted watermark templates are queried.', example='Normal'),
}

model SearchWaterMarkTemplateResponseBody = {
  pageNumber?: long(name='PageNumber', description='The number of the returned page.', example='1'),
  pageSize?: long(name='PageSize', description='The number of entries returned on each page.', example='10'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='FC029D04-8F47-57FF-A759-23383C15617D'),
  totalCount?: long(name='TotalCount', description='The total number of returned entries.', example='1'),
  waterMarkTemplateList?: {
    waterMarkTemplate?: [ 
    {
      dx?: string(name='Dx', description='The horizontal offset. Unit: pixel.', example='100'),
      dy?: string(name='Dy', description='The vertical offset. Unit: pixel.', example='100'),
      height?: string(name='Height', description='The height of the watermark image. Unit: pixel.', example='8'),
      id?: string(name='Id', description='The ID of the watermark template.', example='88c6ca184c0e4578645b665e2a12****'),
      name?: string(name='Name', description='The name of the watermark template.', example='example-watermark'),
      ratioRefer?: {
        dx?: string(name='Dx', description='The horizontal offset of the watermark relative to the output video image. Default value: **0**. The default value indicates no offset.

The value can be an integer or a decimal.

*   **Integer**: the vertical offset. This indicates the absolute position. Unit: pixel.
*   **Decimal**: the ratio of the horizontal offset to the width of the output video. The ratio varies based on the size of the video. Four decimal places are supported, such as 0.9999. More decimal places are discarded.', example='0.51'),
        dy?: string(name='Dy', description='The vertical offset of the watermark relative to the output video image. Default value: **0**. The default value indicates no offset. The value can be an integer or a decimal.

*   **Integer**: the vertical offset. This indicates the absolute position. Unit: pixel.
*   **Decimal**: the ratio of the vertical offset to the height of the output video. The ratio varies based on the size of the video. Four decimal places are supported, such as 0.9999. More decimal places are discarded.', example='0.2'),
        height?: string(name='Height', description='The height of the watermark image in the output video. The value can be an integer or a decimal.

*   **Integer**: the height of the watermark image. This indicates the absolute position. Unit: pixel.
*   **Decimal**: the ratio of the height of the watermark image to the height of the output video. The ratio varies based on the size of the video. Four decimal places are supported, such as 0.9999. More decimal places are discarded.', example='0.33'),
        width?: string(name='Width', description='The width of the watermark image in the output video. The value can be an integer or a decimal.

*   **Integer**: the width of the watermark image. This indicates the absolute position. Unit: pixel.
*   **Decimal**: the ratio of the width of the watermark image to the width of the output video. The ratio varies based on the size of the video. Four decimal places are supported, such as 0.9999. More decimal places are discarded.', example='0.36'),
      }(name='RatioRefer', description='The values of the Height, Width, Dx, and Dy parameters relative to the reference edges. If the values of the Height, Width, Dx, and Dy parameters are decimals between 0 and 1, the values are calculated by referring to the following edges in sequence:

*   **Width**: the width edge.
*   **Height**: the height edge.
*   **Long**: the long edge.
*   **Short**: the short edge.'),
      referPos?: string(name='ReferPos', description='The position of the watermark. Valid values:

*   TopRight: the upper-right corner.
*   TopLeft: the upper-left corner.
*   BottomRight: the lower-right corner.
*   BottomLeft: the lower-left corner.', example='TopRight'),
      state?: string(name='State', description='The status of the watermark template. Valid values: Valid values:

*   **Normal**: The watermark template is normal.
*   **Deleted**: The watermark template is deleted.', example='Normal'),
      timeline?: {
        duration?: string(name='Duration', description='The display duration of the watermark. Default value: **ToEND**. The default value indicates that the watermark is displayed until the video ends.', example='ToEND'),
        start?: string(name='Start', description='The beginning of the time range during which the watermark is displayed.

*   Unit: seconds.
*   Default value: **0**.', example='0'),
      }(name='Timeline', description='The timeline of the watermark.'),
      type?: string(name='Type', description='The type of the watermark. Valid values:

*   Image: an image watermark.
*   Text: a text watermark.

>  Only watermarks of the **Image** types are supported.', example='Image'),
      width?: string(name='Width', description='The width of the watermark image. Unit: pixel.', example='8'),
    }
  ](name='WaterMarkTemplate')
  }(name='WaterMarkTemplateList', description='The details of the watermark templates.'),
}

model SearchWaterMarkTemplateResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: SearchWaterMarkTemplateResponseBody(name='body'),
}

/**
  * You can call this operation to query watermark templates in the specified state. If you do not specify the state, all watermark templates are queried by default.
  * ## QPS limit
  * You can call this API operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request SearchWaterMarkTemplateRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return SearchWaterMarkTemplateResponse
 */
async function searchWaterMarkTemplateWithOptions(request: SearchWaterMarkTemplateRequest, runtime: Util.RuntimeOptions): SearchWaterMarkTemplateResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.pageNumber)) {
    query['PageNumber'] = request.pageNumber;
  }
  if (!Util.isUnset(request.pageSize)) {
    query['PageSize'] = request.pageSize;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.state)) {
    query['State'] = request.state;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'SearchWaterMarkTemplate',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * You can call this operation to query watermark templates in the specified state. If you do not specify the state, all watermark templates are queried by default.
  * ## QPS limit
  * You can call this API operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request SearchWaterMarkTemplateRequest
  * @return SearchWaterMarkTemplateResponse
 */
async function searchWaterMarkTemplate(request: SearchWaterMarkTemplateRequest): SearchWaterMarkTemplateResponse {
  var runtime = new Util.RuntimeOptions{};
  return searchWaterMarkTemplateWithOptions(request, runtime);
}

model SubmitAnalysisJobRequest {
  analysisConfig?: string(name='AnalysisConfig', description='The job configuration. Set this parameter as required. [~~29253~~](~~29253~~)', example='{"QualityControl":{"RateQuality":25,"MethodStreaming":"network"}}'),
  input?: string(name='Input', description='The input information about the preset template analysis job to be submitted. The value is a JSON object. You must log on to the Object Storage Service (OSS) console to grant the read permissions on the specified OSS bucket to MPS. [~~29253~~](~~29253~~)', example='{"Bucket":"example-bucket","Location":"oss-cn-hangzhou","Object":"example.flv"}'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  pipelineId?: string(name='PipelineId', description='The ID of the MPS queue to which the job is added. To view the MPS queue ID, log on to the **MPS console** and choose **Global Settings** > **Pipelines** in the left-side navigation pane. If you want to enable asynchronous notifications, make sure that the queue is bound to a Message Service (MNS) topic.', example='bb558c1cc25b45309aab5be44d19****'),
  priority?: string(name='Priority', description='The priority of the job in the MPS queue to which the job is added.

*   Valid values: **1 to 10**. A value of 10 indicates the highest priority.
*   Default value: **6**.', example='10'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  userData?: string(name='UserData', description='The custom data. The custom data can contain letters, digits, and hyphens (-), and can be up to 1,024 bytes in length. It cannot start with a special character.', example='testid-001'),
}

model SubmitAnalysisJobResponseBody = {
  analysisJob?: {
    analysisConfig?: {
      propertiesControl?: {
        crop?: {
          height?: string(name='Height', description='The height of the video after the margins were cropped out.

>  This parameter is invalid if the **Mode** parameter is set to Auto or None.', example='8'),
          left?: string(name='Left', description='The left margin that was cropped out.

>  This parameter is invalid if the **Mode** parameter is set to Auto or None.', example='8'),
          mode?: string(name='Mode', description='The cropping mode. Valid values: Valid values:

*   **Auto**: Cropping was automatically run. This is the default value.
*   **Force**: Cropping was forced to run.
*   **None**: Cropping was forced not to run.', example='Auto'),
          top?: string(name='Top', description='The top margin that was cropped out.

>  This parameter is invalid if the **Mode** parameter is set to Auto or None.', example='8'),
          width?: string(name='Width', description='The width of the video after the margins were cropped out.

>  This parameter is invalid if the **Mode** parameter is set to Auto or None.', example='8'),
        }(name='Crop', description='The cropping configuration of video images.'),
        deinterlace?: string(name='Deinterlace', description='Indicates whether deinterlacing was forced to run. Valid values:

*   **Auto**: Deinterlacing was automatically run.
*   **Force**: Deinterlacing was forced to run.
*   **None**: Deinterlacing was forced not to run.', example='Force'),
      }(name='PropertiesControl', description='The control on the attributes of the job output.'),
      qualityControl?: {
        methodStreaming?: string(name='MethodStreaming', description='The playback mode. Valid values:

*   **network**: online playback
*   **local**: playback on on-premises devices
*   Default value: **network**.', example='network'),
        rateQuality?: string(name='RateQuality', description='The quality level of the job output.', example='50'),
      }(name='QualityControl', description='The quality control on the job output.'),
    }(name='AnalysisConfig', description='The job configuration.'),
    code?: string(name='Code', description='The error code returned when the job fails. This parameter is not returned if the job is successful.', example='InvalidParameter.ResourceNotFound'),
    creationTime?: string(name='CreationTime', description='The time when the job was created.', example='2014-01-10T12:00:00Z'),
    id?: string(name='Id', description='The ID of the template analysis job.', example='57f6aa3f84824309bcba67231b40****'),
    inputFile?: {
      bucket?: string(name='Bucket', description='The name of the OSS bucket to which the input file is uploaded.', example='example-bucket'),
      location?: string(name='Location', description='The ID of the OSS region.', example='oss-cn-hangzhou'),
      object?: string(name='Object', description='The name of the input file uploaded to the OSS bucket.', example='example.flv'),
    }(name='InputFile', description='The information about the job input.'),
    MNSMessageResult?: {
      errorCode?: string(name='ErrorCode', description='The error code returned when the job fails. This parameter is not returned if the job is successful.', example='InvalidParameter.ResourceNotFound'),
      errorMessage?: string(name='ErrorMessage', description='The error message returned when the job fails. This parameter is not returned if the job is successful.', example='The resource operated \\"PipelineId\\" cannot be found'),
      messageId?: string(name='MessageId', description='The ID of the success message. This parameter is not returned if the job fails.', example='3ca84a39a9024f19853b21be9cf9****'),
    }(name='MNSMessageResult', description='The message sent by MNS to notify the user of the job result.'),
    message?: string(name='Message', description='The error message returned when the job fails.', example='The resource operated \\"PipelineId\\" cannot be found'),
    percent?: long(name='Percent', description='The transcoding progress.', example='100'),
    pipelineId?: string(name='PipelineId', description='The ID of the MPS queue to which the job is added.', example='bb558c1cc25b45309aab5be44d19****'),
    priority?: string(name='Priority', description='The priority of the job in the MPS queue to which the job is added. 

- Valid values: **1 to 10**. A value of 10 indicates the highest priority.
- Default value: **10**.', example='10'),
    state?: string(name='State', description='The status of the job. Valid values:

*   **Submitted**: The job has been submitted.
*   **Analyzing**: The job is being run.
*   **Success**: The job is successful.
*   **Fail**: The job fails.', example='Success'),
    templateList?: {
      template?: [ 
      {
        audio?: {
          bitrate?: string(name='Bitrate', description='The audio bitrate of the output file.

*   Unit: Kbit/s.
*   Default value: **128**.', example='8'),
          channels?: string(name='Channels', description='The number of sound channels. Default value: **2**.', example='1'),
          codec?: string(name='Codec', description='The audio codec format. Default value: **aac**.', example='mp3'),
          profile?: string(name='Profile', description='The codec profile of the audio file. Valid values when the **Codec** parameter is set to **aac**: aac_low, aac_he, aac_he_v2, aac_ld, and aac_eld.', example='aac_low'),
          qscale?: string(name='Qscale', description='The level of quality control on the audio.', example='10'),
          samplerate?: string(name='Samplerate', description='The sampling rate.

*   Unit: Hz.
*   Default value: **44100**.', example='32000'),
        }(name='Audio', description='The audio codec configuration.'),
        container?: {
          format?: string(name='Format', description='The container format.', example='flv'),
        }(name='Container', description='The container configuration.'),
        id?: string(name='Id', description='The ID of the transcoding template.', example='S00000000-00****'),
        muxConfig?: {
          gif?: {
            finalDelay?: string(name='FinalDelay', description='The interval between two consecutive loops for the GIF format. Unit: 0.01s. For example, a value of 500 indicates 5 seconds.', example='0'),
            loop?: string(name='Loop', description='The number of loops for the GIF or WebP format. Default value: 0.', example='0'),
          }(name='Gif', description='The GIF format.'),
          segment?: {
            duration?: string(name='Duration', description='The length of the segment. Unit: seconds.', example='60'),
          }(name='Segment', description='The segment configuration.'),
        }(name='MuxConfig', description='The transmuxing configuration.'),
        name?: string(name='Name', description='The name of the template.', example='FLV-UD'),
        state?: string(name='State', description='The status of the template. Valid values:

*   **Normal**: The template is in the normal state.
*   **Deleted**: The template is deleted.', example='Normal'),
        transConfig?: {
          transMode?: string(name='TransMode', description='The transcoding mode. Valid values: onepass, twopass, and CBR. Default value: **onepass**.', example='onepass'),
        }(name='TransConfig', description='The general transcoding configuration.'),
        video?: {
          bitrate?: string(name='Bitrate', description='The average bitrate of the video. Unit: Kbit/s.', example='10'),
          bitrateBnd?: {
            max?: string(name='Max', description='The upper limit of the total bitrate. Unit: Kbit/s.', example='20'),
            min?: string(name='Min', description='The lower limit of the total bitrate. Unit: Kbit/s.', example='10'),
          }(name='BitrateBnd', description='The average bitrate range of the video.'),
          bufsize?: string(name='Bufsize', description='The size of the buffer.

*   Unit: KB.
*   Default value: **6000**.', example='5000'),
          codec?: string(name='Codec', description='The video codec format. Default value: **H.264**.', example='H.264'),
          crf?: string(name='Crf', description='The constant rate factor, which is the default quality control setting.

*   Default value when the Codec parameter is set to H.264: **23**. Default value when the Codec parameter is set to H.265: **26**.
*   If this parameter is returned, the setting of the Bitrate parameter is invalid.', example='27'),
          degrain?: string(name='Degrain', description='The level of the independent denoising algorithm.', example='5'),
          fps?: string(name='Fps', description='The frame rate.

*   The value is 60 if the frame rate of the input file exceeds 60.
*   Default value: **the frame rate of the input file**.', example='60'),
          gop?: string(name='Gop', description='The maximum number of frames between two keyframes. Default value: **250**.', example='1'),
          height?: string(name='Height', description='The height of the video.

*   Unit: pixel.
*   Default value: **the original height of the video**.', example='1880'),
          maxrate?: string(name='Maxrate', description='The maximum video bitrate. Unit: Kbit/s.', example='10'),
          pixFmt?: string(name='PixFmt', description='The pixel format for video color encoding. Valid values: standard pixel formats such as yuv420p and yuvj420p.', example='yuvj420p'),
          preset?: string(name='Preset', description='The preset video algorithm. Valid values: veryfast, fast, medium, slow, and slower. Default value: **medium**.', example='medium'),
          profile?: string(name='Profile', description='The codec profile. Valid values:

*   **baseline**: applicable to mobile devices.
*   **main**: applicable to standard-definition devices.
*   **high**: applicable to high-definition devices.
*   Default value: **high**.', example='baseline'),
          qscale?: string(name='Qscale', description='The level of quality control on the video.', example='15'),
          scanMode?: string(name='ScanMode', description='The scan mode. Valid values:

*   **interlaced**: An interlaced scan was performed.
*   **progressive**: A progressive scan was performed.', example='progressive'),
          width?: string(name='Width', description='The width of the video.

*   Unit: pixel.
*   Default value: **the original width of the video**.', example='1990'),
        }(name='Video', description='The video codec configuration.'),
      }
    ](name='Template')
    }(name='TemplateList', description='The list of matched preset templates.'),
    userData?: string(name='UserData', description='The custom data.', example='testid-001'),
  }(name='AnalysisJob', description='The information about the preset template analysis job that was submitted.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='B52658D4-07AB-43CD-82B0-210958A65E23'),
}

model SubmitAnalysisJobResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: SubmitAnalysisJobResponseBody(name='body'),
}

/**
  * *   After you call the SubmitAnalysisJob operation to submit a preset template analysis job, ApsaraVideo Media Processing (MPS) intelligently analyzes the input file of the job and recommends a suitable preset template. You can call the [QueryAnalysisJobList](~~29224~~) operation to query the analysis result or enable asynchronous notifications to receive the analysis result.
  * *   The analysis result is retained only for two weeks since it is generated. It is deleted after two weeks. If you use the recommended preset template in a transcoding job after two weeks, the job fails, and the `AnalysisResultNotFound` error code is returned.
  * ## QPS limit
  * You can call this operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request SubmitAnalysisJobRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return SubmitAnalysisJobResponse
 */
async function submitAnalysisJobWithOptions(request: SubmitAnalysisJobRequest, runtime: Util.RuntimeOptions): SubmitAnalysisJobResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.analysisConfig)) {
    query['AnalysisConfig'] = request.analysisConfig;
  }
  if (!Util.isUnset(request.input)) {
    query['Input'] = request.input;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.pipelineId)) {
    query['PipelineId'] = request.pipelineId;
  }
  if (!Util.isUnset(request.priority)) {
    query['Priority'] = request.priority;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.userData)) {
    query['UserData'] = request.userData;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'SubmitAnalysisJob',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * *   After you call the SubmitAnalysisJob operation to submit a preset template analysis job, ApsaraVideo Media Processing (MPS) intelligently analyzes the input file of the job and recommends a suitable preset template. You can call the [QueryAnalysisJobList](~~29224~~) operation to query the analysis result or enable asynchronous notifications to receive the analysis result.
  * *   The analysis result is retained only for two weeks since it is generated. It is deleted after two weeks. If you use the recommended preset template in a transcoding job after two weeks, the job fails, and the `AnalysisResultNotFound` error code is returned.
  * ## QPS limit
  * You can call this operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request SubmitAnalysisJobRequest
  * @return SubmitAnalysisJobResponse
 */
async function submitAnalysisJob(request: SubmitAnalysisJobRequest): SubmitAnalysisJobResponse {
  var runtime = new Util.RuntimeOptions{};
  return submitAnalysisJobWithOptions(request, runtime);
}

model SubmitFpDBDeleteJobRequest {
  delType?: string(name='DelType', description='The operation type. Valid values:

*   **Purge**: clears the media fingerprint library. If you set the DelType parameter to Purge, the content in the library is deleted, but the library is not deleted.
*   **Delete**: deletes the media fingerprint library. If you set the DelType parameter to Delete, both the library and the content in the library are deleted.
*   Default value: **Purge**.', example='Purge'),
  fpDBId?: string(name='FpDBId', description='The ID of the media fingerprint library. You can obtain the library ID from the response parameters of the [CreateFpShotDB](~~170149~~) operation.', example='88c6ca184c0e47098a5b665e2a12****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  pipelineId?: string(name='PipelineId', description='The ID of the ApsaraVideo Media Processing (MPS) queue. This ID can be used to associate the job with a notification method. To view the MPS queue ID, log on to the **MPS console** and choose **Global Settings** > **Pipelines** in the left-side navigation pane.', example='fb712a6890464059b1b2ea7c8647****'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  userData?: string(name='UserData', description='The custom data. The value can contain letters and digits and can be up to 128 bytes in length.', example='example data'),
}

model SubmitFpDBDeleteJobResponseBody = {
  jobId?: string(name='JobId', description='The ID of the job. We recommend that you keep this ID for subsequent operation calls.', example='d98459323c024947a104f6a50cbf****'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='4247B23C-26DE-529F-8D9F-FD6811AE979B'),
}

model SubmitFpDBDeleteJobResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: SubmitFpDBDeleteJobResponseBody(name='body'),
}

/**
  * You can call this operation to clear or delete a specified media fingerprint library based on the library ID. If you clear a media fingerprint library, the content in the library is deleted, but the library is not deleted. If you delete a media fingerprint library, both the library and the content in the library are deleted. If you do not specify the operation type, the system clears the media fingerprint library by default.
  * ## QPS limit
  * You can call this operation up to 150 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request SubmitFpDBDeleteJobRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return SubmitFpDBDeleteJobResponse
 */
async function submitFpDBDeleteJobWithOptions(request: SubmitFpDBDeleteJobRequest, runtime: Util.RuntimeOptions): SubmitFpDBDeleteJobResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.delType)) {
    query['DelType'] = request.delType;
  }
  if (!Util.isUnset(request.fpDBId)) {
    query['FpDBId'] = request.fpDBId;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.pipelineId)) {
    query['PipelineId'] = request.pipelineId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.userData)) {
    query['UserData'] = request.userData;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'SubmitFpDBDeleteJob',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * You can call this operation to clear or delete a specified media fingerprint library based on the library ID. If you clear a media fingerprint library, the content in the library is deleted, but the library is not deleted. If you delete a media fingerprint library, both the library and the content in the library are deleted. If you do not specify the operation type, the system clears the media fingerprint library by default.
  * ## QPS limit
  * You can call this operation up to 150 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request SubmitFpDBDeleteJobRequest
  * @return SubmitFpDBDeleteJobResponse
 */
async function submitFpDBDeleteJob(request: SubmitFpDBDeleteJobRequest): SubmitFpDBDeleteJobResponse {
  var runtime = new Util.RuntimeOptions{};
  return submitFpDBDeleteJobWithOptions(request, runtime);
}

model SubmitFpFileDeleteJobRequest {
  fileIds?: string(name='FileIds', description='The IDs of the media files that you want to delete. Separate multiple file IDs with commas (,). You can delete up to 200 media files at a time. You can obtain the file IDs from the response parameters of the [ListFpShotFiles](~~209266~~) operation.', example='41e6536e4f2250e2e9bf26cdea19****'),
  fpDBId?: string(name='FpDBId', description='The ID of the media fingerprint library. You can obtain the library ID from the response parameters of the [CreateFpShotDB](~~170149~~) operation.', example='88c6ca184c0e432bbf5b665e2a15****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  pipelineId?: string(name='PipelineId', description='The ID of the ApsaraVideo Media Processing (MPS) queue. This ID can be used to associate the job with a notification method. To view the MPS queue ID, log on to the **MPS console** and choose **Global Settings** > **Pipelines** in the left-side navigation pane.', example='ed450ea0bfbd41e29f80a401fb4d****'),
  primaryKeys?: string(name='PrimaryKeys', example='24e0fba7188fae707e146esa54****'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  userData?: string(name='UserData', description='The custom data. The value can contain letters and digits and can be up to 128 bytes in length.', example='example data'),
}

model SubmitFpFileDeleteJobResponseBody = {
  jobId?: string(name='JobId', description='The ID of the deleted media file.', example='39f8e0bc005e4f309379701645f4****'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='D127C68E-F1A1-4CE5-A874-8FF724881A12'),
}

model SubmitFpFileDeleteJobResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: SubmitFpFileDeleteJobResponseBody(name='body'),
}

/**
  * You can call this operation to delete up to 200 media files from a media fingerprint library at a time.
  * ## QPS limit
  * You can call this operation up to 150 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request SubmitFpFileDeleteJobRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return SubmitFpFileDeleteJobResponse
 */
async function submitFpFileDeleteJobWithOptions(request: SubmitFpFileDeleteJobRequest, runtime: Util.RuntimeOptions): SubmitFpFileDeleteJobResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.fileIds)) {
    query['FileIds'] = request.fileIds;
  }
  if (!Util.isUnset(request.fpDBId)) {
    query['FpDBId'] = request.fpDBId;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.pipelineId)) {
    query['PipelineId'] = request.pipelineId;
  }
  if (!Util.isUnset(request.primaryKeys)) {
    query['PrimaryKeys'] = request.primaryKeys;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.userData)) {
    query['UserData'] = request.userData;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'SubmitFpFileDeleteJob',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * You can call this operation to delete up to 200 media files from a media fingerprint library at a time.
  * ## QPS limit
  * You can call this operation up to 150 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request SubmitFpFileDeleteJobRequest
  * @return SubmitFpFileDeleteJobResponse
 */
async function submitFpFileDeleteJob(request: SubmitFpFileDeleteJobRequest): SubmitFpFileDeleteJobResponse {
  var runtime = new Util.RuntimeOptions{};
  return submitFpFileDeleteJobWithOptions(request, runtime);
}

model SubmitFpShotJobRequest {
  fpShotConfig?: string(name='FpShotConfig', description='The configurations of the media fingerprint analysis job. The value is a JSON object. For more information, see the "FpShotConfig" section of the [Parameter details](~~93568~~) topic.', example='{“PrimaryKey”:”12345****”, “SaveType”: “save”}'),
  input?: string(name='Input', description='The object Storage Service (OSS) URL of the job input. The value is a JSON object. You can query the OSS URL in the OSS or ApsaraVideo Media Processing (MPS) console. For more information, see the "InputFile" section of the [Parameter details](~~93568~~) topic.

>  The OSS bucket must reside in the same region as the specified MPS region.', example='{“Bucket”:”example-bucket-****”,“Location”:”oss-cn-shanghai”,“Object”:”example-****.flv”}'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  pipelineId?: string(name='PipelineId', description='The ID of the MPS queue. This ID can be used to associate the job with a notification method. To view the MPS queue ID, log on to the **MPS console** and choose **Global Settings** > **Pipelines** in the left-side navigation pane.', example='88c6ca184c0e47098a5b665e2a12****'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  userData?: string(name='UserData', description='The custom data. The value can be up to 128 bytes in length and cannot start with a special character.', example='testid-****'),
}

model SubmitFpShotJobResponseBody = {
  jobId?: string(name='JobId', description='The ID of the media fingerprint analysis job. We recommend that you keep this ID for subsequent operation calls.', example='2a0697e35a7342859f733a9190c4****'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='25818875-5F78-4A13-BEF6-D7393642CA58'),
}

model SubmitFpShotJobResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: SubmitFpShotJobResponseBody(name='body'),
}

/**
  * *   You can call this operation to submit a video or text fingerprint analysis job.
  * *   This operation asynchronously submits a job. The query results may not have been generated when the response is returned. After the results are generated, an asynchronous message is returned.
  * *   You can submit a text fingerprint analysis job only in the China (Shanghai) region. The input file of the job must be in one of the following formats:
  *     *   Image formats: JPEG, PNG, and BMP.
  *     *   Video formats: MP4, AVI, MKV, MPG, TS, MOV, FLV, MXF.
  *     *   Video encoding formats: MPEG2, MPEG4, H264, HEVC, and WMV.
  * ## QPS limit
  * You can call this operation up to 150 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request SubmitFpShotJobRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return SubmitFpShotJobResponse
 */
async function submitFpShotJobWithOptions(request: SubmitFpShotJobRequest, runtime: Util.RuntimeOptions): SubmitFpShotJobResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.fpShotConfig)) {
    query['FpShotConfig'] = request.fpShotConfig;
  }
  if (!Util.isUnset(request.input)) {
    query['Input'] = request.input;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.pipelineId)) {
    query['PipelineId'] = request.pipelineId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.userData)) {
    query['UserData'] = request.userData;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'SubmitFpShotJob',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * *   You can call this operation to submit a video or text fingerprint analysis job.
  * *   This operation asynchronously submits a job. The query results may not have been generated when the response is returned. After the results are generated, an asynchronous message is returned.
  * *   You can submit a text fingerprint analysis job only in the China (Shanghai) region. The input file of the job must be in one of the following formats:
  *     *   Image formats: JPEG, PNG, and BMP.
  *     *   Video formats: MP4, AVI, MKV, MPG, TS, MOV, FLV, MXF.
  *     *   Video encoding formats: MPEG2, MPEG4, H264, HEVC, and WMV.
  * ## QPS limit
  * You can call this operation up to 150 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request SubmitFpShotJobRequest
  * @return SubmitFpShotJobResponse
 */
async function submitFpShotJob(request: SubmitFpShotJobRequest): SubmitFpShotJobResponse {
  var runtime = new Util.RuntimeOptions{};
  return submitFpShotJobWithOptions(request, runtime);
}

model SubmitIProductionJobRequest {
  functionName?: string(name='FunctionName', example='ImageCartoonize'),
  input?: string(name='Input', example='oss://example-****.oss-cn-shanghai.aliyuncs.com/example.mp4'),
  jobParams?: string(name='JobParams', example='{"Model":"gif"}'),
  modelId?: string(name='ModelId', example='null'),
  notifyUrl?: string(name='NotifyUrl', example='mns://125340688170****.oss-cn-shanghai.aliyuncs.com/queues/example-pipeline'),
  output?: string(name='Output', example='oss://example-****.oss-cn-shanghai.aliyuncs.com/iproduction/{source}-{timestamp}-{sequenceId}.srt'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  pipelineId?: string(name='PipelineId', example='39f8e0bc005e4f309379701645f4****'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  scheduleParams?: string(name='ScheduleParams', example='null'),
  userData?: string(name='UserData', example='null'),
}

model SubmitIProductionJobResponseBody = {
  jobId?: string(name='JobId', example='39f8e0bc005e4f309379701645f4****'),
  requestId?: string(name='RequestId', example='5210DBB0-E327-4D45-ADBC-0B83C8796E26'),
  result?: string(name='Result', example='{ "Code":"Success", "Details":[], "FunctionName":"ImageCartoonize", "JobId":"39f8e0bc005e4f309379701645f4****", "Message":"success", "State":"Success", "Type":"IProduction" }'),
}

model SubmitIProductionJobResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: SubmitIProductionJobResponseBody(name='body'),
}

async function submitIProductionJobWithOptions(request: SubmitIProductionJobRequest, runtime: Util.RuntimeOptions): SubmitIProductionJobResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.functionName)) {
    query['FunctionName'] = request.functionName;
  }
  if (!Util.isUnset(request.input)) {
    query['Input'] = request.input;
  }
  if (!Util.isUnset(request.jobParams)) {
    query['JobParams'] = request.jobParams;
  }
  if (!Util.isUnset(request.modelId)) {
    query['ModelId'] = request.modelId;
  }
  if (!Util.isUnset(request.notifyUrl)) {
    query['NotifyUrl'] = request.notifyUrl;
  }
  if (!Util.isUnset(request.output)) {
    query['Output'] = request.output;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.pipelineId)) {
    query['PipelineId'] = request.pipelineId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.scheduleParams)) {
    query['ScheduleParams'] = request.scheduleParams;
  }
  if (!Util.isUnset(request.userData)) {
    query['UserData'] = request.userData;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'SubmitIProductionJob',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

async function submitIProductionJob(request: SubmitIProductionJobRequest): SubmitIProductionJobResponse {
  var runtime = new Util.RuntimeOptions{};
  return submitIProductionJobWithOptions(request, runtime);
}

model SubmitJobsRequest {
  input?: string(name='Input', description='The input of the jobs. Specify the value in a JSON object. Referer is an optional parameter. If you need to set a referer whitelist in a transcoding job, you must set the Referer parameter in the input. For more information about the fields in the Input parameter, see [Input](~~29253~~).

>  You can specify the `{ObjectPrefix}`, `{FileName}`, and `{ExtName}` variables in an OSS URL. MPS can dynamically replace the variables with the actual values. For example, if you specify `a/b/c/test.flv` as the object to be used as the input file and `{ObjectPrefix}{FileName}-cn.srt` as the object to be used as the external subtitle file, MPS replaces the variables to obtain the OSS URL a/b/c/test-cn.srt of the external subtitle file. The OSS URL that you specify must be URL-encoded. Therefore, you must specify the OSS URL of the external subtitle file as `%7bObjectPrefix%7d%7bFileName%7d-cn.srt`. Therefore, you must specify the OSS URL of the external subtitle file as `a/b/c/test-cn.srt`.', example='a/b/c/test-cn.srt'),
  outputBucket?: string(name='OutputBucket', description='The name of the output bucket. To obtain the name of an output bucket, you can log on to the [MPS console](https://mps.console.aliyun.com/overview) and choose **Workflows** > **Media Buckets** in the left-side navigation pane.', example='exampleBucket'),
  outputLocation?: string(name='OutputLocation', description='The region in which the output bucket resides.', example='oss-cn-hangzhou'),
  outputs?: string(name='Outputs', description='The outputs of the jobs.

*   Specify the value in a JSON array of Output objects. You can specify up to 30 Output objects.
*   For more information about job outputs, see [Terms](~~29212~~).
*   For more information about the fields in each Output object, see [Output](~~29253~~).

>  If you need to merge images in a transcoding job, set the width and height of each image to be processed to even numbers. Otherwise, the job may fail.', example='[{"OutputObject":"exampleOutput.mp4","TemplateId":"6181666213ab41b9bc21da8ff5ff****","WaterMarks":[{"InputFile":{"Bucket":"exampleBucket","Location":"oss-cn-hangzhou","Object":"image_01.png"},"WaterMarkTemplateId":"9b772ce2740d4d55876d8b542d47****"}],"UserData":"testid-001"}]'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  pipelineId?: string(name='PipelineId', description='The ID of the MPS queue. To obtain the ID of an MPS queue, you can log on to the [MPS console](https://mps.console.aliyun.com/overview) and choose **Global Settings** > **Pipelines** in the left-side navigation pane.

*   For more information about MPS queues, see [Terms](~~29212~~).
*   If you want to use asynchronous notifications, make sure that an MNS topic is bound to the MPS queue to be used. For more information about how to bind an MNS topic to an MPS queue, see [Enable the feature for sending notifications on transcoding jobs](~~51469~~).', example='dd3dae411e704030b921e52698e5****'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model SubmitJobsResponseBody = {
  jobResultList?: {
    jobResult?: [ 
    {
      code?: string(name='Code', description='The error code returned if the job fails to be created. This parameter is not returned if the job is created.', example='InvalidParameter.NullValue'),
      job?: {
        code?: string(name='Code', description='The error code returned if the job failed. This parameter is not returned if the job is successful.', example='InternalError'),
        creationTime?: string(name='CreationTime', description='The time when the job was created.', example='2014-01-10T12:00:00Z'),
        finishTime?: string(name='FinishTime', description='The time when the job was complete.', example='2014-01-10T12:20:00Z'),
        input?: {
          bucket?: string(name='Bucket', description='The name of the OSS bucket in which the job input is stored.', example='example-bucket-****'),
          location?: string(name='Location', description='The ID of the OSS region in which the job input resides.', example='oss-cn-hangzhou'),
          object?: string(name='Object', description='The name of the OSS object that is used as the job input.', example='example.flv'),
        }(name='Input', description='The information about the job input.'),
        jobId?: string(name='JobId', description='The ID of the job.', example='31fa3c9ca8134f9cec2b4b0b0f78****'),
        MNSMessageResult?: {
          errorCode?: string(name='ErrorCode', description='The error code returned if the job failed. This parameter is not returned if the job is successful.', example='InvalidParameter.ResourceNotFound'),
          errorMessage?: string(name='ErrorMessage', description='The error message returned if the job failed. This parameter is not returned if the job is successful.', example='The resource operated "%s" cannot be found.'),
          messageId?: string(name='MessageId', description='The ID of the error message returned if the job failed. This parameter is not returned if the job is successful.', example='123'),
        }(name='MNSMessageResult', description='The message sent by MNS to notify users of the job result.'),
        message?: string(name='Message', description='The error message returned if the job failed. This parameter is not returned if the job is successful.', example='The operation has failed due to some unknown error, exception or failure.'),
        output?: {
          amixList?: {
            amix?: [ 
            {
              amixURL?: string(name='AmixURL', description='The URL of the audio track to be mixed as the background music. 

- The URL can be an OSS URL or the value of the `Input` parameter in the form of a string.
- If you want to mix two audio tracks in a video, set this parameter to the value of the Input parameter in the form of a string.', example='https://outpu***.oss-cn-shanghai.aliyuncs.com/mp4-to-mp3%5E1571025263578816%40.mp3'),
              duration?: string(name='Duration', description='The duration of the audio track. The value is in the number or time format.', example='20'),
              map?: string(name='Map', description='The audio track to be mixed. Format: 0:a:{audio_index}. Example: 0:a:0.', example='0:a:0'),
              mixDurMode?: string(name='MixDurMode', description='The mode to specify the mixing duration. Valid values: **first** and **long**.  

- **first**: The length of the output media equals the length of the input media.
- **long**: The length of the output media equals the length of the output media or the length of the input media, whichever is longer.
- Default value: **long**.', example='long'),
              start?: string(name='Start', description='The start point in time of the audio track. The value is in the number or time format. Examples: 1:25:36.240 and 32000.23.', example='0'),
            }
          ](name='Amix')
          }(name='AmixList', description='The list of audio tracks to be mixed.'),
          audio?: {
            bitrate?: string(name='Bitrate', description='The audio bitrate of the output file.

*   Unit: Kbit/s.
*   Default value: **128**.', example='128'),
            channels?: string(name='Channels', description='The number of sound channels.

*   If the value of the Codec parameter is mp3, the value of this parameter can only be **1** or **2**.
*   If the value of the Codec parameter is aac, the value of this parameter can only be **1**, **2**, **4**, **5**, **6**, or **8**.
*   Default value: **2**.', example='6'),
            codec?: string(name='Codec', description='The audio codec.

*   Valid values: **aac**, **mp3**, **vorbis**, and **flac**.
*   Default value: **aac**.', example='aac'),
            profile?: string(name='Profile', description='The codec profile of the audio.

>  Valid values when the value of the **Codec** parameter is **aac**: **aac_low**, **aac_he**, **aac_he_v2**, **aac_ld**, and **aac_eld**.', example='aac_low'),
            qscale?: string(name='Qscale', description='The level of quality control on the audio.', example='15'),
            samplerate?: string(name='Samplerate', description='The sampling rate.

*   Valid values: **22050**, **32000**, **44100**, **48000**, and **96000**.
*   Unit: Hz.
*   Default value: **44100**.

>  If the video container format is FLV and the audio codec is MP3, the sampling rate cannot be 32000, 48000, or 96000. If the audio codec is MP3, the sampling rate cannot be 96000.', example='32000'),
            volume?: {
              level?: string(name='Level', description='The volume adjustment range.

*   Unit: dB.
*   Default value: **-20db**.', example='-20'),
              method?: string(name='Method', description='The volume adjustment method. Valid values:

*   **auto**: The volume is automatically adjusted.
*   **dynamic**: The volume is dynamically adjusted.
*   **linear**: The volume is linearly adjusted.', example='auto'),
            }(name='Volume', description='The volume configurations.'),
          }(name='Audio', description='The audio configurations.

>  If this parameter is specified in the request, the value overwrites the corresponding parameter in the specified transcoding template.'),
          audioStreamMap?: string(name='AudioStreamMap', description='The sequence number of the audio stream.

*   Format: 0:a:{Sequence number}. Example: 0:a:0.
*   The sequence number is the index of the audio stream in the list and starts from 0.
*   If you do not specify a sequence number, the default audio stream is used.', example='0:a:0'),
          clip?: {
            timeSpan?: {
              duration?: string(name='Duration', description='The duration of the clip. 

- Format: `hh:mm:ss[.SSS]` or `sssss[.SSS]`.
- Valid values: `[00:00:00.000,23:59:59.999]` or `[0.000,86399.999]`.', example='01:00:59.999'),
              seek?: string(name='Seek', description='The time when the clip starts. 

- Format: `hh:mm:ss[.SSS]` or `sssss[.SSS]`.
- Valid values: `[00:00:00.000,23:59:59.999]` or `[0.000,86399.999]`.', example='01:59:59.999'),
            }(name='TimeSpan', description='The time span of the clip.'),
          }(name='Clip', description='The information about the clip.'),
          container?: {
            format?: string(name='Format', description='The container format. 

- Default value: **mp4**.
- Video formats include FLV, MP4, HLS (M3U8 + TS), and MPEG-DASH (MPD + fMP4).
- Audio formats include MP3, MP4, Ogg, FLAC, and M4A.
- Image formats include GIF and WebP.
- If the container format is GIF, the video codec must be GIF.
- If the container format is WebP, the video codec must be WebP.
- If the container format is FLV, the video codec cannot be H.265.', example='flv'),
          }(name='Container', description='The container configurations.'),
          deWatermark?: string(name='DeWatermark', description='The configurations of watermark blurring. The value is a JSON object. For more information, see **DeWatermark** in [Parameter details](~~29253~~).', example='{"0": [{"l": 10,"t": 10,"w": 10,"h": 10},{"l": 100,"t": 0.1,"w": 10,"h": 10}],"128000": [],"250000": [{"l": 0.2,"t": 0.1,"w": 0.01,"h": 0.05}]}'),
          digiWaterMark?: {
            alpha?: string(name='Alpha', description='The transparency of the text or image. 

- Value values: **(0,1]**.
- Default value: **1.0**.', example='1.0'),
            inputFile?: {
              bucket?: string(name='Bucket', description='The name of the OSS bucket in which the output file is stored.', example='example-bucket-****'),
              location?: string(name='Location', description='The ID of the OSS region in which the input file resides.', example='oss-cn-hangzhou'),
              object?: string(name='Object', description='The name of the OSS object that is used as the input file.', example='example-intput.flv'),
            }(name='InputFile', description='The input file.'),
            type?: string(name='Type', description='The type of the watermark. If this parameter is specified in the request, this value overwrites the corresponding parameter in the watermark template. Valid values: 

- **Image**: an image watermark. This is the default value.
- **Text**: a text watermark.', example='Image'),
          }(name='DigiWaterMark', description='The digital watermarks.'),
          encryption?: {
            id?: string(name='Id', description='The encryption ID.', example='31fa3c9ca8134f9cec2b4b0b0f78****'),
            key?: string(name='Key', description='The key that is used to encrypt the video.', example='encryptionkey128'),
            keyType?: string(name='KeyType', description='The key encryption method. Keys cannot be transmitted to MPS in plaintext. Keys must be encrypted by using Base64 or Key Management Service (KMS). For example, if the key is encryptionkey128, you can encrypt the key by using the following method: Base64("encryptionkey128") or KMS(Base64("encryptionkey128").', example='Base64'),
            keyUri?: string(name='KeyUri', description='The URL that is used to request the key. The URL is Base64-encoded.', example='https://1161758785*****.cn-shanghai.fc.aliyuncs.com/2016-08-15/proxy/HLS-decyptServer/decyptServer/'),
            skipCnt?: string(name='SkipCnt', description='The number of unencrypted frames at the beginning of the video. Leaving these frames unencrypted enables video playback to quickly start.', example='3'),
            type?: string(name='Type', description='The encryption type. The value is **hls-aes-128**.', example='hls-aes-128'),
          }(name='Encryption', description='The encryption configuration. Only outputs in the M3U8 format are supported.'),
          m3U8NonStandardSupport?: {
            TS?: {
              md5Support?: boolean(name='Md5Support', description='Indicates whether to support the output of the MD5 value of the TS file in the M3U8 video. Valid values:

*   **true**: supported.
*   **false**: not supported.', example='true'),
              sizeSupport?: boolean(name='SizeSupport', description='Indicates whether to support the output of the size of the TS file in the M3U8 video. Valid values:

*   **true**: supported.
*   **false**: not supported.', example='true'),
            }(name='TS', description='The non-standard support configuration for TS files. The value is a JSON object. For more information, see **TS** in [Parameter details](~~29253~~).'),
          }(name='M3U8NonStandardSupport', description='The non-standard support configuration for M3U8. The value is a JSON object. For more information, see **M3U8NonStandardSupport** in [Parameter details](~~29253~~).'),
          mergeConfigUrl?: string(name='MergeConfigUrl', description='You can specify either the **MergeList** or **MergeConfigUrl** parameter.  

- The configuration file that you specify by using the MergeConfigUrl parameter can contain up to 50 merged clips.
- The MergeConfigUrl parameter specifies the URL of the configuration file for merging clips.
- Make sure that the configuration file is stored as an object in OSS and that MPS can access the OSS object. For information about the file content, see the details about merging parameters.
- Example of the content of mergeConfigfile: `{"MergeList":[{"MergeURL":"http://exampleBucket****.oss-cn-hangzhou.aliyuncs.com/video_01.mp4"}]}`.', example='`{"MergeList":[{"MergeURL":"http://exampleBucket****.oss-cn-hangzhou.aliyuncs.com/video_01.mp4"}]}'),
          mergeList?: {
            merge?: [ 
            {
              duration?: string(name='Duration', description='The duration of the clip.

*   Format: `hh:mm:ss[.SSS]` or `sssss[.SSS]`.
*   Examples: 01:59:59.999 and 32000.23.', example='00000.20'),
              mergeURL?: string(name='MergeURL', description='The OSS URL of the clip.

*   Example: `http://example-bucket-****.oss-cn-hangzhou.aliyuncs.com/example-object-****.flv`.
*   The object must be URL-encoded by using the UTF-8 standard.', example='http://example-bucket-****.oss-cn-hangzhou.aliyuncs.com/example-object-****.flv'),
              roleArn?: string(name='RoleArn', description='The Alibaba Cloud Resource Name (ARN) of the role used for delegated authorization.', example='acs:ram::<your uid>:role/<your role name>'),
              start?: string(name='Start', description='The start point in time of the clip.

*   Format: `hh:mm:ss[.SSS]` or `sssss[.SSS]`.
*   Examples: 01:59:59.999 and 32000.23.', example='00000.50'),
            }
          ](name='Merge')
          }(name='MergeList', description='The configurations for merging clips.'),
          muxConfig?: {
            gif?: {
              ditherMode?: string(name='DitherMode', description='The color dithering algorithm of the palette. Valid values: **sierra** and **bayer**.', example='bayer'),
              finalDelay?: string(name='FinalDelay', description='The duration for which the final frame is paused. Unit: centiseconds.', example='0'),
              isCustomPalette?: string(name='IsCustomPalette', description='Indicates whether the custom palette is used. Valid values:

- **true**: The custom palette is used.
- **false**: The custom palette is not used.', example='false'),
              loop?: string(name='Loop', description='The loop count.', example='0'),
            }(name='Gif', description='The transmuxing configuration for GIF.'),
            segment?: {
              duration?: string(name='Duration', description='The length of the segment. The value is an integer. Unit: seconds. 

- Valid values: **[1,10]**.
- Default value: **10**.', example='20'),
            }(name='Segment', description='The segment configurations. The value is a JSON object.'),
            webp?: {
              loop?: string(name='Loop', description='The loop count.', example='0'),
            }(name='Webp', description='The transmuxing configuration for WebP.'),
          }(name='MuxConfig', description='The transmuxing configurations. If this parameter is specified in the request, the value overwrites the corresponding parameter in the specified transcoding template.'),
          openingList?: {
            opening?: [ 
            {
              height?: string(name='Height', description='The height of the opening part. 

- Valid values: **0 to 4096**, **-1**, and **full**.
- Default value: **-1**.
- A value of **-1** indicates that the height of the source of the opening part is retained.
- A value of **full** indicates that the height of the main part is used for the opening part.', example='1080'),
              start?: string(name='Start', description='The amount of time after which the opening part is played. The value starts from 0. 

- Unit: seconds.
- Default value: **0**.', example='1'),
              width?: string(name='Width', description='The width of the opening part. 

- Valid values: **0 to 4096**, **-1**, and **full**.
- Default value: **-1**.
- A value of **-1** indicates that the height of the source of the opening part is retained.
- A value of **full** indicates that the height of the main part is used for the opening part.', example='1920'),
              openUrl?: string(name='openUrl', description='The OSS URL of the opening part of the video.', example='http://example-bucket-****.oss-cn-hangzhou.aliyuncs.com/opening_01.flv'),
            }
          ](name='Opening')
          }(name='OpeningList', description='The list of opening parts. The value is a JSON object.'),
          outSubtitleList?: {
            outSubtitle?: [ 
            {
              map?: string(name='Map', description='The video stream. Format: `0:{Stream}:{Stream sequence number}`, which is `0:v:{video_index}`. The value of Stream is v, which indicates a video stream. The sequence number is the index of the video stream in the list and starts from 0.', example='0:v:0'),
              message?: string(name='Message', description='The error message returned if the job fails to be created. This parameter is not returned if the job is created.', example='The specified parameter “%s” cannot be null.'),
              outSubtitleFile?: {
                bucket?: string(name='Bucket', description='The name of the OSS bucket in which the output file is stored.', example='example-bucket-****'),
                location?: string(name='Location', description='The ID of the OSS region in which the output file resides.', example='oss-cn-hangzhou'),
                object?: string(name='Object', description='The name of the OSS object that is used as the output file.', example='example-output.flv'),
                roleArn?: string(name='RoleArn', description='The ARN of the role used for delegated authorization.', example='acs:ram::<your uid>:role/<your role name>'),
              }(name='OutSubtitleFile', description='The details of the output file.'),
              success?: boolean(name='Success', description='Indicates whether the job is created. Valid values:

*   **true**: The job is created.
*   **false**: The job fails to be created.', example='true'),
            }
          ](name='OutSubtitle')
          }(name='OutSubtitleList', description='The output subtitle list.'),
          outputFile?: {
            bucket?: string(name='Bucket', description='The name of the OSS bucket in which the output file is stored.', example='example-bucket-****'),
            location?: string(name='Location', description='The ID of the OSS region in which the output file resides.', example='oss-cn-hangzhou'),
            object?: string(name='Object', description='The name of the OSS object that is used as the output file.', example='example-output.flv'),
            roleArn?: string(name='RoleArn', description='The ARN of the role used for delegated authorization.', example='acs:ram::<your uid>:role/<your role name>'),
          }(name='OutputFile', description='The details of the output file.'),
          priority?: string(name='Priority', description='The priority of the job in the MPS queue to which the job is added. 

- A value of **10** indicates the highest priority.
- Default value: **6**.', example='5'),
          properties?: {
            bitrate?: string(name='Bitrate', description='The bitrate of the video.', example='1000'),
            duration?: string(name='Duration', description='The duration of the video.', example='55'),
            fileFormat?: string(name='FileFormat', description='The format of the video.', example='QuickTime / MOV'),
            fileSize?: string(name='FileSize', description='The size of the file.', example='3509895'),
            format?: {
              bitrate?: string(name='Bitrate', description='The total bitrate.', example='1000'),
              duration?: string(name='Duration', description='The total duration.', example='55'),
              formatLongName?: string(name='FormatLongName', description='The full name of the container format.', example='QuickTime / MOV'),
              formatName?: string(name='FormatName', description='The short name of the container format. Valid values: mov, mp4, m4a, 3gp, 3g2, and mj2.', example='mov'),
              numPrograms?: string(name='NumPrograms', description='The total number of program streams.', example='0'),
              numStreams?: string(name='NumStreams', description='The total number of media streams.', example='2'),
              size?: string(name='Size', description='The size of the file.', example='3509895'),
              startTime?: string(name='StartTime', description='The start time.', example='0.000000'),
            }(name='Format', description='The format information.'),
            fps?: string(name='Fps', description='The frame rate of the video. The value is a number.', example='25'),
            height?: string(name='Height', description='The length of the video.', example='720'),
            streams?: {
              audioStreamList?: {
                audioStream?: [ 
                {
                  bitrate?: string(name='Bitrate', description='The bitrate of the audio stream.', example='128.806'),
                  channelLayout?: string(name='ChannelLayout', description='The output layout of the sound channels.', example='stereo'),
                  channels?: string(name='Channels', description='The number of sound channels.', example='2'),
                  codecLongName?: string(name='CodecLongName', description='The full name of the encoding format.', example='AAC (Advanced Audio Coding)'),
                  codecName?: string(name='CodecName', description='The short name of the encoding format.', example='aac'),
                  codecTag?: string(name='CodecTag', description='The tag of the encoding format.', example='0x6134706d'),
                  codecTagString?: string(name='CodecTagString', description='The tag string of the encoding format.', example='mp4a'),
                  codecTimeBase?: string(name='CodecTimeBase', description='The codec time base.', example='1/44100'),
                  duration?: string(name='Duration', description='The duration of the audio stream.', example='17.159546'),
                  index?: string(name='Index', description='The sequence number of the audio stream. The value indicates the position of the audio stream in all audio streams.', example='1'),
                  lang?: string(name='Lang', description='The language of the audio stream. For more information, see [FFmpeg language definition](https://www.ffmpeg.org/ffmpeg-all.html#Metadata) and [ISO 639](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes).', example='eng'),
                  numFrames?: string(name='NumFrames', description='The total number of frames.', example='25'),
                  sampleFmt?: string(name='SampleFmt', description='The sampling format of the audio stream.', example='fltp'),
                  samplerate?: string(name='Samplerate', description='The sampling rate of the audio stream.', example='44100'),
                  startTime?: string(name='StartTime', description='The start point in time of the audio stream.', example='0.000000'),
                  timebase?: string(name='Timebase', description='The time base of the audio stream.', example='1/44100'),
                }
              ](name='AudioStream')
              }(name='AudioStreamList', description='The list of audio streams.'),
              subtitleStreamList?: {
                subtitleStream?: [ 
                {
                  index?: string(name='Index', description='The sequence number of the subtitle stream. The value indicates the position of the subtitle stream in all subtitle streams.', example='1'),
                  lang?: string(name='Lang', description='The language of the subtitle stream. For more information, see [FFmpeg language definition](https://www.ffmpeg.org/ffmpeg-all.html#Metadata) and [ISO 639](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes).', example='eng'),
                }
              ](name='SubtitleStream')
              }(name='SubtitleStreamList', description='The list of subtitle streams.'),
              videoStreamList?: {
                videoStream?: [ 
                {
                  avgFPS?: string(name='AvgFPS', description='The average frame rate of the video stream.', example='23.976025'),
                  bitrate?: string(name='Bitrate', description='The bitrate of the video stream.', example='1496.46'),
                  codecLongName?: string(name='CodecLongName', description='The full name of the encoding format.', example='H.264 / AVC / MPEG-4 AVC / MPEG-4 part 10'),
                  codecName?: string(name='CodecName', description='The short name of the encoding format.', example='h264'),
                  codecTag?: string(name='CodecTag', description='The tag of the encoding format.', example='0x31637661'),
                  codecTagString?: string(name='CodecTagString', description='The tag string of the encoding format.', example='avc1'),
                  codecTimeBase?: string(name='CodecTimeBase', description='The codec time base.', example='1001/48000'),
                  dar?: string(name='Dar', description='The display aspect ratio (DAR) of the video stream.', example='16:9'),
                  duration?: string(name='Duration', description='The duration of the video stream.', example='17.225542'),
                  fps?: string(name='Fps', description='The frame rate of the video stream.', example='23.976025'),
                  hasBFrames?: string(name='HasBFrames', description='Indicates whether the video stream contains bidirectional frames (B-frames).', example='2'),
                  height?: string(name='Height', description='The height of the video stream in pixels.', example='720'),
                  index?: string(name='Index', description='The sequence number of the video stream. The value indicates the position of the video stream in all video streams.', example='0'),
                  lang?: string(name='Lang', description='The language of the video stream. For more information, see [FFmpeg language definition](https://www.ffmpeg.org/ffmpeg-all.html#Metadata) and [ISO 639](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes).', example='eng'),
                  level?: string(name='Level', description='The codec level.', example='51'),
                  networkCost?: {
                    avgBitrate?: string(name='AvgBitrate', description='The average bitrate of the video stream.', example='100'),
                    costBandwidth?: string(name='CostBandwidth', description='The maximum bandwidth that is consumed.', example='10'),
                    preloadTime?: string(name='PreloadTime', description='The amount of preload time.', example='8'),
                  }(name='NetworkCost', description='The network bandwidth consumption.'),
                  numFrames?: string(name='NumFrames', description='The total number of frames.', example='25'),
                  pixFmt?: string(name='PixFmt', description='The pixel format of the video stream.', example='yuv420p'),
                  profile?: string(name='Profile', description='The codec profile.', example='high'),
                  sar?: string(name='Sar', description='The sample aspect ratio (SAR) of the video stream.', example='1:1'),
                  startTime?: string(name='StartTime', description='The start point in time of the video stream.', example='0.000000'),
                  timebase?: string(name='Timebase', description='The time base of the video stream.', example='1/24000'),
                  width?: string(name='Width', description='The width of the video stream in pixels.', example='1280'),
                }
              ](name='VideoStream')
              }(name='VideoStreamList', description='The list of video streams.'),
            }(name='Streams', description='The stream information.'),
            width?: string(name='Width', description='The width of the video.', example='1280'),
          }(name='Properties', description='The media properties.'),
          rotate?: string(name='Rotate', description='The rotation angle of the video, in the clockwise direction.', example='180'),
          subtitleConfig?: {
            extSubtitleList?: {
              extSubtitle?: [ 
              {
                charEnc?: string(name='CharEnc', description='The character set used by the external subtitle. 

- Valid values: **UTF-8**, **GBK**, **BIG5**, and **auto**.
- Default value: **auto**.

>  If you set this parameter to **auto**, the detected character set may not be the actual character set. We recommend that you set this parameter to another value.', example='UTF-8'),
                fontName?: string(name='FontName', description='The font of the hard subtitles converted from external subtitles. Default value: **SimSun**. For more information, see [Fonts](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/font-name).', example='"WenQuanYi Zen Hei", "Yuanti SC Regular", "SimSun"'),
                input?: {
                  bucket?: string(name='Bucket', description='The name of the OSS bucket in which the input file is stored.', example='example-bucket-****'),
                  location?: string(name='Location', description='The ID of the OSS region in which the input file resides.', example='oss-cn-hangzhou'),
                  object?: string(name='Object', description='The name of the OSS object that is used as the input file.', example='example-output.flv'),
                }(name='Input', description='The OSS object that is used as the external subtitle. The value is a JSON object. Files in the **SRT** or **ASS** format are supported.'),
              }
            ](name='ExtSubtitle')
            }(name='ExtSubtitleList', description='The list of external subtitles. The value is a JSON array that contains up to **four** objects.'),
            subtitleList?: {
              subtitle?: [ 
              {
                map?: string(name='Map', description='The audio track. Format: `0:{Stream}:{Stream sequence number}`, which is `0:a:{audio_index}`. The value of Stream is a, which indicates an audio stream. The sequence number is the index of the audio stream in the list and starts from 0.', example='0:a:0'),
              }
            ](name='Subtitle')
            }(name='SubtitleList', description='The list of subtitles.'),
          }(name='SubtitleConfig', description='The subtitle configurations.'),
          superReso?: {
            isHalfSample?: string(name='IsHalfSample', description='Indicates whether to obtain parameters related to the sampling rate. Valid values:

- **true**: The parameters are obtained.
- **false**: The parameters are not obtained.', example='true'),
          }(name='SuperReso', description='The configurations for using the resolution of the source video.'),
          tailSlateList?: {
            tailSlate?: [ 
            {
              bgColor?: string(name='BgColor', description='The color of the bars that are added to the ending part if the size of the ending part is smaller than that of the main part. Default value: **White**. For more information, see [Background colors](http://docs-aliyun.cn-hangzhou.oss.aliyun-inc.com/assets/attach/29253/cn_zh/1502784952344/color.txt?spm=a2c4g.11186623.2.63.241240f77qp3Yy&file=color.txt).', example='White'),
              blendDuration?: string(name='BlendDuration', description='The amount of time between the end of the main part and the beginning of the ending part. During the transition, the last frame of the main part fades out, and the first frame of the ending part fades in. Unit: seconds. Default value: **0**.', example='2'),
              height?: string(name='Height', description='The height of the ending part. 

- Valid values: **0 to 4096**, **-1**, and **full**.
- A value of **-1** indicates that the height of the source of the ending part is retained. A value of **full** indicates that the height of the main part is used for the ending part.
- Default value: **-1**.', example='1080'),
              isMergeAudio?: boolean(name='IsMergeAudio', description='Indicates whether to merge the audio content of the ending part. Valid values:

- **true**: The audio content of the ending part is merged.
- **false**: The audio content of the ending part is not merged.', example='false'),
              start?: string(name='Start', description='The start time.', example='1'),
              tailUrl?: string(name='TailUrl', description='The OSS URL of the ending part.', example='http://example-bucket-****.oss-cn-hangzhou.aliyuncs.com/opening_01.flv'),
              width?: string(name='Width', description='The width of the ending part. 

- Valid values: **0 to 4096**, **-1**, and **full**.
- A value of **-1** indicates that the height of the source of the ending part is retained. A value of **full** indicates that the height of the main part is used for the ending part.
- Default value: **-1**.', example='1920'),
            }
          ](name='TailSlate')
          }(name='TailSlateList', description='The list of ending parts. The value is a JSON object.'),
          templateId?: string(name='TemplateId', description='The ID of the transcoding template.', example='S00000000-000010'),
          transConfig?: {
            adjDarMethod?: string(name='AdjDarMethod', description='The method of resolution adjustment. Default value: **none**. Valid values:

*   rescale: resizes the video image.
*   crop:crops the video image.
*   pad: scales out the video image to fill the view.
*   none: no adjustment.', example='crop'),
            isCheckAudioBitrate?: string(name='IsCheckAudioBitrate', description='Indicates whether the audio bitrate is checked. If the output audio bitrate is higher than the input audio bitrate, the system considers that the output bitrate equals the input bitrate.

>  If the same audio encoder is used before and after transcoding and the audio bitrate after transcoding is greater than the audio bitrate before transcoding, the audio bitrate is reset, and the specified audio bitrate does not take effect.

*   **true**: The audio bitrate is checked.
*   **false**: The video bitrate is not checked.
*   Default value: **false**.', example='false'),
            isCheckAudioBitrateFail?: string(name='IsCheckAudioBitrateFail', description='Indicates whether the audio bitrate is checked. If the output audio bitrate is higher than the input audio bitrate, a transcoding failure is returned without video transcoding. This parameter takes precedence over the **IsCheckAudioBitrate** parameter.

*   **false**: The audio bitrate is checked.
*   **true**: The audio bitrate is not checked.', example='true'),
            isCheckReso?: string(name='IsCheckReso', description='Indicates whether the resolution is checked. If the output resolution is higher than the input resolution based on the width or height, the system considers that the output resolution equals the input resolution.

*   **true**: The resolution is checked.
*   **false**: The resolution is not checked.
*   Default value: **false**.', example='false'),
            isCheckResoFail?: string(name='IsCheckResoFail', description='Indicates whether the resolution is checked. If the output resolution is higher than the input resolution based on the width or height, a transcoding failure is returned without video transcoding. This parameter takes precedence over the IsCheckReso parameter.

*   **true**: The resolution is checked.
*   **false**: The resolution is not checked.
*   Default value: **false**.', example='false'),
            isCheckVideoBitrate?: string(name='IsCheckVideoBitrate', description='Indicates whether the video bitrate is checked. If the bitrate of the output video is higher than that of the input video, the system considers that the output bitrate equals the input bitrate.

*   **true**: The video bitrate is checked.
*   **false**: The video bitrate is not checked.
*   Default value: **false**.', example='false'),
            isCheckVideoBitrateFail?: string(name='IsCheckVideoBitrateFail', description='Indicates whether the video bitrate is checked. If the bitrate of the output video is higher than that of the input video, a transcoding failure is returned without video transcoding. This parameter takes precedence over the **IsCheckVideoBitrate** parameter.

*   **true**: The video bitrate is checked.
*   **false**: The video bitrate is not checked.
*   Default value: **false**.', example='true'),
            transMode?: string(name='TransMode', description='The transcoding mode. Valid values:

*   **onepass**: transcoding based on one-pass algorithms, which has higher accuracy.
*   **twopass**: transcoding based on two-pass algorithms, which has lower accuracy.
*   **CBR**: transcoding based on a fixed bitrate.', example='onepass'),
          }(name='TransConfig', description='The general transcoding configurations.

>  If this parameter is specified in the request, the value overwrites the corresponding parameter in the specified transcoding template.'),
          userData?: string(name='UserData', description='The custom data.', example='example data'),
          video?: {
            bitrate?: string(name='Bitrate', description='The bitrate of the output video. Unit: Kbit/s.', example='10'),
            bitrateBnd?: {
              max?: string(name='Max', description='The upper limit of the total bitrate. Unit: Kbit/s.', example='20'),
              min?: string(name='Min', description='The lower limit of the total bitrate. Unit: Kbit/s.', example='10'),
            }(name='BitrateBnd', description='The average bitrate range of the video.'),
            bufsize?: string(name='Bufsize', description='The size of the buffer.

*   Unit: KB.
*   Default value: **6000**.', example='1000'),
            codec?: string(name='Codec', description='The video codec.

*   Valid values: **H.264**, **H.265**, **GIF**, and **WEBP**.
*   Default value: **H.264**.', example='H.264'),
            crf?: string(name='Crf', description='The constant rate factor.

*   If the **Crf** parameter is specified, the setting of the **Bitrate** parameter becomes invalid.
*   Default value when the value of the Codec parameter is H.264: **23**. Default value when the value of the Codec parameter is H.265: **26**.', example='22'),
            crop?: string(name='Crop', description='The method of video cropping. Valid values:

*   **border**: automatically detects and removes borders.
*   A value in the format of width:height:left:top: crops the video based on the custom settings.', example='1280:800:0:140'),
            degrain?: string(name='Degrain', description='The level of the independent denoising algorithm.', example='5'),
            fps?: string(name='Fps', description='The frame rate of the video.

*   Unit: frames per second.
*   Valid values: 0 to 60. The value is 60 if the frame rate of the input file exceeds 60.
*   Default value: the frame rate of the input file.', example='60'),
            gop?: string(name='Gop', description='The Group of Pictures (GOP) size. The GOP size can be the maximum interval of keyframes or the maximum number of frames in a frame group. Unit: seconds.

*   Default value: 10s.
*   If the value specifies the maximum number of frames, the value does not contain a unit.', example='1'),
            height?: string(name='Height', description='The height of the video.

*   Unit: pixel.
*   Default value: the original video height.', example='1280'),
            maxFps?: string(name='MaxFps', description='The maximum frame rate.', example='15'),
            maxrate?: string(name='Maxrate', description='The maximum bitrate of the video. Unit: Kbit/s.', example='10'),
            pad?: string(name='Pad', description='The black borders to be added to the video.

*   Format: width:height:left:top.
*   Unit: pixel.', example='1280:800:0:140'),
            pixFmt?: string(name='PixFmt', description='The pixel format for video color encoding.

*   Valid values: standard pixel formats such as **yuv420p** and **yuvj420p**.

    **

    **Note** If a non-standard pixel format such as yuvj420p(pc, bt470bg/bt470bg/smpte170m) is used, compatibility with the pixel format must be configured. Otherwise, the transcoding fails. Default value: yuv420p or the original pixel format. )', example='yuvj420p'),
            preset?: string(name='Preset', description='The preset video algorithm. Default value: **medium**. Valid values:

*   **veryfast**
*   **fast**
*   **medium**
*   **slow**
*   **slower**', example='veryfast'),
            profile?: string(name='Profile', description='The encoding profile. This parameter is returned only for the H.264 codec. Default value: **high**. Valid values:

>  If multiple definitions exist, we recommend that you set this parameter to baseline for the lowest definition to ensure normal playback on low-end devices. Set this parameter to main or high for other definitions.

*   **baseline**: applicable to mobile devices.
*   **main**: applicable to standard-definition devices.
*   **high**: applicable to high-definition devices.', example='baseline'),
            qscale?: string(name='Qscale', description='The level of quality control on the video.', example='15'),
            resoPriority?: string(name='ResoPriority', description='The priority of the resource.', example='1'),
            scanMode?: string(name='ScanMode', description='The scan mode. Valid values:

*   **interlaced**: An interlaced scan is performed.
*   **progressive**: A progressive scan is performed.
*   **auto**: A scan is performed based on the video source.', example='interlaced'),
            width?: string(name='Width', description='The width of the video.

*   Unit: pixel.
*   Default value: **the original video width**.', example='1080'),
          }(name='Video', description='The video configurations.

>  If this parameter is specified in the request, the setting of the **AliyunVideoCodec** parameter in the template specified by the **TemplateId** parameter is overwritten by the setting of this parameter.'),
          videoStreamMap?: string(name='VideoStreamMap', description='The sequence number of the video stream. 

- Format: 0:a:{Sequence number}. Example: 0:a:0.
- The sequence number is the index of the video stream in the list and starts from 0.
- If you do not specify a sequence number, the default video stream is used.', example='0:a:0'),
          waterMarkConfigUrl?: string(name='WaterMarkConfigUrl', description='The URL of the watermark configuration file.', example='http://example.com/configure'),
          waterMarkList?: {
            waterMark?: [ 
            {
              dx?: string(name='Dx', description='The horizontal offset of the watermark relative to the output video. If this parameter is specified in the request, the value overwrites the corresponding parameter in the watermark template. Default value: 0. The value can be an integer or a decimal.

*   An integer indicates the pixel value of the horizontal offset.

    *   Valid values: **\\[8,4096]**.
    *   Unit: pixel.

*   A decimal indicates the ratio of the horizontal offset to the width in the output video resolution.

    *   Valid values: (0,1).
    *   The decimal number can be accurate to four decimal places, such as 0.9999. Excessive digits are automatically deleted.', example='1'),
              dy?: string(name='Dy', description='The vertical offset of the watermark image relative to the output video. If this parameter is specified in the request, the value overwrites the corresponding parameter in the watermark template. The value can be an integer or a decimal.

*   An integer indicates the pixel value of the vertical offset.

    *   Valid values: **\\[8,4096]**.
    *   Unit: pixel.

*   A decimal indicates the ratio of the vertical offset to the height in the output video resolution.

    *   Valid values: **(0,1)**.
    *   The decimal number can be accurate to four decimal places, such as 0.9999. Excessive digits are automatically deleted.', example='1'),
              height?: string(name='Height', description='The height of the watermark image. If this parameter is specified in the request, the value overwrites the corresponding parameter in the watermark template. The value can be an integer or a decimal.

*   An integer indicates the pixel value of the watermark height.

    *   Valid values: **\\[8,4096]**.
    *   Unit: pixel.

*   A decimal indicates the ratio of the watermark height to the height in the output video resolution.

    *   Valid values: **(0,1)**.
    *   The decimal number can be accurate to four decimal places, such as 0.9999. Excessive digits are automatically deleted.', example='1280'),
              inputFile?: {
                bucket?: string(name='Bucket', description='The name of the OSS bucket in which the input file is stored.', example='example-bucket'),
                location?: string(name='Location', description='The ID of the OSS region in which the input file resides.', example='oss-cn-hangzhou'),
                object?: string(name='Object', description='The name of the OSS object that is used as the input file.', example='example-logo-****.png'),
              }(name='InputFile', description='The watermark input file. You can use an image in the PNG format or a file in the MOV format as the watermark input.'),
              referPos?: string(name='ReferPos', description='The position of the watermark. If this parameter is specified in the request, the value overwrites the corresponding parameter in the watermark template. Valid values:

*   **TopRight**: the upper-right corner.
*   **TopLeft**: the upper-left corner.
*   **BottomRight**: the lower-right corner.
*   **BottomLeft**: the lower-left corner.', example='TopRight'),
              type?: string(name='Type', description='The type of the watermark. If this parameter is specified in the request, the value overwrites the corresponding parameter in the watermark template. For more information, see [Parameter details](~~29253~~). Valid values:

*   **Image**: an image watermark.
*   **Text**: a text watermark.', example='Image'),
              waterMarkTemplateId?: string(name='WaterMarkTemplateId', description='The ID of the watermark template.', example='88c6ca184c0e47098a5b665e2a12****'),
              width?: string(name='Width', description='The width of the watermark image. If this parameter is specified in the request, the value overwrites the corresponding parameter in the watermark template. The value can be an integer or a decimal.

*   An integer indicates the pixel value of the watermark width.

    *   Valid values: **\\[8,4096]**.
    *   Unit: pixel.

*   A decimal indicates the ratio of the watermark width to the width in the output video resolution.

    *   Valid values: **(0,1)**.
    *   The decimal number can be accurate to four decimal places, such as 0.9999. Excessive digits are automatically deleted.', example='1080'),
            }
          ](name='WaterMark')
          }(name='WaterMarkList', description='The list of watermarks.

>  If watermarks are truncated or fail to be added to the video, check whether the text watermarks that you add contain special characters. If the text watermarks contain special characters, you must escape the special characters. Alternatively, \\<prop="china">[submit a ticket](https://smartservice.console.aliyun.com/service/create-ticket?product=mts)\\</prop>\\<prop="intl">[submit a ticket](https://workorder-intl.console.aliyun.com/?spm=5176.12246746.top-nav.dticket.68797bbcm8H408#/ticket/add/?productId=1232)\\</prop> to contact Alibaba Cloud customer service for compatibility processing.'),
        }(name='Output', description='The outputs of the jobs.'),
        percent?: long(name='Percent', description='The transcoding progress.', example='100'),
        pipelineId?: string(name='PipelineId', description='The ID of the MPS queue.', example='88c6ca184c0e47098a5b665e2a126797'),
        state?: string(name='State', description='The state of the job. Valid values:

*   **Submitted**: The job is submitted.
*   **TranscodeFail**: The job is failed.', example='Submitted'),
      }(name='Job', description='The details of the job. If the job fails to be submitted, no job ID is generated.'),
      message?: string(name='Message', description='The error message returned if the job fails to be created. This parameter is not returned if the job is created.', example='The specified parameter "%s" cannot be null.'),
      success?: boolean(name='Success', description='Indicates whether the job is created. Valid values:

- **true**: The job is created.
- **false**: The job fails to be created.', example='true'),
    }
  ](name='JobResult')
  }(name='JobResultList', description='The transcoding jobs that are generated.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='25818875-5F78-4A45S71F6-D73936451234'),
}

model SubmitJobsResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: SubmitJobsResponseBody(name='body'),
}

/**
  * *   If the transcoding jobs and workflows created in the ApsaraVideo Media Processing (MPS) console cannot meet your business requirements, you can call the SubmitJobs operation to submit transcoding jobs. Set transcoding parameters as required when you call the SubmitJobs operation.
  * *   If you want to use multiple accounts in MPS, you can create RAM users by using your Alibaba Cloud account and grant the MPSfullaccess permission to the RAM users. If the Alibaba Cloud account that is used to query transcoding jobs is not the Alibaba Cloud account that is used to create the transcoding jobs, no data is returned. For more information, see [Create and grant permissions to a RAM user](~~44569~~).
  * *   A transcoding job is generated for each transcoding output. This API operation returns the transcoding jobs that are generated.
  * *   A video is re-encoded during transcoding in MPS. The bitrate of the transcoded video may be different from that of the source video. If you want to retain the bitrate of a video during transcoding, you can use a container format conversion template. For more information, see [Preset template details](~~29256~~).
  * *   Jobs are added to an MPS queue in which the jobs are scheduled and executed. After the jobs are executed, you can call the QueryJobList operation to query the results of the jobs. Alternatively, you can enable asynchronous notifications so that you can be automatically notified of the job results.
  * >To enable asynchronous notifications, you must bind a Message Service (MNS) topic to the MPS queue in which the transcoding jobs are executed. If an asynchronous message is returned for a transcoding job in the MPS queue, MPS forwards the message to the specified MNS topic.
  * *   To use an intelligent preset template to transcode a video, you must first call the [SubmitAnalysisJob](~~29223~~) operation to submit a preset template analysis job for the video. After the preset template analysis job is complete, you can call the [QueryAnalysisJobList](~~29224~~) operation to obtain the intelligent preset templates that are applicable to the video.
  * > When you submit a transcoding job, set the `TemplateId` parameter to the ID of an applicable preset template. If you specify a preset template that is not in the applicable preset templates when you submit a transcoding job, the transcoding job fails.
  * *   If you use a static preset template to transcode a video, you do not need to submit a preset template analysis job first.
  * *   The size of the file in a transcoding job is up to 100 GB. Otherwise, the transcoding job may fail.
  * *   For information about transcoding FAQ, see [FAQs in MPS](~~38986~~).
  * ## QPS limit
  * You can call this API operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request SubmitJobsRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return SubmitJobsResponse
 */
async function submitJobsWithOptions(request: SubmitJobsRequest, runtime: Util.RuntimeOptions): SubmitJobsResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.input)) {
    query['Input'] = request.input;
  }
  if (!Util.isUnset(request.outputBucket)) {
    query['OutputBucket'] = request.outputBucket;
  }
  if (!Util.isUnset(request.outputLocation)) {
    query['OutputLocation'] = request.outputLocation;
  }
  if (!Util.isUnset(request.outputs)) {
    query['Outputs'] = request.outputs;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.pipelineId)) {
    query['PipelineId'] = request.pipelineId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'SubmitJobs',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * *   If the transcoding jobs and workflows created in the ApsaraVideo Media Processing (MPS) console cannot meet your business requirements, you can call the SubmitJobs operation to submit transcoding jobs. Set transcoding parameters as required when you call the SubmitJobs operation.
  * *   If you want to use multiple accounts in MPS, you can create RAM users by using your Alibaba Cloud account and grant the MPSfullaccess permission to the RAM users. If the Alibaba Cloud account that is used to query transcoding jobs is not the Alibaba Cloud account that is used to create the transcoding jobs, no data is returned. For more information, see [Create and grant permissions to a RAM user](~~44569~~).
  * *   A transcoding job is generated for each transcoding output. This API operation returns the transcoding jobs that are generated.
  * *   A video is re-encoded during transcoding in MPS. The bitrate of the transcoded video may be different from that of the source video. If you want to retain the bitrate of a video during transcoding, you can use a container format conversion template. For more information, see [Preset template details](~~29256~~).
  * *   Jobs are added to an MPS queue in which the jobs are scheduled and executed. After the jobs are executed, you can call the QueryJobList operation to query the results of the jobs. Alternatively, you can enable asynchronous notifications so that you can be automatically notified of the job results.
  * >To enable asynchronous notifications, you must bind a Message Service (MNS) topic to the MPS queue in which the transcoding jobs are executed. If an asynchronous message is returned for a transcoding job in the MPS queue, MPS forwards the message to the specified MNS topic.
  * *   To use an intelligent preset template to transcode a video, you must first call the [SubmitAnalysisJob](~~29223~~) operation to submit a preset template analysis job for the video. After the preset template analysis job is complete, you can call the [QueryAnalysisJobList](~~29224~~) operation to obtain the intelligent preset templates that are applicable to the video.
  * > When you submit a transcoding job, set the `TemplateId` parameter to the ID of an applicable preset template. If you specify a preset template that is not in the applicable preset templates when you submit a transcoding job, the transcoding job fails.
  * *   If you use a static preset template to transcode a video, you do not need to submit a preset template analysis job first.
  * *   The size of the file in a transcoding job is up to 100 GB. Otherwise, the transcoding job may fail.
  * *   For information about transcoding FAQ, see [FAQs in MPS](~~38986~~).
  * ## QPS limit
  * You can call this API operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request SubmitJobsRequest
  * @return SubmitJobsResponse
 */
async function submitJobs(request: SubmitJobsRequest): SubmitJobsResponse {
  var runtime = new Util.RuntimeOptions{};
  return submitJobsWithOptions(request, runtime);
}

model SubmitMediaCensorJobRequest {
  barrages?: string(name='Barrages', description='The live comments.', example='hello world'),
  coverImages?: string(name='CoverImages', description='The OSS URL of the image file that is used as the thumbnail. To view the OSS URL of the image file, you can log on to the **MPS console** and choose **Media Management** > **Media List** in the left-side navigation pane. You can specify up to five thumbnails in a JSON array.

*   Bucket: the name of the OSS bucket that stores the input file.
*   Location: the OSS region. The OSS region must be the same as the region in which the MPS service is activated.
*   Object: the OSS object to be moderated.', example='[{"Bucket":"example-bucket-****","Location":"oss-cn-shanghai","Object":"example-****.jpeg"}]'),
  description?: string(name='Description', description='The description of the video. The value can be up to 128 bytes in size.', example='example description'),
  externalUrl?: string(name='ExternalUrl', description='The URL of the video.', example='http://www.example.com/video-****/test-****.mp4'),
  input?: string(name='Input', description='The Object Storage Service (OSS) URL of the video or audio file to be moderated. To view the OSS URL of the video or audio file, you can log on to the **MPS console** and choose **Media Management** > **Media List** in the left-side navigation pane. To moderate an image file, use the `CoverImage` parameter to specify the OSS URL of the image file.

The value is a JSON object. For more information, see [Input](~~29253~~).

*   Bucket: the name of the OSS bucket that stores the input file.
*   Location: the OSS region. The OSS region must be the same as the region in which the MPS service is activated.
*   Object: the OSS object to be moderated.', example='{"Bucket":"example-bucket-****","Location":"oss-cn-shanghai","Object":"example-****.flv"}'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  pipelineId?: string(name='PipelineId', description='The ID of the MPS queue. This ID can be used to associate the job with a notification method. To view the ID of the MPS queue, you can log on to the **MPS console** and choose **Global Settings** > **Pipelines** in the left-side navigation pane. An empty string ("") indicates that the default MPS queue is used to run the job. By default, an MPS queue can process a maximum of 10 concurrent content moderation jobs. To increase the limit, [submit a ticket](https://workorder-intl.console.aliyun.com/?spm=5176.12246746.top-nav.dticket.68797bbcm8H408#/ticket/add/?productId=1232).

>  MPS queues are automatically created by the system. For more information about how to query and update MPS queues, see [UpdatePipeline](~~188374~~).', example='b22c173cced04565b1f38f1ecc39****'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  title?: string(name='Title', description='The title of the video. The value can be up to 64 bytes in size.', example='Hello World'),
  userData?: string(name='UserData', description='The custom data. The value can be up to 128 bytes in size.', example='UserDatatestid-001-****'),
  videoCensorConfig?: string(name='VideoCensorConfig', description='The video moderation configurations and the OSS URLs of the output snapshots. To view the OSS URLs of the output snapshots, you can log on to the **MPS console** and choose **Media Management** > **Media List** in the left-side navigation pane.

The value is a JSON object.

*   OutputFile:

    *   Bucket: the name of the OSS bucket that stores the output snapshots.
    *   Location: the OSS region. The OSS region must be the same as the region in which the MPS service is activated.
    *   Object: the OSS object to be generated. In the value, {Count} indicates the sequence number of the frame snapshot.

*   StoreVideoTimeline: specifies whether to generate the `{jobId}.video_timeline` file. The file is stored in OSS. A value of true indicates that the file is generated. A value of false indicates that the file is not generated. If you do not specify this parameter, the file is not generated by default. For more information about the format of the file, see the sample {jobId}.video_timeline file in [QueryMediaCensorJobDetail](~~91779~~).

*   SaveType: the output mode. A value of abnormal indicates that snapshots are generated only for illegal frames. A value of all indicates that snapshots are generated for all frames.

*   Biztype: the moderation template. If you do not specify this parameter or set the value to common, the default template is used. You can submit a ticket to create a custom moderation template. Then, set this parameter to your user ID to use the custom moderation template.

*   Scenes: the moderation scenarios. You can specify the moderation scenarios that you want to use. If you do not specify this parameter, the terrorism and porn moderation scenarios are used by default. Valid values:

    *   porn: pornographic content detection

    *   terrorism: terrorist content detection

    *   ad: ad violation detection

    *   live: undesirable scene detection

    *   logo: special logo detection

    *   audio: audio anti-spam

> If the input file contains audio tracks and the audio moderation scenario is specified, the audio tracks are moderated. If the input file does not contain audio tracks, you do not need to specify the audio moderation scenario.', example='{"Scenes" : ["porn"], "OutputFile":{"Bucket": "example-001","Location": "oss-cn-hangzhou","Object": "test/example-{Count}.jpg"},"SaveType" : "abnormal","BizType":"common"}'),
}

model SubmitMediaCensorJobResponseBody = {
  jobId?: string(name='JobId', description='The ID of the content moderation job. We recommend that you keep this ID for subsequent operation calls.', example='88c6ca184c0e47098a5b665e2****'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='25818875-5F78-4A13-BEF6-D7393642CA58'),
}

model SubmitMediaCensorJobResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: SubmitMediaCensorJobResponseBody(name='body'),
}

/**
  * The job that you submit by calling this operation is run in asynchronous mode. The job is added to an ApsaraVideo Media Processing (MPS) pipeline and then scheduled, queued, and run. You can call the [QueryMediaCensorJobDetail](~~91779~~) operation or configure an asynchronous notification to obtain the job result.
  * ## QPS limit
  * You can call this operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request SubmitMediaCensorJobRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return SubmitMediaCensorJobResponse
 */
async function submitMediaCensorJobWithOptions(request: SubmitMediaCensorJobRequest, runtime: Util.RuntimeOptions): SubmitMediaCensorJobResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.barrages)) {
    query['Barrages'] = request.barrages;
  }
  if (!Util.isUnset(request.coverImages)) {
    query['CoverImages'] = request.coverImages;
  }
  if (!Util.isUnset(request.description)) {
    query['Description'] = request.description;
  }
  if (!Util.isUnset(request.externalUrl)) {
    query['ExternalUrl'] = request.externalUrl;
  }
  if (!Util.isUnset(request.input)) {
    query['Input'] = request.input;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.pipelineId)) {
    query['PipelineId'] = request.pipelineId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.title)) {
    query['Title'] = request.title;
  }
  if (!Util.isUnset(request.userData)) {
    query['UserData'] = request.userData;
  }
  if (!Util.isUnset(request.videoCensorConfig)) {
    query['VideoCensorConfig'] = request.videoCensorConfig;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'SubmitMediaCensorJob',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * The job that you submit by calling this operation is run in asynchronous mode. The job is added to an ApsaraVideo Media Processing (MPS) pipeline and then scheduled, queued, and run. You can call the [QueryMediaCensorJobDetail](~~91779~~) operation or configure an asynchronous notification to obtain the job result.
  * ## QPS limit
  * You can call this operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request SubmitMediaCensorJobRequest
  * @return SubmitMediaCensorJobResponse
 */
async function submitMediaCensorJob(request: SubmitMediaCensorJobRequest): SubmitMediaCensorJobResponse {
  var runtime = new Util.RuntimeOptions{};
  return submitMediaCensorJobWithOptions(request, runtime);
}

model SubmitMediaInfoJobRequest {
  async?: boolean(name='Async', description='Specifies whether to enable the asynchronous mode for the job. We recommend that you set this parameter to true. Valid values:

*   **true**: enables the asynchronous mode.
*   **false**: does not enable the asynchronous mode.', example='true'),
  input?: string(name='Input', description='The information about the input media file. The value is a JSON string. You must perform the following operations to grant MPS the permissions to access the OSS bucket in which the input media file is stored: Log on to the **MPS console**, choose **Workflows** > **Media Buckets** in the left-side navigation pane, and then click **Add Bucket**. Then, you must URL-encode the object. For example, `{"Bucket":"example-bucket","Location":"example-location","Object":"example%2Fexample.flv"}` indicates the `example-bucket.example-location.aliyuncs.com/example/example.flv` file.

>  The OSS bucket must reside in the region in which you use MPS.', example='{"Bucket":"example-bucket","Location":"example-location","Object":"example%2Fexample.flv"}'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  pipelineId?: string(name='PipelineId', description='The ID of the MPS queue to which the analysis job is submitted. To view the ID of the MPS queue, log on to the **MPS console** and choose **Global Settings** > **Pipelines** in the left-side navigation pane.', example='88c6ca184c0e432bbf5b665e2a15****'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  userData?: string(name='UserData', description='The custom data. The custom data can contain letters, digits, and hyphens (-), and can be up to 1,024 bytes in length. It cannot start with a hyphen (-).', example='testid-001'),
}

model SubmitMediaInfoJobResponseBody = {
  mediaInfoJob?: {
    async?: boolean(name='Async', description='Indicates whether the job is run in asynchronous mode.', example='true'),
    code?: string(name='Code', description='The error code returned if the job fails.', example='InvalidParameter.JsonObjectFormatInvalid'),
    creationTime?: string(name='CreationTime', description='The time when the job was created.', example='2014-01-10T12:00:00Z'),
    input?: {
      bucket?: string(name='Bucket', description='The name of the OSS bucket in which the input media file is stored.', example='example-bucket'),
      location?: string(name='Location', description='The region of the OSS bucket in which the input media file is stored.', example='example-location'),
      object?: string(name='Object', description='The name of the OSS object that is used as the input media file.', example='example.flv'),
    }(name='Input', description='The information about the input media file.'),
    jobId?: string(name='JobId', description='The ID of the job.', example='23ca1d184c0e4341e5b665e2a12****'),
    MNSMessageResult?: {
      errorCode?: string(name='ErrorCode', description='The error code that is returned if the job fails. This parameter is not returned if the job is successful.', example='The parameter \\"Input\\" does not conform to the JSON Object specification'),
      errorMessage?: string(name='ErrorMessage', description='The error message that is returned if the job fails. This parameter is not returned if the job is successful.', example='InvalidParameter.JsonObjectFormatInvalid'),
      messageId?: string(name='MessageId', description='The ID of the message that is returned if the job is successful. This parameter is not returned if the job fails.', example='16f01ad6175e4230ac42bb5182cd****'),
    }(name='MNSMessageResult', description='The details of the message that is sent to Message Service (MNS) to notify users of the job result.'),
    message?: string(name='Message', description='The error message returned if the job fails.', example='The parameter ”*” does not conform to the JSON Object specification'),
    pipelineId?: string(name='PipelineId', description='The ID of the MPS queue to which the analysis job is submitted.', example='88c6ca184c0e432bbf5b665e2a15****'),
    properties?: {
      bitrate?: string(name='Bitrate', description='The bitrate. Unit: Kbit/s.', example='1630.045'),
      duration?: string(name='Duration', description='The total duration of the input media file. Unit: seconds.', example='17.226000'),
      fileFormat?: string(name='FileFormat', description='The format of the input media file.', example='QuickTime/MOV'),
      fileSize?: string(name='FileSize', description='The size of the file. Unit: bytes.', example='3509895'),
      format?: {
        bitrate?: string(name='Bitrate', description='The total bitrate. Unit: Kbit/s.', example='1630.045'),
        duration?: string(name='Duration', description='The total duration of the input media file. Unit: seconds.', example='17.226000'),
        formatLongName?: string(name='FormatLongName', description='The full name of the container format.', example='QuickTime/MOV'),
        formatName?: string(name='FormatName', description='The short name of the container format. For more information about the parameters, see [Parameter details](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/parameter-details).', example='mov'),
        numPrograms?: string(name='NumPrograms', description='The total number of program streams.', example='0'),
        numStreams?: string(name='NumStreams', description='The total number of media streams.', example='2'),
        size?: string(name='Size', description='The size of the file. Unit: bytes.', example='3509895'),
        startTime?: string(name='StartTime', description='The start time.', example='0.000000'),
      }(name='Format', description='The format information.'),
      fps?: string(name='Fps', description='The frame rate.', example='25'),
      height?: string(name='Height', description='The height of the video. Unit: pixels.', example='1080'),
      streams?: {
        audioStreamList?: {
          audioStream?: [ 
          {
            bitrate?: string(name='Bitrate', description='The bitrate. Unit: Kbit/s.', example='128.806'),
            channelLayout?: string(name='ChannelLayout', description='The output layout of the sound channels.', example='stereo'),
            channels?: string(name='Channels', description='The number of sound channels.', example='2'),
            codecLongName?: string(name='CodecLongName', description='The full name of the encoding format.', example='AAC (Advanced Audio Coding)'),
            codecName?: string(name='CodecName', description='The short name of the encoding format. Default value: acc. Valid values:

- **acc**
- **mp3**
- **mp4**
- **ogg**
- **flac**', example='aac'),
            codecTag?: string(name='CodecTag', description='The tag of the encoding format.', example='0x6134706d'),
            codecTagString?: string(name='CodecTagString', description='The tag string of the encoding format.', example='mp4a'),
            codecTimeBase?: string(name='CodecTimeBase', description='The codec time base.', example='1/44100'),
            duration?: string(name='Duration', description='The duration. Unit: seconds.', example='17.159546'),
            index?: string(name='Index', description='The sequence number of the audio stream. The value indicates the position of the audio stream in all audio streams.', example='1'),
            lang?: string(name='Lang', description='The language.', example='eng'),
            numFrames?: string(name='NumFrames', description='The total number of frames.', example='123'),
            sampleFmt?: string(name='SampleFmt', description='The sampling format.', example='fltp'),
            samplerate?: string(name='Samplerate', description='The sampling rate. Unit: Hz.', example='44100'),
            startTime?: string(name='StartTime', description='The start time.', example='0.000000'),
            timebase?: string(name='Timebase', description='The time base.', example='1/44100'),
          }
        ](name='AudioStream')
        }(name='AudioStreamList', description='The audio streams. A media file can contain up to four audio streams.'),
        subtitleStreamList?: {
          subtitleStream?: [ 
          {
            codecLongName?: string(name='CodecLongName', description='The full name of the encoding format.', example='ASS (Advanced SSA) subtitle'),
            codecName?: string(name='CodecName', description='The short name of the encoding format. Valid values:

- **srt**
- **ass**', example='ass'),
            codecTag?: string(name='CodecTag', description='The tag of the encoding format.', example='0x0000'),
            codecTagString?: string(name='CodecTagString', description='The tag string of the encoding format.', example='[0][0][0][0]'),
            codecTimeBase?: string(name='CodecTimeBase', description='The codec time base.', example='0/1'),
            duration?: string(name='Duration', description='The duration. Unit: seconds.', example='1370.116000'),
            index?: string(name='Index', description='The sequence number of the subtitle stream. The value indicates the position of the subtitle stream in all subtitle streams.', example='3'),
            lang?: string(name='Lang', description='The language.', example='eng'),
            startTime?: string(name='StartTime', description='The start time.', example='0.000000'),
            timebase?: string(name='Timebase', description='The time base.', example='1/1000'),
          }
        ](name='SubtitleStream')
        }(name='SubtitleStreamList', description='The subtitle streams. A media file can contain up to four subtitle streams.'),
        videoStreamList?: {
          videoStream?: [ 
          {
            avgFPS?: string(name='AvgFPS', description='The average frame rate.', example='23.976025'),
            bitrate?: string(name='Bitrate', description='The bitrate. Unit: Kbit/s.', example='1496.46'),
            codecLongName?: string(name='CodecLongName', description='The full name of the encoding format.', example='H.264/AVC/MPEG-4 AVC/MPEG-4 part 10'),
            codecName?: string(name='CodecName', description='The short name of the encoding format. Valid values:

*   **h264**
*   **h265**
*   **gif**
*   **webp**', example='h264'),
            codecTag?: string(name='CodecTag', description='The tag of the encoding format.', example='0x31637661'),
            codecTagString?: string(name='CodecTagString', description='The tag string of the encoding format.', example='avc1'),
            codecTimeBase?: string(name='CodecTimeBase', description='The codec time base.', example='1001/48000'),
            colorPrimaries?: string(name='ColorPrimaries', description='The level of color reconstruction.', example='700'),
            colorRange?: string(name='ColorRange', description='The color range.', example='700'),
            colorTransfer?: string(name='ColorTransfer', description='The color channel.', example='R255 G83 B170'),
            dar?: string(name='Dar', description='The display aspect ratio (DAR). DAR is the proportional relationship between the width and the height of a video. The value is used to determine whether the video is in portrait mode or landscape mode.', example='16:9'),
            duration?: string(name='Duration', description='The duration. Unit: seconds.', example='17.225542'),
            fps?: string(name='Fps', description='The frame rate.', example='25'),
            hasBFrames?: string(name='HasBFrames', description='Indicates whether the video stream contains bidirectional frames (B-frames). A value of 1 indicates that the video stream contains B-frames. A value of 0 indicates that the video stream does not contain B-frames.', example='0'),
            height?: string(name='Height', description='The height of the video. Unit: pixels.', example='1080'),
            index?: string(name='Index', description='The sequence number of the video stream. The value indicates the position of the video stream in all video streams. The sequence number of the first video stream to be played can be specified in some players. Default value: 1.', example='1'),
            lang?: string(name='Lang', description='The language.', example='eng'),
            level?: string(name='Level', description='The codec level.', example='41'),
            networkCost?: {
              avgBitrate?: string(name='AvgBitrate', description='The average bitrate. Unit: Kbit/s.', example='300.34'),
              costBandwidth?: string(name='CostBandwidth', description='The maximum bandwidth that was consumed.', example='10'),
              preloadTime?: string(name='PreloadTime', description='The time consumed to preload the video.', example='8'),
            }(name='NetworkCost', description='The network bandwidth that was consumed.'),
            numFrames?: string(name='NumFrames', description='The total number of frames.', example='100'),
            pixFmt?: string(name='PixFmt', description='The pixel format.', example='yuv420p'),
            profile?: string(name='Profile', description='The codec profile.', example='High'),
            rotate?: string(name='Rotate', description='The rotation angle of the video.', example='90'),
            sar?: string(name='Sar', description='The sample aspect ratio (SAR).', example='1:1'),
            startTime?: string(name='StartTime', description='The start time.', example='0.042000'),
            timebase?: string(name='Timebase', description='The time base.', example='1/24000'),
            width?: string(name='Width', description='The width of the video. Unit: pixels.', example='1920'),
          }
        ](name='VideoStream')
        }(name='VideoStreamList', description='The video streams. A media file can contain up to four video streams.'),
      }(name='Streams', description='The media streams contained in the input media file.'),
      width?: string(name='Width', description='The width of the video. Unit: pixels.', example='1920'),
    }(name='Properties', description='The properties of the input media file.'),
    state?: string(name='State', description='The status of the job. Valid values:

*   **Success**: The job is successful.
*   **Fail**: The job fails.
*   **Analyzing**: The job is being run.', example='Analyzing'),
    userData?: string(name='UserData', description='The custom data.', example='testid-001'),
  }(name='MediaInfoJob', description='The details of the media information analysis job.'),
  requestId?: string(name='RequestId', description='The ID of the request. 

>  If a request error occurs, check whether the input media file is valid. If the input media file is valid but the issue persists, [submit a ticket](https://workorder-intl.console.aliyun.com/?spm=5176.12246746.top-nav.ditem-sub.35da7bbcitpQnr#/ticket/createIndex) to contact Alibaba Cloud customer service.', example='62D9BE16-B7D5-550C-A482-7A0F60E09877'),
}

model SubmitMediaInfoJobResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: SubmitMediaInfoJobResponseBody(name='body'),
}

/**
  * After you call the SubmitMediaInfoJob operation, ApsaraVideo Media Processing (MPS) analyzes the input media file and generates the analysis results. You can call the [QueryMediaInfoJobList](~~29221~~) operation to query the analysis results.
  * >  We recommend that you submit a media information analysis job after you confirm that the media file is uploaded to Object Storage Service (OSS). You can configure upload callbacks to be notified of the upload status of files.
  * ## QPS limit
  * You can call this operation up to 50 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request SubmitMediaInfoJobRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return SubmitMediaInfoJobResponse
 */
async function submitMediaInfoJobWithOptions(request: SubmitMediaInfoJobRequest, runtime: Util.RuntimeOptions): SubmitMediaInfoJobResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.async)) {
    query['Async'] = request.async;
  }
  if (!Util.isUnset(request.input)) {
    query['Input'] = request.input;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.pipelineId)) {
    query['PipelineId'] = request.pipelineId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.userData)) {
    query['UserData'] = request.userData;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'SubmitMediaInfoJob',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * After you call the SubmitMediaInfoJob operation, ApsaraVideo Media Processing (MPS) analyzes the input media file and generates the analysis results. You can call the [QueryMediaInfoJobList](~~29221~~) operation to query the analysis results.
  * >  We recommend that you submit a media information analysis job after you confirm that the media file is uploaded to Object Storage Service (OSS). You can configure upload callbacks to be notified of the upload status of files.
  * ## QPS limit
  * You can call this operation up to 50 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request SubmitMediaInfoJobRequest
  * @return SubmitMediaInfoJobResponse
 */
async function submitMediaInfoJob(request: SubmitMediaInfoJobRequest): SubmitMediaInfoJobResponse {
  var runtime = new Util.RuntimeOptions{};
  return submitMediaInfoJobWithOptions(request, runtime);
}

model SubmitSmarttagJobRequest {
  content?: string(name='Content', example='example content ****'),
  contentAddr?: string(name='ContentAddr', example='http://exampleBucket.oss-cn-shanghai.aliyuncs.com/mps-test/ai-tag.mp4'),
  contentType?: string(name='ContentType', example='application/zip'),
  input?: string(name='Input', example='oss://mybucket-****/example-****.mp4'),
  notifyUrl?: string(name='NotifyUrl', example='https://example.com/endpoint/aliyun/ai?id=76401125000***'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  params?: string(name='Params', example='false'),
  pipelineId?: string(name='PipelineId', example='2'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  templateId?: string(name='TemplateId', example='39f8e0bc005e4f309379701645f4****'),
  title?: string(name='Title', example='example-title-****'),
  userData?: string(name='UserData', example='{"key":"value"}'),
}

model SubmitSmarttagJobResponseBody = {
  jobId?: string(name='JobId', example='39f8e0bc005e4f309379701645f4****'),
  requestId?: string(name='RequestId', example='7B117AF5-2A16-412C-B127-FA6175ED1AD0'),
}

model SubmitSmarttagJobResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: SubmitSmarttagJobResponseBody(name='body'),
}

async function submitSmarttagJobWithOptions(request: SubmitSmarttagJobRequest, runtime: Util.RuntimeOptions): SubmitSmarttagJobResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.content)) {
    query['Content'] = request.content;
  }
  if (!Util.isUnset(request.contentAddr)) {
    query['ContentAddr'] = request.contentAddr;
  }
  if (!Util.isUnset(request.contentType)) {
    query['ContentType'] = request.contentType;
  }
  if (!Util.isUnset(request.input)) {
    query['Input'] = request.input;
  }
  if (!Util.isUnset(request.notifyUrl)) {
    query['NotifyUrl'] = request.notifyUrl;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.params)) {
    query['Params'] = request.params;
  }
  if (!Util.isUnset(request.pipelineId)) {
    query['PipelineId'] = request.pipelineId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.templateId)) {
    query['TemplateId'] = request.templateId;
  }
  if (!Util.isUnset(request.title)) {
    query['Title'] = request.title;
  }
  if (!Util.isUnset(request.userData)) {
    query['UserData'] = request.userData;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'SubmitSmarttagJob',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

async function submitSmarttagJob(request: SubmitSmarttagJobRequest): SubmitSmarttagJobResponse {
  var runtime = new Util.RuntimeOptions{};
  return submitSmarttagJobWithOptions(request, runtime);
}

model SubmitSnapshotJobRequest {
  input?: string(name='Input', description='The information about the job input. The value must be a JSON object. You must grant MPS-related permissions to the Object Storage Service (OSS) bucket that stores the OSS object to be used as the job input. To grant the permissions, you can log on to the MPS console, choose Workflows > Media Buckets in the left-side navigation pane, and then click Add Bucket. Then, you must perform URL encoding for the OSS object. Example: `{"Bucket":"example-bucket","Location":"example-location","Object":"example%2Ftest.flv"}`. This example indicates the `"example-bucket.example-location.aliyuncs.com/example/test.flv"` object.

>  The OSS bucket must reside in the same region as your MPS service.', example='{"Bucket":"example-bucket","Location":"example-location","Object":"example%2Ftest.flv"}'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  pipelineId?: string(name='PipelineId', description='The ID of the MPS queue to which you want to submit the snapshot job. To obtain the ID, you can log on to the **MPS console** and choose **Global Settings** > **Pipelines** in the left-side navigation pane.

>  Make sure that an available Message Service (MNS) topic is bound to the specified MPS queue. Otherwise, the relevant messages may fail to be sent as expected.', example='dd3dae411e704030b921e52698e5****'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  snapshotConfig?: string(name='SnapshotConfig', description='The snapshot capturing configuration. For more information, see the "AliyunSnapshotConfig" section in the [Data types](~~29253~~) topic.

>  If you set the Interval parameter that is nested under SnapshotConfig, snapshots are captured at the specified intervals. The default value of the Interval parameter is 10, in seconds. If an input video is short but you specify large values for both the Num and Interval parameters, the actual number of snapshots captured may be smaller than the specified number. For example, if you set the Num parameter to 5 and the Interval parameter to 3 for a video of 10 seconds, the number of snapshots captured cannot reach 5.', example='{"OutputFile":{"Bucket":"example-001","Location":"example-location","Object":"{Count}.jpg"},"Time":"5","Num":"10","Interval":"20"}'),
  userData?: string(name='UserData', description='The custom data. The custom data can contain letters, digits, and hyphens (-) and be up to 1,024 bytes in size. The custom data cannot start with a special character.', example='testid-001'),
}

model SubmitSnapshotJobResponseBody = {
  requestId?: string(name='RequestId', description='The ID of the request.', example='19B6D8C5-A5DD-467A-B435-29D393C71E2D'),
  snapshotJob?: {
    code?: string(name='Code', description='The error code returned when the job fails. This parameter is not returned if the job is successful.', example='ResourceContentBad'),
    count?: string(name='Count', description='The number of snapshots that are captured.', example='1'),
    creationTime?: string(name='CreationTime', description='The time when the job was created.', example='2021-05-19T03:11:48Z'),
    id?: string(name='Id', description='The ID of the snapshot job.', example='f4e3b9ba9f3840c39d6e288056f0****'),
    input?: {
      bucket?: string(name='Bucket', description='The OSS bucket that stores the object.', example='example'),
      location?: string(name='Location', description='The ID of the region in which the OSS bucket that stores the object is located.', example='example-location\\"'),
      object?: string(name='Object', description='The OSS object that is used as the input file.', example='example.flv'),
      roleArn?: string(name='RoleArn', description='The ARN of the specified RAM role. Format: acs:ram::$accountID:role/$roleName.', example='acs:ram::1:role/testrole'),
    }(name='Input', description='The information about the job input.'),
    MNSMessageResult?: {
      errorCode?: string(name='ErrorCode', description='The error code returned when the job fails. This parameter is not returned if the job is successful.', example='InvalidParameter'),
      errorMessage?: string(name='ErrorMessage', description='The error message returned when the job fails. This parameter is not returned if the job is successful.', example='The resource operated InputFile is bad'),
      messageId?: string(name='MessageId', description='The ID of the message. This parameter is not returned if the job fails.', example='799454621135656C7F815F198A76****'),
    }(name='MNSMessageResult', description='The message sent by MNS to notify the user of the job result.'),
    message?: string(name='Message', description='The error message returned when the job fails. This parameter is not returned if the job is successful.', example='The resource operated InputFile is bad'),
    pipelineId?: string(name='PipelineId', description='The ID of the MPS queue to which the snapshot job is submitted.', example='dd3dae411e704030b921e52698e5****'),
    snapshotConfig?: {
      frameType?: string(name='FrameType', description='The snapshot type. Default value: **normal**. Valid values:

*   **normal**: normal frames.
*   **intra**: I-frames (keyframes).

>  If the FrameType parameter is set to intra in the request, only keyframes are captured. If no keyframe is found at a specified time point, the keyframe closest to the specified time point is captured. Keyframes are captured faster than normal frames if the same snapshot rules are applied.', example='intra'),
      height?: string(name='Height', description='The height of a captured snapshot.', example='8'),
      interval?: string(name='Interval', description='The interval for capturing snapshots.

*   If this parameter is specified in the request, snapshots are captured at intervals. The value must be greater than 0 in the request.
*   Unit: seconds.
*   Default value: **10**.', example='20'),
      num?: string(name='Num', description='The number of snapshots. If the Num parameter is set in the request, snapshots are captured at intervals.', example='10'),
      outputFile?: {
        bucket?: string(name='Bucket', description='The OSS bucket that stores the object.', example='example'),
        location?: string(name='Location', description='The ID of the region in which the OSS bucket that stores the object is located.', example='example-location'),
        object?: string(name='Object', description='The OSS object that is generated as the output file of the snapshot job.', example='test.png'),
        roleArn?: string(name='RoleArn', description='The Alibaba Cloud Resource Name (ARN) of the specified RAM role. Format: acs:ram::$accountID:role/$roleName.', example='acs:ram::1:role/testrole'),
      }(name='OutputFile', description='The information about the output file of the snapshot job.'),
      tileOut?: {
        cellHeight?: string(name='CellHeight', description='The height of a single image. The default value is the height of a captured snapshot.', example='100'),
        cellSelStep?: string(name='CellSelStep', description='The stride of a single image.', example='3'),
        cellWidth?: string(name='CellWidth', description='The width of a single image. The default value is the width of a captured snapshot.', example='100'),
        color?: string(name='Color', description='The background color.

*   Default value: **black**.
*   You can set the Color parameter to a **color keyword** or **random** in the request.

>  If you want to set the background color to black, you can specify the color keyword in one of the following three formats: Black, black, and #000000.', example='black'),
        columns?: string(name='Columns', description='The number of columns that the tiled image contains. Default value: **10**.', example='10'),
        isKeepCellPic?: string(name='IsKeepCellPic', description='Indicates whether the single images are retained. Valid values:

*   **true**: The single images are retained.
*   **false**: The single images are not retained.
*   Default value: **true**.', example='false'),
        lines?: string(name='Lines', description='The number of rows that the tiled image contains. Default value: **10**.', example='10'),
        margin?: string(name='Margin', description='The margin width of the tiled image.

*   Default value: **0**.
*   Unit: pixel.', example='5'),
        padding?: string(name='Padding', description='The distance between two consecutive single images in the tiled image.

*   Default value: **0**.
*   Unit: pixel.', example='0'),
      }(name='TileOut', description='The tiling configuration.'),
      tileOutputFile?: {
        bucket?: string(name='Bucket', description='The OSS bucket that stores the object.', example='example'),
        location?: string(name='Location', description='The ID of the region in which the OSS bucket that stores the object is located.', example='example-location'),
        object?: string(name='Object', description='The OSS object that is generated as the output file of the tiling job.', example='example.png'),
        roleArn?: string(name='RoleArn', description='The ARN of the specified RAM role. Format: acs:ram::$accountID:role/$roleName.', example='acs:ram::1:role/testrole'),
      }(name='TileOutputFile', description='The information about the output file of the tiling job.'),
      time?: string(name='Time', description='The start time for capturing snapshots. Unit: milliseconds.', example='5'),
      width?: string(name='Width', description='The width of a captured snapshot.', example='8'),
    }(name='SnapshotConfig', description='The snapshot capturing configuration.'),
    state?: string(name='State', description='The status of the snapshot job. Valid values:

- **Submitted**: The job is submitted.
- **Snapshoting**: The job is being processed.
- **Success**: The job is successful.
- **Fail**: The job fails.', example='Snapshoting'),
    tileCount?: string(name='TileCount', description='The number of single images that are contained in the tiled image.', example='5'),
    userData?: string(name='UserData', description='The custom data.', example='testid-001'),
  }(name='SnapshotJob', description='The information about the snapshot job.'),
}

model SubmitSnapshotJobResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: SubmitSnapshotJobResponseBody(name='body'),
}

/**
  * *   Only JPG images can be generated by calling this operation.
  * *   Asynchronous mode: This operation may return a response before snapshots are captured. Snapshot jobs are queued in the background and asynchronously processed by ApsaraVideo Media Processing (MPS). If the **Interval** or **Num** parameter is set, the snapshot job is processed in asynchronous mode. For information about frequently asked questions (FAQ) about capturing snapshots, see [FAQ about capturing snapshots](~~60805~~).
  * *   Notifications: When you submit a snapshot job, the **PipelineId** parameter is required. An asynchronous message is sent only after the notification feature is enabled for the MPS queue.
  * ## QPS limit
  * You can call this operation up to 50 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request SubmitSnapshotJobRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return SubmitSnapshotJobResponse
 */
async function submitSnapshotJobWithOptions(request: SubmitSnapshotJobRequest, runtime: Util.RuntimeOptions): SubmitSnapshotJobResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.input)) {
    query['Input'] = request.input;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.pipelineId)) {
    query['PipelineId'] = request.pipelineId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.snapshotConfig)) {
    query['SnapshotConfig'] = request.snapshotConfig;
  }
  if (!Util.isUnset(request.userData)) {
    query['UserData'] = request.userData;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'SubmitSnapshotJob',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * *   Only JPG images can be generated by calling this operation.
  * *   Asynchronous mode: This operation may return a response before snapshots are captured. Snapshot jobs are queued in the background and asynchronously processed by ApsaraVideo Media Processing (MPS). If the **Interval** or **Num** parameter is set, the snapshot job is processed in asynchronous mode. For information about frequently asked questions (FAQ) about capturing snapshots, see [FAQ about capturing snapshots](~~60805~~).
  * *   Notifications: When you submit a snapshot job, the **PipelineId** parameter is required. An asynchronous message is sent only after the notification feature is enabled for the MPS queue.
  * ## QPS limit
  * You can call this operation up to 50 times per second per account. Requests that exceed this limit are dropped and you will experience service interruptions. We recommend that you take note of this limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request SubmitSnapshotJobRequest
  * @return SubmitSnapshotJobResponse
 */
async function submitSnapshotJob(request: SubmitSnapshotJobRequest): SubmitSnapshotJobResponse {
  var runtime = new Util.RuntimeOptions{};
  return submitSnapshotJobWithOptions(request, runtime);
}

model SubmitVideoQualityJobRequest {
  input?: string(name='Input', example='oss://example-bucket-****.oss-cn-shanghai.aliyuncs.com/example.mp4'),
  jobParams?: string(name='JobParams', example='0'),
  modelId?: string(name='ModelId', example='null'),
  notifyUrl?: string(name='NotifyUrl', example='mns://142610549622****.mns.cn-shanghai.aliyuncs.com/queues/quequ-****'),
  output?: string(name='Output', example='oss://example-bucket-****.oss-cn-shanghai.aliyuncs.com/example1.txt'),
  pipelineId?: string(name='PipelineId', example='50842ce19a684a189625e9ab777f****'),
  scheduleParams?: string(name='ScheduleParams', example='0'),
  sourceType?: string(name='SourceType', example='Video'),
  userData?: string(name='UserData', example='example data'),
  userId?: long(name='UserId', example='174809843091****'),
}

model SubmitVideoQualityJobResponseBody = {
  code?: string(name='Code', example='Success'),
  jobId?: string(name='JobId', example='0c8f04aa60bd4377a906bd6c91ec****'),
  message?: string(name='Message', example='The job is completed successfully'),
  requestId?: string(name='RequestId', example='5210DBB0-E327-4D45-ADBC-0B83C8796E26'),
}

model SubmitVideoQualityJobResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: SubmitVideoQualityJobResponseBody(name='body'),
}

async function submitVideoQualityJobWithOptions(request: SubmitVideoQualityJobRequest, runtime: Util.RuntimeOptions): SubmitVideoQualityJobResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.input)) {
    query['Input'] = request.input;
  }
  if (!Util.isUnset(request.jobParams)) {
    query['JobParams'] = request.jobParams;
  }
  if (!Util.isUnset(request.modelId)) {
    query['ModelId'] = request.modelId;
  }
  if (!Util.isUnset(request.notifyUrl)) {
    query['NotifyUrl'] = request.notifyUrl;
  }
  if (!Util.isUnset(request.output)) {
    query['Output'] = request.output;
  }
  if (!Util.isUnset(request.pipelineId)) {
    query['PipelineId'] = request.pipelineId;
  }
  if (!Util.isUnset(request.scheduleParams)) {
    query['ScheduleParams'] = request.scheduleParams;
  }
  if (!Util.isUnset(request.sourceType)) {
    query['SourceType'] = request.sourceType;
  }
  if (!Util.isUnset(request.userData)) {
    query['UserData'] = request.userData;
  }
  if (!Util.isUnset(request.userId)) {
    query['UserId'] = request.userId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'SubmitVideoQualityJob',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

async function submitVideoQualityJob(request: SubmitVideoQualityJobRequest): SubmitVideoQualityJobResponse {
  var runtime = new Util.RuntimeOptions{};
  return submitVideoQualityJobWithOptions(request, runtime);
}

model TagCustomPersonRequest {
  categoryDescription?: string(name='CategoryDescription', example='CategoryDescription001-****'),
  categoryId?: string(name='CategoryId', example='CategoryId001-****'),
  categoryName?: string(name='CategoryName', example='CategoryNametest-****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  personDescription?: string(name='PersonDescription', example='PersonDescriptiontest-****'),
  personId?: string(name='PersonId', example='PersonId001-****'),
  personName?: string(name='PersonName', example='PersonNametest-****'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model TagCustomPersonResponseBody = {
  requestId?: string(name='RequestId', example='FD8B5B8C-0C3D-5776-B3B1-EE6AD11F905A'),
}

model TagCustomPersonResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: TagCustomPersonResponseBody(name='body'),
}

async function tagCustomPersonWithOptions(request: TagCustomPersonRequest, runtime: Util.RuntimeOptions): TagCustomPersonResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.categoryDescription)) {
    query['CategoryDescription'] = request.categoryDescription;
  }
  if (!Util.isUnset(request.categoryId)) {
    query['CategoryId'] = request.categoryId;
  }
  if (!Util.isUnset(request.categoryName)) {
    query['CategoryName'] = request.categoryName;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.personDescription)) {
    query['PersonDescription'] = request.personDescription;
  }
  if (!Util.isUnset(request.personId)) {
    query['PersonId'] = request.personId;
  }
  if (!Util.isUnset(request.personName)) {
    query['PersonName'] = request.personName;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'TagCustomPerson',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

async function tagCustomPerson(request: TagCustomPersonRequest): TagCustomPersonResponse {
  var runtime = new Util.RuntimeOptions{};
  return tagCustomPersonWithOptions(request, runtime);
}

model UnbindInputBucketRequest {
  bucket?: string(name='Bucket', description='The name of the input media bucket to be unbound. To obtain the media bucket name, you can log on to the **ApsaraVideo Media Processing (MPS)** console and choose **Workflows** > **Media Buckets** in the left-side navigation pane. Alternatively, you can log on to the **Object Storage Service (OSS) console** and click **My OSS Paths**.', example='example-bucket-****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  roleArn?: string(name='RoleArn', description='The Alibaba Cloud Resource Name (ARN) of the RAM role.

The trusted entity of the RAM role is an Alibaba Cloud account. For more information, see [Create a RAM role for a trusted Alibaba Cloud account](~~93691~~) or [CreateRole](~~28710~~).

Format: `acs:ram::<account_id>:role/<role_name>`.

You can view the ARN in the RAM console or by calling operations.

*   For more information about how to view the ARN in the RAM console, see [How do I find the ARN of the RAM role?](~~39744~~)
*   For more information about how to view the ARN by calling operations, see [ListRoles](~~28713~~) or [GetRole](~~28711~~).', example='acs:ram::174809843091****:role/exampleRole'),
}

model UnbindInputBucketResponseBody = {
  requestId?: string(name='RequestId', description='The ID of the request.', example='4AEA0480-32F4-1656-92B3-F4D4CDE6BBB3'),
}

model UnbindInputBucketResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: UnbindInputBucketResponseBody(name='body'),
}

/**
  * You can call this operation to unbind an input media bucket from the media library based on the name of the output media bucket.
  * ## QPS limit
  * You can call this operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request UnbindInputBucketRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return UnbindInputBucketResponse
 */
async function unbindInputBucketWithOptions(request: UnbindInputBucketRequest, runtime: Util.RuntimeOptions): UnbindInputBucketResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.bucket)) {
    query['Bucket'] = request.bucket;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.roleArn)) {
    query['RoleArn'] = request.roleArn;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'UnbindInputBucket',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * You can call this operation to unbind an input media bucket from the media library based on the name of the output media bucket.
  * ## QPS limit
  * You can call this operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request UnbindInputBucketRequest
  * @return UnbindInputBucketResponse
 */
async function unbindInputBucket(request: UnbindInputBucketRequest): UnbindInputBucketResponse {
  var runtime = new Util.RuntimeOptions{};
  return unbindInputBucketWithOptions(request, runtime);
}

model UnbindOutputBucketRequest {
  bucket?: string(name='Bucket', description='The name of the output media bucket to be unbound. To obtain the media bucket name, you can log on to the **ApsaraVideo Media Processing (MPS)** console and choose **Workflows** > **Media Buckets** in the left-side navigation pane. Alternatively, you can log on to the **Object Storage Service (OSS) console** and click **My OSS Paths**.', example='example-bucket-****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model UnbindOutputBucketResponseBody = {
  requestId?: string(name='RequestId', description='The ID of the request.', example='4AEA0480-32F4-1656-92B3-F4D4CDE6BBB3'),
}

model UnbindOutputBucketResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: UnbindOutputBucketResponseBody(name='body'),
}

/**
  * You can call this operation to unbind an output media bucket from the media library based on the name of the output media bucket.
  * ## QPS limit
  * You can call this operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request UnbindOutputBucketRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return UnbindOutputBucketResponse
 */
async function unbindOutputBucketWithOptions(request: UnbindOutputBucketRequest, runtime: Util.RuntimeOptions): UnbindOutputBucketResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.bucket)) {
    query['Bucket'] = request.bucket;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'UnbindOutputBucket',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * You can call this operation to unbind an output media bucket from the media library based on the name of the output media bucket.
  * ## QPS limit
  * You can call this operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request UnbindOutputBucketRequest
  * @return UnbindOutputBucketResponse
 */
async function unbindOutputBucket(request: UnbindOutputBucketRequest): UnbindOutputBucketResponse {
  var runtime = new Util.RuntimeOptions{};
  return unbindOutputBucketWithOptions(request, runtime);
}

model UnregisterCustomFaceRequest {
  categoryId?: string(name='CategoryId', example='CategoryId001-****'),
  faceId?: string(name='FaceId', example='15****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  personId?: string(name='PersonId', example='PersonId001-****'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model UnregisterCustomFaceResponseBody = {
  requestId?: string(name='RequestId', example='1A3347BF-7BCE-40A6-B33E-43C2B8A9A278'),
}

model UnregisterCustomFaceResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: UnregisterCustomFaceResponseBody(name='body'),
}

async function unregisterCustomFaceWithOptions(request: UnregisterCustomFaceRequest, runtime: Util.RuntimeOptions): UnregisterCustomFaceResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.categoryId)) {
    query['CategoryId'] = request.categoryId;
  }
  if (!Util.isUnset(request.faceId)) {
    query['FaceId'] = request.faceId;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.personId)) {
    query['PersonId'] = request.personId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'UnregisterCustomFace',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

async function unregisterCustomFace(request: UnregisterCustomFaceRequest): UnregisterCustomFaceResponse {
  var runtime = new Util.RuntimeOptions{};
  return unregisterCustomFaceWithOptions(request, runtime);
}

model UpdateMediaRequest {
  cateId?: long(name='CateId', description='The ID of the category to which the media file belongs. The value must be an integer.

*   If you do not specify this parameter, the value is NULL.
*   The value cannot be negative.', example='1'),
  coverURL?: string(name='CoverURL', description='The storage location of the thumbnail that you want to specify for the media file. To obtain the URL, you can log on to the **MPS console** and choose **Workflows** > **Media Buckets**. Alternatively, you can log on to the **Object Storage Service (OSS) console** and click **My OSS Paths**.

*   The value can be up to 3,200 bytes in length.
*   The URL complies with RFC 2396 and is encoded in UTF-8, with reserved characters being percent-encoded.', example='http://example-bucket-****.oss-cn-hangzhou.aliyuncs.com/test****.jpg'),
  description?: string(name='Description', description='The description of the media file. Multiple character types, such as letters and digits, are supported.

*   If you do not specify this parameter, the value is NULL.
*   The value is encoded in UTF-8 and can be up to 1,024 bytes in length.', example='example description'),
  mediaId?: string(name='MediaId', description='The ID of the media file whose basic information you want to update. To obtain the ID of the media file, you can log on to the **ApsaraVideo Media Processing (MPS) console** and choose **Media Management** > **Media List** in the left-side navigation pane.', example='3e1cd21131a94525be55acf65888****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  tags?: string(name='Tags', description='The tags that you want to add for the media file.

*   Separate multiple tags with commas (,). You can specify up to 16 tags for a media file.
*   Each tag can be up to 32 bytes in length.
*   The value is encoded in UTF-8.', example='tag1,tag2'),
  title?: string(name='Title', description='The title of the media file. Multiple character types, such as letters and digits, are supported.

*   If you do not specify this parameter, the value is NULL.
*   The value is encoded in UTF-8 and can be up to 128 bytes in length.', example='hello'),
}

model UpdateMediaResponseBody = {
  media?: {
    bitrate?: string(name='Bitrate', description='The bitrate of the media file.', example='2659.326'),
    cateId?: long(name='CateId', description='The ID of the category to which the media file belongs.', example='1'),
    censorState?: string(name='CensorState', description='The review status of the video. Valid values:

*   **Initiated**: The media file is uploaded but not reviewed.
*   **Pass**: The media file is uploaded and passes the review.', example='Initiated'),
    coverURL?: string(name='CoverURL', description='The storage location of the media thumbnail.', example='http://example-bucket-****.oss-cn-shanghai.aliyuncs.com/example-****.jpg'),
    creationTime?: string(name='CreationTime', description='The time when the media file was created.', example='2016-09-14T08:30:33Z'),
    description?: string(name='Description', description='The description of the media file.', example='example description'),
    duration?: string(name='Duration', description='The duration of the media file.', example='7.965000'),
    file?: {
      state?: string(name='State', description='The status of the input file. Valid values:

*   **Normal**: The input file is normal.
*   **Deleted**: The input file is deleted.', example='Normal'),
      URL?: string(name='URL', description='The URL of the input file.', example='http://example-bucket-****.oss-cn-hangzhou.aliyuncs.com//example-****.mp4'),
    }(name='File', description='The information about the input file.'),
    format?: string(name='Format', description='The format of the media file. Valid values: mov, mp4, m4a, 3gp, 3g2, and mj2.', example='mov'),
    fps?: string(name='Fps', description='The frame rate of the media file.', example='25.0'),
    height?: string(name='Height', description='The height of the media file.', example='1080'),
    mediaId?: string(name='MediaId', description='The ID of the media file.', example='3e1cd21131a94525be55acf65888****'),
    publishState?: string(name='PublishState', description='The publishing status of the media file. Valid values:

*   **Initiated**: The media file is in the initial state.
*   **UnPublish**: The media file has not been published, and the playback permission on the OSS object is Private.
*   **Published**: The media file has been published, and the playback permission on the OSS object is Default.
*   **Deleted**: The media file is deleted.', example='Published'),
    runIdList?: {
      runId?: [ string ](name='RunId')
    }(name='RunIdList', description='The IDs of the media workflow execution instances.'),
    size?: string(name='Size', description='The size of the media file.', example='2647692'),
    tags?: {
      tag?: [ string ](name='Tag')
    }(name='Tags', description='The tags of the media file.'),
    title?: string(name='Title', description='The title of the media file.', example='hello'),
    width?: string(name='Width', description='The width of the media file.', example='1920'),
  }(name='Media', description='The detailed information about the media file.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='6A88246F-C91F-42BD-BABE-DB0DF993F960'),
}

model UpdateMediaResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: UpdateMediaResponseBody(name='body'),
}

/**
  * The basic information that you can update by calling this operation includes the title, description, and category of a media file. This operation applies to a full update. You must set all the parameters unless you want to replace the value of a specific parameter with a NULL value.
  * ## Limits on QPS
  * You can call this operation up to 100 times per second. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request UpdateMediaRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return UpdateMediaResponse
 */
async function updateMediaWithOptions(request: UpdateMediaRequest, runtime: Util.RuntimeOptions): UpdateMediaResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.cateId)) {
    query['CateId'] = request.cateId;
  }
  if (!Util.isUnset(request.coverURL)) {
    query['CoverURL'] = request.coverURL;
  }
  if (!Util.isUnset(request.description)) {
    query['Description'] = request.description;
  }
  if (!Util.isUnset(request.mediaId)) {
    query['MediaId'] = request.mediaId;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.tags)) {
    query['Tags'] = request.tags;
  }
  if (!Util.isUnset(request.title)) {
    query['Title'] = request.title;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'UpdateMedia',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * The basic information that you can update by calling this operation includes the title, description, and category of a media file. This operation applies to a full update. You must set all the parameters unless you want to replace the value of a specific parameter with a NULL value.
  * ## Limits on QPS
  * You can call this operation up to 100 times per second. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request UpdateMediaRequest
  * @return UpdateMediaResponse
 */
async function updateMedia(request: UpdateMediaRequest): UpdateMediaResponse {
  var runtime = new Util.RuntimeOptions{};
  return updateMediaWithOptions(request, runtime);
}

model UpdateMediaCategoryRequest {
  cateId?: long(name='CateId', description='The ID of the category. The value cannot be negative.', example='1'),
  mediaId?: string(name='MediaId', description='The ID of the media file whose category you want to update.

>  To obtain the ID of a media file, you can call the [AddMedia](~~44458~~) operation. Alternatively, perform the following operations in the ApsaraVideo Media Processing (MPS) console: In the left-side navigation pane, choose **Media Management > Media List**. Find the required video and click **Manage**. The ID of the video is displayed on the Basics tab.', example='3e1cd21131a94525be55acf65888****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model UpdateMediaCategoryResponseBody = {
  requestId?: string(name='RequestId', description='The ID of the request.', example='E3931857-E3D3-4D6E-9C7B-D2C09441BD01'),
}

model UpdateMediaCategoryResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: UpdateMediaCategoryResponseBody(name='body'),
}

/**
  * You can call this operation to update only the category of a media file. For more information about how to update all the information about a media file, see [UpdateMedia](~~44464~~).
  *
  * @param request UpdateMediaCategoryRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return UpdateMediaCategoryResponse
 */
async function updateMediaCategoryWithOptions(request: UpdateMediaCategoryRequest, runtime: Util.RuntimeOptions): UpdateMediaCategoryResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.cateId)) {
    query['CateId'] = request.cateId;
  }
  if (!Util.isUnset(request.mediaId)) {
    query['MediaId'] = request.mediaId;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'UpdateMediaCategory',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * You can call this operation to update only the category of a media file. For more information about how to update all the information about a media file, see [UpdateMedia](~~44464~~).
  *
  * @param request UpdateMediaCategoryRequest
  * @return UpdateMediaCategoryResponse
 */
async function updateMediaCategory(request: UpdateMediaCategoryRequest): UpdateMediaCategoryResponse {
  var runtime = new Util.RuntimeOptions{};
  return updateMediaCategoryWithOptions(request, runtime);
}

model UpdateMediaCoverRequest {
  coverURL?: string(name='CoverURL', description='The URL of the thumbnail that you want to specify for the media file. The URL complies with RFC 2396 and is encoded in UTF-8. The URL can be up to 3,200 bytes in length.

>  To obtain the thumbnail URL, you can find the image in the Object Storage Service (OSS) bucket and click the image to view details. In the View Details panel, copy the part before the question mark (?) from the URL field. You can enter only an HTTP URL.', example='http://example-bucket-****.oss-cn-hangzhou.aliyuncs.com//example-****.mp4'),
  mediaId?: string(name='MediaId', description='The ID of the media file whose thumbnail you want to update. To obtain the ID of a media file, you can call the [AddMedia](~~44458~~) operation. Alternatively, perform the following operations in the ApsaraVideo Media Processing (MPS) console: In the left-side navigation pane, choose **Media Management** > **Media List**. Find the required video and click **Manage**. The ID of the video is displayed on the Basics tab.', example='6cc3aa66d1cb4bb2adf14e726c0a****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model UpdateMediaCoverResponseBody = {
  requestId?: string(name='RequestId', description='The ID of the request.', example='0DC39B9E-13D4-40BA-AE76-CFF9BD64239D'),
}

model UpdateMediaCoverResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: UpdateMediaCoverResponseBody(name='body'),
}

/**
  * You can call this operation to update only the thumbnail of a media file. For more information about how to update all the information about a media file, see [UpdateMedia](~~44464~~).
  * ## Limits on QPS
  * You can call this operation up to 100 times per second. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request UpdateMediaCoverRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return UpdateMediaCoverResponse
 */
async function updateMediaCoverWithOptions(request: UpdateMediaCoverRequest, runtime: Util.RuntimeOptions): UpdateMediaCoverResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.coverURL)) {
    query['CoverURL'] = request.coverURL;
  }
  if (!Util.isUnset(request.mediaId)) {
    query['MediaId'] = request.mediaId;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'UpdateMediaCover',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * You can call this operation to update only the thumbnail of a media file. For more information about how to update all the information about a media file, see [UpdateMedia](~~44464~~).
  * ## Limits on QPS
  * You can call this operation up to 100 times per second. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request UpdateMediaCoverRequest
  * @return UpdateMediaCoverResponse
 */
async function updateMediaCover(request: UpdateMediaCoverRequest): UpdateMediaCoverResponse {
  var runtime = new Util.RuntimeOptions{};
  return updateMediaCoverWithOptions(request, runtime);
}

model UpdateMediaPublishStateRequest {
  mediaId?: string(name='MediaId', description='The ID of the media file whose publishing status you want update. To obtain the ID of a media file, you can call the [AddMedia](~~44458~~) operation. Alternatively, perform the following operations in the ApsaraVideo Media Processing (MPS) console: In the left-side navigation pane, choose **Media Management** > **Media List**. Find the required video and click **Manage**. The ID of the video is displayed on the Basics tab.', example='3e6149d5a8c944c09b1a8d2dc3e4****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  publish?: boolean(name='Publish', description='The publishing status. Default value: **Initialed**. Valid values:

*   **true**: published
*   **false**: unpublished', example='true'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
}

model UpdateMediaPublishStateResponseBody = {
  requestId?: string(name='RequestId', description='The ID of the request.', example='91B6CAB9-034C-4E4E-A40B-E7F5C81E688C'),
}

model UpdateMediaPublishStateResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: UpdateMediaPublishStateResponseBody(name='body'),
}

/**
  * The published state indicates that the access control list (ACL) of media playback resources and snapshot files is set to inherit the ACL of the bucket to which they belong. The unpublished state indicates that the ACL of media playback resources and snapshot files is set to private.
  * ## QPS limit
  * You can call this operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request UpdateMediaPublishStateRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return UpdateMediaPublishStateResponse
 */
async function updateMediaPublishStateWithOptions(request: UpdateMediaPublishStateRequest, runtime: Util.RuntimeOptions): UpdateMediaPublishStateResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.mediaId)) {
    query['MediaId'] = request.mediaId;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.publish)) {
    query['Publish'] = request.publish;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'UpdateMediaPublishState',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * The published state indicates that the access control list (ACL) of media playback resources and snapshot files is set to inherit the ACL of the bucket to which they belong. The unpublished state indicates that the ACL of media playback resources and snapshot files is set to private.
  * ## QPS limit
  * You can call this operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request UpdateMediaPublishStateRequest
  * @return UpdateMediaPublishStateResponse
 */
async function updateMediaPublishState(request: UpdateMediaPublishStateRequest): UpdateMediaPublishStateResponse {
  var runtime = new Util.RuntimeOptions{};
  return updateMediaPublishStateWithOptions(request, runtime);
}

model UpdateMediaWorkflowRequest {
  mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the media workflow that you want to update. To obtain the ID of the media workflow, you can log on to the **ApsaraVideo Media Processing (MPS) console** and choose **Workflows** > **Workflow Settings** in the left-side navigation pane.', example='6307eb0d3f85477882d205aa040f****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  topology?: string(name='Topology', description='The new topology of the media workflow. The value is a JSON object that contains the activity list and activity dependencies.', example='{"Activities":{"Act-Start":{"Parameters":{"PipelineId":"130266f58161436a80bf07cb12c8****","InputFile":"{\\"Bucket\\": \\"example-bucket-****\\",\\"Location\\": \\"cn-shanghai\\"}"},"Type":"Start"},"Act-Report":{"Parameters":{},"Type":"Report"},"Act-Transcode-M3U8":{"Parameters":{"Outputs":"[{\\"Object\\":\\"transcode/{ObjectPrefix}{FileName}\\",\\"TemplateId\\": \\"957d1719ee85ed6527b90cf62726****\\"}]","OutputBucket":"example-bucket-****","OutputLocation":"cn-shanghai"},"Type":"Transcode"}},"Dependencies":{"Act-Start":["Act-Transcode-M3U8"],"Act-Report":[],"Act-Transcode-M3U8":["Act-Report"]}}'),
}

model UpdateMediaWorkflowResponseBody = {
  mediaWorkflow?: {
    creationTime?: string(name='CreationTime', description='The time when the media workflow was created.', example='2016-04-01T05:29:38Z'),
    mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the media workflow that is updated.', example='6307eb0d3f85477882d205aa040f****'),
    name?: string(name='Name', description='The name of the media workflow.', example='examp-mediaworkflow-****'),
    state?: string(name='State', description='The status of the media workflow. Valid values:

*   **Inactive**: The media workflow is deactivated.
*   **Active**: The media workflow is activated.', example='Active'),
    topology?: string(name='Topology', description='The topology of the media workflow.', example='{
      "Activities": {
            "Act-Start": {
                  "Parameters": {
                        "PipelineId": "130266f58161436a80bf07cb12c8****",
                        "InputFile": "{\\"Bucket\\": \\"example-bucket-****\\",\\"Location\\": \\"cn-shanghai\\"}"
                  },
                  "Type": "Start"
            },
            "Act-Report": {
                  "Parameters": {},
                  "Type": "Report"
            },
            "Act-Transcode-M3U8": {
                  "Parameters": {
                        "Outputs": "[{\\"Object\\":\\"transcode/{ObjectPrefix}{FileName}\\",\\"TemplateId\\": \\"957d1719ee85ed6527b90cf62726****\\"}]",
                        "OutputBucket": "example-bucket-****",
                        "OutputLocation": "cn-shanghai"
                  },
                  "Type": "Transcode"
            }
      },
      "Dependencies": {
            "Act-Start": [
                  "Act-Transcode-M3U8"
            ],
            "Act-Report": [],
            "Act-Transcode-M3U8": [
                  "Act-Report"
            ]
      }
}'),
    triggerMode?: string(name='TriggerMode', description='The trigger mode of the media workflow. Valid values:

*   **OssAutoTrigger**: The media workflow is automatically triggered.
*   **NotInAuto**: The media workflow is not automatically triggered.', example='OssAutoTrigger'),
  }(name='MediaWorkflow', description='The details of the media workflow.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='7D752035-97DA-54E5-88E2-E8405EEA****'),
}

model UpdateMediaWorkflowResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: UpdateMediaWorkflowResponseBody(name='body'),
}

/**
  * *   You can call this operation to update the topology of a media workflow. To update the trigger mode of a media workflow, call the [UpdateMediaWorkflowTriggerMode](~~70372~~) operation.
  * *   After you delete or deactivate a media workflow, the workflow cannot be used. In this case, the workflow is not automatically triggered when you upload a file to the bucket specified by the workflow.
  * ## Limits on QPS
  * You can call this operation up to 100 times per second. If the number of the calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request UpdateMediaWorkflowRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return UpdateMediaWorkflowResponse
 */
async function updateMediaWorkflowWithOptions(request: UpdateMediaWorkflowRequest, runtime: Util.RuntimeOptions): UpdateMediaWorkflowResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.mediaWorkflowId)) {
    query['MediaWorkflowId'] = request.mediaWorkflowId;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.topology)) {
    query['Topology'] = request.topology;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'UpdateMediaWorkflow',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * *   You can call this operation to update the topology of a media workflow. To update the trigger mode of a media workflow, call the [UpdateMediaWorkflowTriggerMode](~~70372~~) operation.
  * *   After you delete or deactivate a media workflow, the workflow cannot be used. In this case, the workflow is not automatically triggered when you upload a file to the bucket specified by the workflow.
  * ## Limits on QPS
  * You can call this operation up to 100 times per second. If the number of the calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request UpdateMediaWorkflowRequest
  * @return UpdateMediaWorkflowResponse
 */
async function updateMediaWorkflow(request: UpdateMediaWorkflowRequest): UpdateMediaWorkflowResponse {
  var runtime = new Util.RuntimeOptions{};
  return updateMediaWorkflowWithOptions(request, runtime);
}

model UpdateMediaWorkflowTriggerModeRequest {
  mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the media workflow to be updated. To obtain the workflow ID, you can log on to the **ApsaraVideo Media Processing (MPS) console** and choose **Workflows** > **Workflow Settings**.', example='e00732b977da427d9177a4dee646****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  triggerMode?: string(name='TriggerMode', description='The trigger mode of the media workflow. Valid values:

*   **OssAutoTrigger**: automatically triggers the media workflow.
*   **NotInAuto**: does not automatically trigger the media workflow.', example='NotInAuto'),
}

model UpdateMediaWorkflowTriggerModeResponseBody = {
  mediaWorkflow?: {
    creationTime?: string(name='CreationTime', description='The time when the media workflow was created.', example='2016-04-01T05:29:37Z'),
    mediaWorkflowId?: string(name='MediaWorkflowId', description='The ID of the media workflow.', example='e00732b977da427d9177a4dee646****'),
    name?: string(name='Name', description='The name of the media workflow.', example='example-mediaworkflow-****'),
    state?: string(name='State', description='The status of the media workflow. Valid values:

*   **Inactive**: disabled
*   **Active**: enabled', example='Inactive'),
    topology?: string(name='Topology', description='The topology of the media workflow.', example='{mediaworkflow","State":"Active","Topology":"{\\"Activities\\":{\\"Act-Start\\":{\\"Parameters\\":{\\"PipelineId\\":\\"130266f58161436a80bf07cb12c8****\\",\\"InputFile\\":\\"{\\\\\\"Bucket\\\\\\": \\\\\\"example-bucket-****\\\\\\",\\\\\\"Location\\\\\\": \\\\\\"cn-shanghai\\\\\\"}\\"},\\"Type\\":\\"Start\\"},\\"Act-Report\\":{\\"Parameters\\":{},\\"Type\\":\\"Report\\"},\\"Act-Transcode-M3U8\\":{\\"Parameters\\":{\\"Outputs\\":\\"[{\\\\\\"Object\\\\\\":\\\\\\"transcode/{ObjectPrefix}{FileName}\\\\\\",\\\\\\"TemplateId\\\\\\": \\\\\\"957d1719ee85ed6527b90cf62726****\\\\\\"}]\\",\\"OutputBucket\\":\\"example-bucket-****\\",\\"OutputLocation\\":\\"cn-shanghai\\"},\\"Type\\":\\"Transcode\\"}},\\"Dependencies\\":{\\"Act-Start\\":[\\"Act-Transcode-M3U8\\"],\\"Act-Report\\":[],\\"Act-Transcode-M3U8\\":[\\"Act-Report\\"]}}","MediaWorkflowId":"93ab850b4f6f44eab54b6e91d24d****"}]},"RequestId":"16CD0CDD-457E-420D-9755-8385075A1234"}'),
    triggerMode?: string(name='TriggerMode', description='The trigger mode of the media workflow. Valid values:

*   **OssAutoTrigger**: The media workflow is automatically triggered.
*   **NotInAuto**: The media workflow is not automatically triggered.', example='NotInAuto'),
  }(name='MediaWorkflow', description='The details of the media workflow.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='16CD0CDD-457E-420D-9755-8385075A1234'),
}

model UpdateMediaWorkflowTriggerModeResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: UpdateMediaWorkflowTriggerModeResponseBody(name='body'),
}

/**
  * You can call this operation only to modify the trigger mode of a workflow. To modify other information, call the [UpdateMediaWorkflow](~~44438~~) operation.
  * ## QPS limit
  * You can call this operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request UpdateMediaWorkflowTriggerModeRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return UpdateMediaWorkflowTriggerModeResponse
 */
async function updateMediaWorkflowTriggerModeWithOptions(request: UpdateMediaWorkflowTriggerModeRequest, runtime: Util.RuntimeOptions): UpdateMediaWorkflowTriggerModeResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.mediaWorkflowId)) {
    query['MediaWorkflowId'] = request.mediaWorkflowId;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.triggerMode)) {
    query['TriggerMode'] = request.triggerMode;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'UpdateMediaWorkflowTriggerMode',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * You can call this operation only to modify the trigger mode of a workflow. To modify other information, call the [UpdateMediaWorkflow](~~44438~~) operation.
  * ## QPS limit
  * You can call this operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request UpdateMediaWorkflowTriggerModeRequest
  * @return UpdateMediaWorkflowTriggerModeResponse
 */
async function updateMediaWorkflowTriggerMode(request: UpdateMediaWorkflowTriggerModeRequest): UpdateMediaWorkflowTriggerModeResponse {
  var runtime = new Util.RuntimeOptions{};
  return updateMediaWorkflowTriggerModeWithOptions(request, runtime);
}

model UpdatePipelineRequest {
  extendConfig?: string(name='ExtendConfig'),
  name?: string(name='Name', description='The new name of the MPS queue. The value can contain letters, digits, and special characters such as hyphens (-) and can be up to 128 bytes in size. The value cannot start with a special character.', example='example-pipeline-****'),
  notifyConfig?: string(name='NotifyConfig', description='The Message Service (MNS) configuration, such as the information about the MNS queue or topic. For more information, see [NotifyConfig](~~29253~~).', example='{"Topic":"example-topic-****"}'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  pipelineId?: string(name='PipelineId', description='The ID of the MPS queue that you want to update. To obtain the ID of the MPS queue, you can log on to the **MPS console** and choose **Global Settings** > **Pipelines** in the left-side navigation pane.', example='d1ce4d3efcb549419193f50f1fcd****'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  role?: string(name='Role', description='The role that is assigned to the current RAM user. To obtain the role, you can log on to the **RAM console** and choose **Identities** > **Roles** in the left-side navigation pane.', example='AliyunMTSDefaultRole'),
  state?: string(name='State', description='The new state of the MPS queue.

*   **Active**: The MPS queue is active. Jobs in the MPS queue can be scheduled and run by MPS.
*   **Paused**: The MPS queue is paused. Jobs in the MPS queue cannot be scheduled or run by MPS, and all jobs remain in the Submitted state. Jobs that are running will not be affected.', example='Paused'),
}

model UpdatePipelineResponseBody = {
  pipeline?: {
    extendConfig?: {
      isBoostNew?: boolean(name='IsBoostNew'),
      maxMultiSpeed?: int32(name='MaxMultiSpeed'),
      multiSpeedDowngradePolicy?: string(name='MultiSpeedDowngradePolicy'),
    }(name='ExtendConfig'),
    id?: string(name='Id', description='The ID of the MPS queue.', example='d1ce4d3efcb549419193f50f1fcd****'),
    name?: string(name='Name', description='The new name of the MPS queue.', example='example-pipeline-****'),
    notifyConfig?: {
      mqTag?: string(name='MqTag', description='The tags of the messages.', example='mts-test'),
      mqTopic?: string(name='MqTopic', description='The queue of messages that are received.', example='example1,example2'),
      queueName?: string(name='QueueName', description='The queue that is created in MNS.', example='example-queue-****'),
      topic?: string(name='Topic', description='The topic that is created in MNS.', example='example-topic-****'),
    }(name='NotifyConfig', description='The MNS configuration.'),
    quotaAllocate?: long(name='QuotaAllocate', description='The quota that is allocated to the MPS queue.', example='10'),
    role?: string(name='Role', description='The role that is assigned to the current RAM user.', example='AliyunMTSExampleRole'),
    speed?: string(name='Speed', description='The type of the MPS queue. Default value: **Standard**. Valid values:

*   **Boost**: MPS queue with transcoding speed boosted.
*   **Standard**: standard MPS queue.
*   **NarrowBandHDV2**: MPS queue that supports Narrowband HD™ 2.0.
*   **AIVideoCover**: MPS queue for intelligent snapshot capture.
*   **AIVideoFPShot**: MPS queue for media fingerprinting.
*   **AIVideoCensor**: MPS queue for automated review.
*   **AIVideoMCU**: MPS queue for smart tagging.
*   **AIVideoSummary**: MPS queue for video synopsis.
*   **AIVideoPorn**: MPS queue for pornography detection in videos.
*   **AIAudioKWS**: MPS queue for keyword recognition in audio.
*   **AIAudioASR**: MPS queue for speech-to-text conversion.', example='Standard'),
    state?: string(name='State', description='The status of the pipeline. Valid values:

*   **Active**: The MPS queue is active.
*   **Paused**: The MPS queue is paused.', example='Paused'),
  }(name='Pipeline', description='The details of the MPS queue.'),
  requestId?: string(name='RequestId', description='The ID of the request.', example='1FE0F96B-544D-4244-9D83-DFCFB0E5A231'),
}

model UpdatePipelineResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: UpdatePipelineResponseBody(name='body'),
}

/**
  * *   You can call this operation to modify the name, status, and notification settings of a specified MPS queue.
  * *   If a paused MPS queue is selected in a workflow or a job, such as a video review or media fingerprint job, the workflow or job fails.
  * ## Limits on QPS
  * You can call this operation up to 100 times per second. If the number of the calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request UpdatePipelineRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return UpdatePipelineResponse
 */
async function updatePipelineWithOptions(request: UpdatePipelineRequest, runtime: Util.RuntimeOptions): UpdatePipelineResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.extendConfig)) {
    query['ExtendConfig'] = request.extendConfig;
  }
  if (!Util.isUnset(request.name)) {
    query['Name'] = request.name;
  }
  if (!Util.isUnset(request.notifyConfig)) {
    query['NotifyConfig'] = request.notifyConfig;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.pipelineId)) {
    query['PipelineId'] = request.pipelineId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.role)) {
    query['Role'] = request.role;
  }
  if (!Util.isUnset(request.state)) {
    query['State'] = request.state;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'UpdatePipeline',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * *   You can call this operation to modify the name, status, and notification settings of a specified MPS queue.
  * *   If a paused MPS queue is selected in a workflow or a job, such as a video review or media fingerprint job, the workflow or job fails.
  * ## Limits on QPS
  * You can call this operation up to 100 times per second. If the number of the calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request UpdatePipelineRequest
  * @return UpdatePipelineResponse
 */
async function updatePipeline(request: UpdatePipelineRequest): UpdatePipelineResponse {
  var runtime = new Util.RuntimeOptions{};
  return updatePipelineWithOptions(request, runtime);
}

model UpdateSmarttagTemplateRequest {
  analyseTypes?: string(name='AnalyseTypes', example='ocr,asr'),
  faceCategoryIds?: string(name='FaceCategoryIds', example='celebrity'),
  faceCustomParamsConfig?: string(name='FaceCustomParamsConfig', example='{ "faceDetThreshold":0.999, "faceRegThreshold":0.9 }'),
  industry?: string(name='Industry', example='common'),
  isDefault?: boolean(name='IsDefault', example='true'),
  keywordConfig?: string(name='KeywordConfig', example='{ "type": "name,location,organization,other" }'),
  knowledgeConfig?: string(name='KnowledgeConfig', example='{ "movie":"name,alias,chnl,genre", "music":"songName,artistName", "person":"name,gender" }'),
  labelType?: string(name='LabelType', example='hmi'),
  labelVersion?: string(name='LabelVersion', example='1.0'),
  landmarkGroupIds?: string(name='LandmarkGroupIds', example='common'),
  objectGroupIds?: string(name='ObjectGroupIds', example='general,item,weapon,animal'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  scene?: string(name='Scene', example='search'),
  templateId?: string(name='TemplateId', example='05de22f255284c7a8d2aab535dde****'),
  templateName?: string(name='TemplateName', example='template-example-****'),
}

model UpdateSmarttagTemplateResponseBody = {
  requestId?: string(name='RequestId', example='5210DBB0-E327-4D45-ADBC-0B83C8796E26'),
}

model UpdateSmarttagTemplateResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: UpdateSmarttagTemplateResponseBody(name='body'),
}

async function updateSmarttagTemplateWithOptions(request: UpdateSmarttagTemplateRequest, runtime: Util.RuntimeOptions): UpdateSmarttagTemplateResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.analyseTypes)) {
    query['AnalyseTypes'] = request.analyseTypes;
  }
  if (!Util.isUnset(request.faceCategoryIds)) {
    query['FaceCategoryIds'] = request.faceCategoryIds;
  }
  if (!Util.isUnset(request.faceCustomParamsConfig)) {
    query['FaceCustomParamsConfig'] = request.faceCustomParamsConfig;
  }
  if (!Util.isUnset(request.industry)) {
    query['Industry'] = request.industry;
  }
  if (!Util.isUnset(request.isDefault)) {
    query['IsDefault'] = request.isDefault;
  }
  if (!Util.isUnset(request.keywordConfig)) {
    query['KeywordConfig'] = request.keywordConfig;
  }
  if (!Util.isUnset(request.knowledgeConfig)) {
    query['KnowledgeConfig'] = request.knowledgeConfig;
  }
  if (!Util.isUnset(request.labelType)) {
    query['LabelType'] = request.labelType;
  }
  if (!Util.isUnset(request.labelVersion)) {
    query['LabelVersion'] = request.labelVersion;
  }
  if (!Util.isUnset(request.landmarkGroupIds)) {
    query['LandmarkGroupIds'] = request.landmarkGroupIds;
  }
  if (!Util.isUnset(request.objectGroupIds)) {
    query['ObjectGroupIds'] = request.objectGroupIds;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.scene)) {
    query['Scene'] = request.scene;
  }
  if (!Util.isUnset(request.templateId)) {
    query['TemplateId'] = request.templateId;
  }
  if (!Util.isUnset(request.templateName)) {
    query['TemplateName'] = request.templateName;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'UpdateSmarttagTemplate',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

async function updateSmarttagTemplate(request: UpdateSmarttagTemplateRequest): UpdateSmarttagTemplateResponse {
  var runtime = new Util.RuntimeOptions{};
  return updateSmarttagTemplateWithOptions(request, runtime);
}

model UpdateTemplateRequest {
  audio?: string(name='Audio', description='The configuration of the audio stream. The value is a JSON object. For more information, see [Audio](~~29253~~).', example='{"Codec":"aac","Samplerate":"44100","Bitrate":"500","Channels":"2"}'),
  container?: string(name='Container', description='The container format. The value is a JSON object. Default format: **MP4**.

*   Video formats: FLV, MP4, HLS (m3u8 + TS), and MPEG-DASH (MPD + fMP4)
*   Audio formats: MP3, MP4, Ogg, FLAC, and M4A
*   Images formats: GIF and WebP

For more information, see [Container](~~29253~~).', example='{"Format":"mp4"}'),
  muxConfig?: string(name='MuxConfig', description='The transmuxing configuration. The value is a JSON object. For more information, see [MuxConfig](~~29253~~).', example='{"Segment":{"Duration":"10"}}'),
  name?: string(name='Name', description='The name of the template. The value can be up to 128 bytes in size.', example='MPS-example'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  templateId?: string(name='TemplateId', description='The ID of the template. You can obtain the template ID from the response of the [AddTemplate](~~213306~~) operation.', example='16f01ad6175e4230ac42bb5182cd****'),
  transConfig?: string(name='TransConfig', description='The general transcoding configuration. The value is a JSON object. For more information, see [TransConfig](~~29253~~).', example='{"TransMode":"onepass"}'),
  video?: string(name='Video', description='The configuration of the video stream. The value is a JSON object. For more information, see [Video](~~29253~~).', example='{"Codec":"H.264","Profile":"high","Bitrate":"500","Crf":"15","Width":"256","Height":"800","Fps":"25","Gop":"10"}'),
}

model UpdateTemplateResponseBody = {
  requestId?: string(name='RequestId', description='The ID of the request.', example='5E4FB22E-B9EA-4E24-8FFC-B407EA71QW21'),
  template?: {
    audio?: {
      bitrate?: string(name='Bitrate', description='The audio bitrate of the output file.

*   Valid values: **\\[8, 1000]**.
*   Unit: Kbit/s.
*   Default value: **128**.', example='500'),
      channels?: string(name='Channels', description='The number of sound channels. Default value: **2**.', example='2'),
      codec?: string(name='Codec', description='The audio codec. Default value: **aac**. Valid values:

*   **aac**
*   **mp3**
*   **vorbis**
*   **flac**', example='aac'),
      profile?: string(name='Profile', description='The codec profile of the audio. Valid values when the **Codec** parameter is set to **aac**:

*   **aac_low**
*   **aac_he**
*   **aac_he_v2**
*   **aac_ld**
*   **aac_eld**', example='aac_low'),
      qscale?: string(name='Qscale', description='The strength of the independent noise reduction algorithm.', example='1'),
      remove?: string(name='Remove', description='Indicates whether to delete the audio stream.

*   **true**: The audio stream is deleted.
*   **false**: The audio stream is retained.
*   Default value: **false**.', example='false'),
      samplerate?: string(name='Samplerate', description='The sampling rate.

*   Unit: Hz.
*   Default value: **44100**.', example='44100'),
    }(name='Audio', description='The audio codec configuration.'),
    container?: {
      format?: string(name='Format', description='The container format.', example='mp4'),
    }(name='Container', description='The container configuration.'),
    id?: string(name='Id', description='The ID of the transcoding template.', example='16f01ad6175e4230ac42bb5182cd****'),
    muxConfig?: {
      gif?: {
        ditherMode?: string(name='DitherMode', description='The color dithering algorithm of the palette. Valid values: sierra and bayer.', example='bayer'),
        finalDelay?: string(name='FinalDelay', description='The duration for which the final frame is paused. Unit: milliseconds.', example='false'),
        isCustomPalette?: string(name='IsCustomPalette', description='Indicates whether the custom palette is used.', example='0'),
        loop?: string(name='Loop', description='The loop count.', example='0'),
      }(name='Gif', description='The transmuxing configuration for GIF.'),
      segment?: {
        duration?: string(name='Duration', description='The length of the segment. Unit: seconds.', example='10'),
      }(name='Segment', description='The segment configuration.'),
      webp?: {
        loop?: string(name='Loop', description='The loop count.', example='0'),
      }(name='Webp', description='The transmuxing configuration for WebP.'),
    }(name='MuxConfig', description='The transmuxing configuration.'),
    name?: string(name='Name', description='The name of the template.', example='MPS-example'),
    state?: string(name='State', description='The status of the template.

*   **Normal**: The template is normal.
*   **Deleted**: The template is deleted.', example='Normal'),
    transConfig?: {
      adjDarMethod?: string(name='AdjDarMethod', description='The method of resolution adjustment. Default value: **none**. Valid values:

*   rescale: The input video is rescaled.
*   crop: The input video is cropped.
*   none: No change is made.', example='none'),
      isCheckAudioBitrate?: string(name='IsCheckAudioBitrate', description='Indicates whether to check the audio bitrate.

*   If this feature is enabled and the system detects that the audio bitrate of the output file is greater than that of the input file, the audio bitrate of the input file is retained after transcoding.
*   **true**: The audio bitrate is checked.
*   **false**: The audio bitrate is not checked.
*   Default value: **false**.', example='false'),
      isCheckAudioBitrateFail?: string(name='IsCheckAudioBitrateFail', description='Indicates whether to allow audio bitrate check errors.

*   **true**. If the audio bitrate check fails, the input file is not transcoded.
*   **false**: The audio bitrate is not checked.
*   Default value: **false**.
*   This parameter takes precedence over the IsCheckAudioBitrate parameter.', example='false'),
      isCheckReso?: string(name='IsCheckReso', description='Indicates whether to check the resolution.

*   If this feature is enabled and the system detects that the resolution of the output file is higher than that of the input file based on the width or height, the resolution of the input file is retained after transcoding.
*   **true**: The resolution is checked.
*   **false**: The resolution is not checked.
*   Default value: **false**.', example='false'),
      isCheckResoFail?: string(name='IsCheckResoFail', description='Indicates whether to check the resolution.

*   If this feature is enabled and the system detects that the resolution of the output file is higher than that of the input file based on the width or height, an error that indicates a transcoding failure is returned.
*   **true**: The resolution is checked.
*   **false**: The resolution is not checked.
*   Default value: **false**.', example='false'),
      isCheckVideoBitrate?: string(name='IsCheckVideoBitrate', description='Indicates whether to check the video bitrate.

*   If this feature is enabled and the system detects that the video bitrate of the output file is greater than that of the input file, the video bitrate of the input file is retained after transcoding.
*   **true**: The video bitrate is checked.
*   **false**: The video bitrate is not checked.
*   Default value: **false**.', example='false'),
      isCheckVideoBitrateFail?: string(name='IsCheckVideoBitrateFail', description='Indicates whether to allow video bitrate check errors.

*   **true**. If the video bitrate check fails, the input file is not transcoded.
*   **false**: The video bitrate is not checked.
*   Default value: **false**.
*   This parameter takes precedence over the IsCheckVideoBitrate parameter.', example='false'),
      transMode?: string(name='TransMode', description='The transcoding mode. Default value: **onepass**. Valid values:

*   **onepass**
*   **twopass**
*   **CBR**', example='onepass'),
    }(name='TransConfig', description='The general transcoding configuration.'),
    video?: {
      bitrate?: string(name='Bitrate', description='The average bitrate of the video. Unit: Kbit/s.', example='200'),
      bitrateBnd?: {
        max?: string(name='Max', description='The upper limit of the total bitrate. Unit: Kbit/s.', example='500'),
        min?: string(name='Min', description='The lower limit of the total bitrate. Unit: Kbit/s.', example='100'),
      }(name='BitrateBnd', description='The average bitrate range of the video.'),
      bufsize?: string(name='Bufsize', description='The size of the buffer.

*   Unit: KB.
*   Default value: **6000**.', example='6000'),
      codec?: string(name='Codec', description='The video codec. Default value: **H.264**.', example='H.264'),
      crf?: string(name='Crf', description='The constant rate factor.

*   Default value when the Codec parameter is set to H.264: **23**. Default value when the Codec parameter is set to H.265: **26**.
*   If this parameter is specified, the setting of the Bitrate parameter is invalid.', example='15'),
      crop?: string(name='Crop', description='The method of video cropping. Valid values:

*   **border**: automatically detects and removes borders.
*   **Other values**: the custom cropping modes. Format: width:height:left:top. Example: 1280:800:0:140.', example='border'),
      degrain?: string(name='Degrain', description='The level of video quality control.', example='10'),
      fps?: string(name='Fps', description='The frame rate of the video.

*   The value is 60 if the frame rate of the input video exceeds 60.
*   Default value: the frame rate of the input video.', example='25'),
      gop?: string(name='Gop', description='The maximum number of frames between two keyframes. Default value: **250**.', example='10'),
      height?: string(name='Height', description='The height of the video.

*   Unit: pixel.
*   Default value: the height of the input video.', example='800'),
      longShortMode?: string(name='LongShortMode', description='Indicates whether to enable the auto-rotate screen feature.

*   If this feature is enabled, the width of the output video corresponds to the long side of the input video, which is the height of the input video in portrait mode. The height of the output video corresponds to the short side of the input video, which is the width of the input video in portrait mode.
*   **true**: The auto-rotate screen feature is enabled.
*   **false**: The auto-rotate screen feature is disabled.
*   Default value: **false**.', example='false'),
      maxFps?: string(name='MaxFps', description='The maximum frame rate.', example='60'),
      maxrate?: string(name='Maxrate', description='The maximum video bitrate. Unit: Kbit/s.', example='500'),
      pad?: string(name='Pad', description='The black borders to be added to the video.

*   Format: width:height:left:top.
*   Example: 1280:800:0:140.', example='1280:800:0:140'),
      pixFmt?: string(name='PixFmt', description='The pixel format for video color encoding. Valid values: standard pixel formats such as yuv420p and yuvj420p.', example='yuv420p'),
      preset?: string(name='Preset', description='The preset video algorithm. Default value: **medium**. Valid values:

*   **veryfast**
*   **fast**
*   **medium**
*   **slow**
*   **slower**', example='medium'),
      profile?: string(name='Profile', description='The codec profile.

*   **baseline**: applicable to mobile devices.
*   **main**: applicable to standard-definition devices.
*   **high**: applicable to high-definition devices.
*   Default value: **high**.', example='high'),
      qscale?: string(name='Qscale', description='The strength of the independent noise reduction algorithm.', example='1'),
      remove?: string(name='Remove', description='Indicates whether to delete the video stream.

*   **true**: The video stream is deleted.
*   **false**: The video stream is retained.
*   Default value: **false**.', example='false'),
      resoPriority?: string(name='ResoPriority', description='The policy of resolution adjustment.', example='1'),
      scanMode?: string(name='ScanMode', description='The scan mode. Valid values:

*   **interlaced**
*   **progressive**', example='interlaced'),
      width?: string(name='Width', description='The width of the video.

*   Unit: pixel.
*   Default value: the width of the input video.', example='256'),
    }(name='Video', description='The video codec configuration.'),
  }(name='Template', description='The type of the transcoding template.'),
}

model UpdateTemplateResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: UpdateTemplateResponseBody(name='body'),
}

/**
  * A custom transcoding template cannot be updated if it is being used by a job that has been submitted.
  * ## Limits on QPS
  * You can call this operation up to 100 times per second. If the number of the calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request UpdateTemplateRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return UpdateTemplateResponse
 */
async function updateTemplateWithOptions(request: UpdateTemplateRequest, runtime: Util.RuntimeOptions): UpdateTemplateResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.audio)) {
    query['Audio'] = request.audio;
  }
  if (!Util.isUnset(request.container)) {
    query['Container'] = request.container;
  }
  if (!Util.isUnset(request.muxConfig)) {
    query['MuxConfig'] = request.muxConfig;
  }
  if (!Util.isUnset(request.name)) {
    query['Name'] = request.name;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.templateId)) {
    query['TemplateId'] = request.templateId;
  }
  if (!Util.isUnset(request.transConfig)) {
    query['TransConfig'] = request.transConfig;
  }
  if (!Util.isUnset(request.video)) {
    query['Video'] = request.video;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'UpdateTemplate',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * A custom transcoding template cannot be updated if it is being used by a job that has been submitted.
  * ## Limits on QPS
  * You can call this operation up to 100 times per second. If the number of the calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limits](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request UpdateTemplateRequest
  * @return UpdateTemplateResponse
 */
async function updateTemplate(request: UpdateTemplateRequest): UpdateTemplateResponse {
  var runtime = new Util.RuntimeOptions{};
  return updateTemplateWithOptions(request, runtime);
}

model UpdateWaterMarkTemplateRequest {
  config?: string(name='Config', description='The updated configuration of the watermark template. The value is a JSON object. For more information, see [Watermark template configuration](~~29253~~).', example='{"Width":"10","Height":"30","Dx":"10","Dy":"5","Type":"Image","Timeline":{"Start":"0","Duration":"10"}}'),
  name?: string(name='Name', description='The new name of the watermark template. The value can contain letters and digits and can be up to 128 bytes in size.', example='example-watermark-****'),
  ownerAccount?: string(name='OwnerAccount'),
  ownerId?: long(name='OwnerId'),
  resourceOwnerAccount?: string(name='ResourceOwnerAccount'),
  resourceOwnerId?: long(name='ResourceOwnerId'),
  waterMarkTemplateId?: string(name='WaterMarkTemplateId', description='The ID of the watermark template that you want to update. To obtain the ID of the watermark template, you can log on to the **ApsaraVideo Media Processing (MPS) console** and choose **Global Settings** > **Watermark Templates** in the left-side navigation pane.', example='3780bd69b2b74540bc7b1096f564****'),
}

model UpdateWaterMarkTemplateResponseBody = {
  requestId?: string(name='RequestId', description='The ID of the request.', example='E558894E-40D9-57C6-B5CC-0F5CDF23614E'),
  waterMarkTemplate?: {
    dx?: string(name='Dx', description='The horizontal offset. Unit: pixel.', example='10'),
    dy?: string(name='Dy', description='The vertical offset. Unit: pixel.', example='5'),
    height?: string(name='Height', description='The height of the watermark image. Unit: pixel.', example='30'),
    id?: string(name='Id', description='The ID of the watermark template. We recommend that you keep this ID for subsequent operation calls.', example='3780bd69b2b74540bc7b1096f564****'),
    name?: string(name='Name', description='The name of the watermark template.', example='example-watermark-****'),
    ratioRefer?: {
      dx?: string(name='Dx', description='The horizontal offset of the watermark relative to the output video image. Default value: **0**. The default value indicates no offset. The value can be an integer or a decimal.

*   **Integer**: the vertical offset. This indicates the absolute position. Unit: pixel.
*   **Decimal**: the ratio of the horizontal offset to the width of the output video. The ratio varies based on the size of the video. Four decimal places are supported, such as 0.9999. More decimal places are discarded.', example='0.51'),
      dy?: string(name='Dy', description='The vertical offset of the watermark relative to the output video image. Default value: **0**. The default value indicates no offset. The value can be an integer or a decimal.

*   **Integer**: the vertical offset. This indicates the absolute position. Unit: pixel.
*   **Decimal**: the ratio of the vertical offset to the height of the output video. The ratio varies based on the size of the video. Four decimal places are supported, such as 0.9999. More decimal places are discarded.', example='0.28'),
      height?: string(name='Height', description='The height of the watermark image in the output video. The value can be an integer or a decimal.

*   **Integer**: the height of the watermark image. This indicates the absolute position. Unit: pixel.
*   **Decimal**: the ratio of the height of the watermark image to the height of the output video. The ratio varies based on the size of the video. Four decimal places are supported, such as 0.9999. More decimal places are discarded.', example='0.33'),
      width?: string(name='Width', description='The width of the watermark image in the output video. The value can be an integer or a decimal.

*   **Integer**: the width of the watermark image. This indicates the absolute position. Unit: pixel.
*   **Decimal**: the ratio of the width of the watermark image to the width of the output video. The ratio varies based on the size of the video. Four decimal places are supported, such as 0.9999. More decimal places are discarded.', example='0.36'),
    }(name='RatioRefer', description='The values of the Height, Width, Dx, and Dy parameters relative to the reference edges. If the values of the Height, Width, Dx, and Dy parameters are decimals between 0 and 1, the values are calculated by referring to the following edges in sequence:

*   **Width**: the width edge.
*   **Height**: the height edge.
*   **Long**: the long edge.
*   **Short**: the short edge.'),
    referPos?: string(name='ReferPos', description='The position of the watermark. Valid values:

*   **TopRight**: the upper-right corner.
*   **TopLeft**: the upper-left corner.
*   **BottomRight**: the lower-right corner.
*   **BottomLeft**: the lower-left corner.', example='TopRight'),
    state?: string(name='State', description='The status of the watermark template. Default value: **Normal**.', example='Normal'),
    timeline?: {
      duration?: string(name='Duration', description='The display duration of the watermark. Default value: **ToEND**. The default value indicates that the watermark is displayed until the video ends.', example='10'),
      start?: string(name='Start', description='The beginning of the time range during which the watermark is displayed.

*   Unit: seconds.
*   Default value: **0**.', example='0'),
    }(name='Timeline', description='The timeline of the watermark.'),
    type?: string(name='Type', description='The type of the watermark. Valid values:

*   Image: an image watermark.
*   Text: a text watermark.

>  Only watermarks of the Image type are supported.', example='Image'),
    width?: string(name='Width', description='The width of the watermark image. Unit: pixel.', example='10'),
  }(name='WaterMarkTemplate', description='The details of the watermark template.'),
}

model UpdateWaterMarkTemplateResponse = {
  headers: map[string]string(name='headers'),
  statusCode: int32(name='statusCode'),
  body: UpdateWaterMarkTemplateResponseBody(name='body'),
}

/**
  * *   You can call this operation to update the information about a watermark template based on the ID of the watermark template. For example, you can update the name and configuration of a watermark template.
  * *   A watermark template cannot be updated if it is being used by a job that has been submitted.
  * ## QPS limit
  * You can call this API operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request UpdateWaterMarkTemplateRequest
  * @param runtime runtime options for this request RuntimeOptions
  * @return UpdateWaterMarkTemplateResponse
 */
async function updateWaterMarkTemplateWithOptions(request: UpdateWaterMarkTemplateRequest, runtime: Util.RuntimeOptions): UpdateWaterMarkTemplateResponse {
  Util.validateModel(request);
  var query = {};
  if (!Util.isUnset(request.config)) {
    query['Config'] = request.config;
  }
  if (!Util.isUnset(request.name)) {
    query['Name'] = request.name;
  }
  if (!Util.isUnset(request.ownerAccount)) {
    query['OwnerAccount'] = request.ownerAccount;
  }
  if (!Util.isUnset(request.ownerId)) {
    query['OwnerId'] = request.ownerId;
  }
  if (!Util.isUnset(request.resourceOwnerAccount)) {
    query['ResourceOwnerAccount'] = request.resourceOwnerAccount;
  }
  if (!Util.isUnset(request.resourceOwnerId)) {
    query['ResourceOwnerId'] = request.resourceOwnerId;
  }
  if (!Util.isUnset(request.waterMarkTemplateId)) {
    query['WaterMarkTemplateId'] = request.waterMarkTemplateId;
  }
  var req = new OpenApi.OpenApiRequest{ 
    query = OpenApiUtil.query(query),
  };
  var params = new OpenApi.Params{
    action = 'UpdateWaterMarkTemplate',
    version = '2014-06-18',
    protocol = 'HTTPS',
    pathname = '/',
    method = 'POST',
    authType = 'AK',
    style = 'RPC',
    reqBodyType = 'formData',
    bodyType = 'json',
  };
  return callApi(params, req, runtime);
}

/**
  * *   You can call this operation to update the information about a watermark template based on the ID of the watermark template. For example, you can update the name and configuration of a watermark template.
  * *   A watermark template cannot be updated if it is being used by a job that has been submitted.
  * ## QPS limit
  * You can call this API operation up to 100 times per second per account. If the number of calls per second exceeds the limit, throttling is triggered. As a result, your business may be affected. We recommend that you take note of the limit when you call this operation. For more information, see [QPS limit](https://www.alibabacloud.com/help/en/apsaravideo-for-media-processing/latest/qps-limit).
  *
  * @param request UpdateWaterMarkTemplateRequest
  * @return UpdateWaterMarkTemplateResponse
 */
async function updateWaterMarkTemplate(request: UpdateWaterMarkTemplateRequest): UpdateWaterMarkTemplateResponse {
  var runtime = new Util.RuntimeOptions{};
  return updateWaterMarkTemplateWithOptions(request, runtime);
}

