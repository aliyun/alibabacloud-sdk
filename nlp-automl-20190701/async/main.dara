/**
 *
 */
import BaseClientBuilder;
import TeaAsyncHandler;
import TeaRequest;
import AsyncRequestBody;
import RequestBody;
import AsyncResponseHandler;
import ClientConfiguration;
import ClientExecutionParams;
extends BaseClientBuilder;
type @product = string
type @version = string
type @endpointRule = string
type @endpointMap = map[string]string
type @REQUEST = TeaRequest
type @handler = TeaAsyncHandler

init(configuration: ClientConfiguration){
  @handler = new TeaAsyncHandler(configuration);
  @product = 'nlp-automl';
  @version = '2019-07-01';
  @endpointRule = 'regional';
  @endpointMap = {
  };
}

function close(): void {
  @handler.close();
}

model AddMTInterveneWordRequest {
  packageId?: string(name='PackageId', example='1', position='Query'),
  projectId: string(name='ProjectId', example='1', position='Query'),
  sourceText: string(name='SourceText', example='hello', position='Query'),
  targetText?: string(name='TargetText', position='Query'),
}

model AddMTInterveneWordResponseBody = {
  code?: int32(name='Code', example='200'),
  message?: string(name='Message', example='parameterError'),
  requestId?: string(name='RequestId', example='46E6B40D-2B6C-571B-AC41-86207DE288A5'),
  wordId?: long(name='WordId', example='1'),
}

model AddMTInterveneWordResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: AddMTInterveneWordResponseBody(name='body'),
}

async function addMTInterveneWord(request: AddMTInterveneWordRequest): AddMTInterveneWordResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'AddMTInterveneWord', 'POST', '/', 'json', false, 'json', request);
}

model GetPredictDocRequest {
  docId: long(name='DocId', example='1000', position='Query'),
  product?: string(name='Product', position='Query'),
}

model GetPredictDocResponseBody = {
  requestId?: string(name='RequestId', example='86D18195-D89C-4C8C-9DC4-5FCE789CE6D5'),
  resultContent?: string(name='ResultContent', example='xxx'),
  status?: int32(name='Status', example='2'),
  XLIFFInfo?: string(name='XLIFFInfo', example='xxx'),
}

model GetPredictDocResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: GetPredictDocResponseBody(name='body'),
}

async function getPredictDoc(request: GetPredictDocRequest): GetPredictDocResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'GetPredictDoc', 'POST', '/', 'json', false, 'json', request);
}

model PredictMTModelByDocRequest {
  fileContent: string(name='FileContent', example='[base64 encode content]', position='Body'),
  fileType: string(name='FileType', example='docx', position='Query'),
  modelId: int32(name='ModelId', example='1', position='Query'),
  modelVersion: string(name='ModelVersion', example='v1', position='Query'),
  needXLIFF?: boolean(name='NeedXLIFF', example='true', position='Query'),
}

model PredictMTModelByDocResponseBody = {
  docId?: string(name='DocId', example='1'),
  requestId?: string(name='RequestId', example='86D18195-D89C-4C8C-9DC4-5FCE789CE6D5'),
}

model PredictMTModelByDocResponse = {
  headers?: map[string]string(name='headers'),
  statusCode?: int32(name='statusCode'),
  body?: PredictMTModelByDocResponseBody(name='body'),
}

async function predictMTModelByDoc(request: PredictMTModelByDocRequest): PredictMTModelByDocResponse {
  @handler.validateRequestModel(request);
  return @handler.do('RPC', 'PredictMTModelByDoc', 'POST', '/', 'json', true, 'form', request);
}

